<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es-ES" xml:lang="es-ES"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Ejercicios Avanzados – Problemas de Modelos Estadísticos para la Predicción</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./tema5_glm_soluciones.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea1d7ac60288e0f1efdbc993fd8432ae.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0687a6949ae11671cfb8b930681aab34.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No se han encontrado resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="soluciones.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ejercicios_avanzados_soluciones.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ejercicios Avanzados</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Problemas de Modelos Estadísticos para la Predicción</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema1_regresion_simple_soluciones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Regresión Lineal Simple</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema2_regresion_multiple_soluciones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Regresión Lineal Múltiple</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema3_ingenieria_caracteristicas_soluciones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Ingeniería de Características</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema4_seleccion_validacion_soluciones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Selección de variables, Regularización y Validación</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tema5_glm_soluciones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelos de Regresión Generalizada</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ejercicios_avanzados_soluciones.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ejercicios Avanzados</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#ejercicio-1-derivación-de-estimadores" id="toc-ejercicio-1-derivación-de-estimadores" class="nav-link active" data-scroll-target="#ejercicio-1-derivación-de-estimadores"><span class="header-section-number">6.1</span> Ejercicio 1: Derivación de Estimadores</a></li>
  <li><a href="#ejercicio-2-el-impacto-de-la-multicolinealidad" id="toc-ejercicio-2-el-impacto-de-la-multicolinealidad" class="nav-link" data-scroll-target="#ejercicio-2-el-impacto-de-la-multicolinealidad"><span class="header-section-number">6.2</span> Ejercicio 2: El Impacto de la Multicolinealidad</a></li>
  <li><a href="#ejercicio-3-interpretación-de-coeficientes-en-modelos-transformados" id="toc-ejercicio-3-interpretación-de-coeficientes-en-modelos-transformados" class="nav-link" data-scroll-target="#ejercicio-3-interpretación-de-coeficientes-en-modelos-transformados"><span class="header-section-number">6.3</span> Ejercicio 3: Interpretación de Coeficientes en Modelos Transformados</a></li>
  <li><a href="#ejercicio-4-fundamentos-de-la-regularización" id="toc-ejercicio-4-fundamentos-de-la-regularización" class="nav-link" data-scroll-target="#ejercicio-4-fundamentos-de-la-regularización"><span class="header-section-number">6.4</span> Ejercicio 4: Fundamentos de la Regularización</a></li>
  <li><a href="#ejercicio-5-la-familia-exponencial-y-los-glm" id="toc-ejercicio-5-la-familia-exponencial-y-los-glm" class="nav-link" data-scroll-target="#ejercicio-5-la-familia-exponencial-y-los-glm"><span class="header-section-number">6.5</span> Ejercicio 5: La Familia Exponencial y los GLM</a></li>
  <li><a href="#ejercicio-6-el-problema-de-la-inferencia-en-métodos-stepwise" id="toc-ejercicio-6-el-problema-de-la-inferencia-en-métodos-stepwise" class="nav-link" data-scroll-target="#ejercicio-6-el-problema-de-la-inferencia-en-métodos-stepwise"><span class="header-section-number">6.6</span> Ejercicio 6: El Problema de la Inferencia en Métodos Stepwise</a></li>
  <li><a href="#ejercicio-7-propiedades-de-los-estimadores-mco" id="toc-ejercicio-7-propiedades-de-los-estimadores-mco" class="nav-link" data-scroll-target="#ejercicio-7-propiedades-de-los-estimadores-mco"><span class="header-section-number">6.7</span> Ejercicio 7: Propiedades de los Estimadores MCO</a></li>
  <li><a href="#ejercicio-8-intervalos-de-confianza-vs.-predicción" id="toc-ejercicio-8-intervalos-de-confianza-vs.-predicción" class="nav-link" data-scroll-target="#ejercicio-8-intervalos-de-confianza-vs.-predicción"><span class="header-section-number">6.8</span> Ejercicio 8: Intervalos de Confianza vs.&nbsp;Predicción</a></li>
  <li><a href="#ejercicio-9-estimación-por-máxima-verosimilitud" id="toc-ejercicio-9-estimación-por-máxima-verosimilitud" class="nav-link" data-scroll-target="#ejercicio-9-estimación-por-máxima-verosimilitud"><span class="header-section-number">6.9</span> Ejercicio 9: Estimación por Máxima Verosimilitud</a></li>
  <li><a href="#ejercicio-10-el-coeficiente-de-regresión-parcial" id="toc-ejercicio-10-el-coeficiente-de-regresión-parcial" class="nav-link" data-scroll-target="#ejercicio-10-el-coeficiente-de-regresión-parcial"><span class="header-section-number">6.10</span> Ejercicio 10: El Coeficiente de Regresión Parcial</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ejercicios Avanzados</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="ejercicio-1-derivación-de-estimadores" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="ejercicio-1-derivación-de-estimadores"><span class="header-section-number">6.1</span> Ejercicio 1: Derivación de Estimadores</h2>
<p>Considera el modelo de regresión lineal simple <span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span>. Partiendo de la función objetivo de Mínimos Cuadrados Ordinarios (MCO), <span class="math inline">\(S(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2\)</span>, realiza la derivación matemática completa para obtener las expresiones de los estimadores <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span>. Muestra todos los pasos, desde el cálculo de las derivadas parciales hasta la resolución de las ecuaciones normales.</p>
<details>
<summary>
</summary>
<p>El objetivo es encontrar los valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> que minimizan la Suma de Cuadrados del Error (SSE). Para ello, calculamos las derivadas parciales de la función <span class="math inline">\(S(\beta_0, \beta_1)\)</span> con respecto a cada parámetro y las igualamos a cero.</p>
<ol type="1">
<li><p><strong>Derivada parcial con respecto a <span class="math inline">\(\beta_0\)</span>:</strong> <span class="math display">\[
\frac{\partial S}{\partial \beta_0} = \sum_{i=1}^{n} 2(y_i - \beta_0 - \beta_1 x_i)(-1) = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)
\]</span> Igualando a cero y dividiendo por -2: <span class="math display">\[
\sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \implies \sum y_i - n\hat{\beta}_0 - \hat{\beta}_1 \sum x_i = 0
\]</span> Reordenando, obtenemos la primera <strong>ecuación normal</strong>: <span class="math display">\[
n\hat{\beta}_0 + \hat{\beta}_1 \sum x_i = \sum y_i \quad \text{(1)}
\]</span></p></li>
<li><p><strong>Derivada parcial con respecto a <span class="math inline">\(\beta_1\)</span>:</strong> <span class="math display">\[
\frac{\partial S}{\partial \beta_1} = \sum_{i=1}^{n} 2(y_i - \beta_0 - \beta_1 x_i)(-x_i) = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i)
\]</span> Igualando a cero y dividiendo por -2: <span class="math display">\[
\sum_{i=1}^{n} x_i(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \implies \sum x_iy_i - \hat{\beta}_0 \sum x_i - \hat{\beta}_1 \sum x_i^2 = 0
\]</span> Reordenando, obtenemos la segunda <strong>ecuación normal</strong>: <span class="math display">\[
\hat{\beta}_0 \sum x_i + \hat{\beta}_1 \sum x_i^2 = \sum x_iy_i \quad \text{(2)}
\]</span></p></li>
<li><p><strong>Resolución del sistema:</strong> De la ecuación (1), dividiendo por <span class="math inline">\(n\)</span>, podemos despejar <span class="math inline">\(\hat{\beta}_0\)</span>: <span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 \bar{x} = \bar{y} \implies \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]</span> Esta es la fórmula para el intercepto, que depende de la pendiente.</p>
<p>Sustituimos esta expresión de <span class="math inline">\(\hat{\beta}_0\)</span> en la ecuación (2): <span class="math display">\[
(\bar{y} - \hat{\beta}_1 \bar{x})\sum x_i + \hat{\beta}_1 \sum x_i^2 = \sum x_iy_i
\]</span> <span class="math display">\[
\bar{y}\sum x_i - \hat{\beta}_1 \bar{x}\sum x_i + \hat{\beta}_1 \sum x_i^2 = \sum x_iy_i
\]</span> Agrupamos los términos con <span class="math inline">\(\hat{\beta}_1\)</span>: <span class="math display">\[
\hat{\beta}_1 (\sum x_i^2 - \bar{x}\sum x_i) = \sum x_iy_i - \bar{y}\sum x_i
\]</span> Sabiendo que <span class="math inline">\(\sum x_i = n\bar{x}\)</span> y <span class="math inline">\(\sum y_i = n\bar{y}\)</span>: <span class="math display">\[
\hat{\beta}_1 (\sum x_i^2 - n\bar{x}^2) = \sum x_iy_i - n\bar{x}\bar{y}
\]</span> Las expresiones entre paréntesis son las fórmulas de la suma de cuadrados de X (<span class="math inline">\(S_{xx}\)</span>) y la suma de productos cruzados de X e Y (<span class="math inline">\(S_{xy}\)</span>): <span class="math display">\[
\hat{\beta}_1 S_{xx} = S_{xy}
\]</span> Finalmente, despejamos el estimador de la pendiente: <span class="math display">\[
\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
\]</span> Estas son las expresiones para los estimadores de MCO.</p></li>
</ol>
</details>
</section>
<section id="ejercicio-2-el-impacto-de-la-multicolinealidad" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="ejercicio-2-el-impacto-de-la-multicolinealidad"><span class="header-section-number">6.2</span> Ejercicio 2: El Impacto de la Multicolinealidad</h2>
<p>En un modelo de regresión múltiple con dos predictores estandarizados (<span class="math inline">\(X_1, X_2\)</span>), la varianza del estimador <span class="math inline">\(\hat{\beta}_1\)</span> viene dada por <span class="math inline">\(\text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{n(1-r_{12}^2)}\)</span>, donde <span class="math inline">\(r_{12}\)</span> es la correlación entre <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span>.</p>
<ol type="a">
<li>Explica matemáticamente qué le ocurre a la varianza de <span class="math inline">\(\hat{\beta}_1\)</span> cuando la correlación entre los predictores (<span class="math inline">\(r_{12}\)</span>) se aproxima a 1 (multicolinealidad perfecta).</li>
<li>Relaciona esta fórmula con la del Factor de Inflación de la Varianza (VIF). ¿Cómo demuestra esta expresión que la multicolinealidad “infla” la varianza de los estimadores de los coeficientes?</li>
</ol>
<details>
<summary>
</summary>
<p><strong>a) Efecto de la correlación en la varianza:</strong> La varianza del estimador, <span class="math inline">\(\text{Var}(\hat{\beta}_1)\)</span>, es una medida de su imprecisión. La fórmula <span class="math inline">\(\frac{\sigma^2}{n(1-r_{12}^2)}\)</span> muestra que la varianza depende inversamente del término <span class="math inline">\((1-r_{12}^2)\)</span>. - Si <span class="math inline">\(r_{12} = 0\)</span> (no hay correlación), la varianza es mínima: <span class="math inline">\(\text{Var}(\hat{\beta}_1) = \sigma^2/n\)</span>. - A medida que la correlación <span class="math inline">\(|r_{12}|\)</span> aumenta y se acerca a 1 (multicolinealidad perfecta), el término <span class="math inline">\(r_{12}^2\)</span> también se acerca a 1. - Consecuentemente, el denominador <span class="math inline">\((1-r_{12}^2)\)</span> se aproxima a 0. - Matemáticamente, cuando el denominador de una fracción tiende a cero, el valor de la fracción tiende a infinito. Por lo tanto: <span class="math display">\[
    \lim_{|r_{12}| \to 1} \text{Var}(\hat{\beta}_1) = \lim_{|r_{12}| \to 1} \frac{\sigma^2}{n(1-r_{12}^2)} = \infty
    \]</span> Esto significa que con multicolinealidad severa, la varianza de los estimadores de los coeficientes “explota”, volviéndolos extremadamente inestables y poco fiables.</p>
<p><strong>b) Relación con el Factor de Inflación de la Varianza (VIF):</strong> El VIF para un predictor <span class="math inline">\(X_j\)</span> se define como <span class="math inline">\(VIF_j = \frac{1}{1 - R_j^2}\)</span>, donde <span class="math inline">\(R_j^2\)</span> es el R-cuadrado de la regresión de <span class="math inline">\(X_j\)</span> sobre todos los demás predictores. En el caso de solo dos predictores (<span class="math inline">\(X_1, X_2\)</span>), el <span class="math inline">\(R^2\)</span> de la regresión de <span class="math inline">\(X_1\)</span> sobre <span class="math inline">\(X_2\)</span> es simplemente el cuadrado de su coeficiente de correlación, es decir, <span class="math inline">\(R_1^2 = r_{12}^2\)</span>. Sustituyendo esto en la fórmula del VIF, tenemos: <span class="math display">\[
  VIF_1 = \frac{1}{1 - r_{12}^2}
  \]</span> Ahora podemos reescribir la fórmula de la varianza de <span class="math inline">\(\hat{\beta}_1\)</span> usando el VIF: <span class="math display">\[
  \text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{n} \cdot \frac{1}{1-r_{12}^2} = \frac{\sigma^2}{n} \cdot VIF_1
  \]</span> Esta expresión demuestra que el VIF es, literalmente, el <strong>factor multiplicativo</strong> por el cual la varianza del estimador del coeficiente se “infla” en comparación con el caso base en el que no habría correlación (donde VIF = 1).</p>
</details>
</section>
<section id="ejercicio-3-interpretación-de-coeficientes-en-modelos-transformados" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="ejercicio-3-interpretación-de-coeficientes-en-modelos-transformados"><span class="header-section-number">6.3</span> Ejercicio 3: Interpretación de Coeficientes en Modelos Transformados</h2>
<p>Considera un modelo de regresión <strong>log-log</strong>: <span class="math inline">\(\log(Y_i) = \beta_0 + \beta_1 \log(X_i) + \varepsilon_i\)</span>. Demuestra matemáticamente que el coeficiente <span class="math inline">\(\beta_1\)</span> puede interpretarse como una <strong>elasticidad</strong>, es decir, el cambio porcentual en <span class="math inline">\(Y\)</span> ante un cambio del 1% en <span class="math inline">\(X\)</span>. (Pista: utiliza la derivada de <span class="math inline">\(\log(Y)\)</span> con respecto a <span class="math inline">\(\log(X)\)</span>).</p>
<details>
<summary>
</summary>
<p>La elasticidad de Y con respecto a X se define como el cambio porcentual en Y para un cambio del 1% en X. Para cambios infinitesimales, esta se expresa como: <span class="math display">\[\eta = \frac{\% \Delta Y}{\% \Delta X} = \frac{dY/Y}{dX/X}\]</span> Una propiedad matemática de los logaritmos es que para cambios pequeños, <span class="math inline">\(d(\log(z)) \approx \frac{dz}{z}\)</span>, que representa un cambio relativo o porcentual. Por lo tanto, la elasticidad puede expresarse como la derivada del logaritmo de Y con respecto al logaritmo de X: <span class="math display">\[\eta = \frac{d(\log Y)}{d(\log X)}\]</span> Partiendo de nuestro modelo poblacional (ignorando el término de error para analizar la relación sistemática): <span class="math display">\[\log(Y) = \beta_0 + \beta_1 \log(X)\]</span> Ahora, simplemente calculamos la derivada de la ecuación con respecto a <span class="math inline">\(\log(X)\)</span>: <span class="math display">\[\frac{d(\log Y)}{d(\log X)} = \frac{d}{d(\log X)} (\beta_0 + \beta_1 \log(X))\]</span> El término <span class="math inline">\(\beta_0\)</span> es una constante, por lo que su derivada es 0. El término <span class="math inline">\(\beta_1 \log(X)\)</span> tiene una derivada de <span class="math inline">\(\beta_1\)</span> con respecto a <span class="math inline">\(\log(X)\)</span>. Por lo tanto: <span class="math display">\[\frac{d(\log Y)}{d(\log X)} = \beta_1\]</span> Hemos demostrado que el coeficiente <span class="math inline">\(\beta_1\)</span> es igual a la elasticidad de Y con respecto a X. Así, <span class="math inline">\(\beta_1\)</span> representa el cambio porcentual promedio en Y que se asocia con un aumento del 1% en X.</p>
</details>
</section>
<section id="ejercicio-4-fundamentos-de-la-regularización" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="ejercicio-4-fundamentos-de-la-regularización"><span class="header-section-number">6.4</span> Ejercicio 4: Fundamentos de la Regularización</h2>
<p>Explica desde una perspectiva geométrica por qué la regularización <strong>Lasso (penalización L1)</strong> es capaz de reducir los coeficientes exactamente a cero, realizando así selección de variables, mientras que la regularización <strong>Ridge (penalización L2)</strong> solo puede encoger los coeficientes hacia cero sin anularlos por completo. Apoya tu explicación con un dibujo o descripción de las “regiones de restricción” de ambos métodos en un espacio de dos coeficientes (<span class="math inline">\(\beta_1, \beta_2\)</span>).</p>
<details>
<summary>
</summary>
<p>La estimación en regresión regularizada puede entenderse como un problema de optimización restringida. El objetivo es encontrar el conjunto de coeficientes (<span class="math inline">\(\beta_1, \beta_2, \dots\)</span>) que minimice la Suma de Cuadrados del Error (SSE), sujeto a una restricción en el tamaño de dichos coeficientes.</p>
<ul>
<li><p><strong>Geometría del problema:</strong> El conjunto de todos los posibles valores de los coeficientes para un mismo valor de SSE forma una elipse (en un espacio de dos coeficientes, <span class="math inline">\(\beta_1, \beta_2\)</span>) centrada en la solución de Mínimos Cuadrados Ordinarios (MCO). El objetivo es encontrar la elipse más pequeña posible que toque la “región de restricción”.</p></li>
<li><p><strong>Regresión Ridge (Penalización L2):</strong> La restricción es <span class="math inline">\(\sum \beta_j^2 \leq s\)</span>. En dos dimensiones, <span class="math inline">\(\beta_1^2 + \beta_2^2 \leq s\)</span> es la ecuación de un <strong>círculo</strong>. Esta región es convexa y no tiene “esquinas”. Cuando las elipses del SSE se expanden desde el punto MCO, el primer punto de contacto con el círculo será un punto de tangencia. Debido a la forma suave y redondeada del círculo, es extremadamente improbable que este punto de tangencia ocurra exactamente sobre un eje (donde uno de los coeficientes sería cero). Por lo tanto, Ridge reduce la magnitud de ambos coeficientes, pero no los anula.</p></li>
<li><p><strong>Regresión Lasso (Penalización L1):</strong> La restricción es <span class="math inline">\(\sum |\beta_j| \leq s\)</span>. En dos dimensiones, <span class="math inline">\(|\beta_1| + |\beta_2| \leq s\)</span> es la ecuación de un <strong>rombo (o diamante)</strong>, rotado 45 grados. La característica clave de esta región son sus <strong>vértices afilados, que se encuentran sobre los ejes</strong>. Cuando las elipses del SSE se expanden, es mucho más probable que toquen la región de restricción en uno de estos vértices que en una de las aristas. Si el punto de contacto es un vértice sobre un eje (por ejemplo, el punto (0, <span class="math inline">\(\beta_2\)</span>)), significa que el otro coeficiente (<span class="math inline">\(\beta_1\)</span>) es <strong>exactamente cero</strong>. Es esta propiedad geométrica, las “esquinas” de la región de penalización L1, lo que induce la escasez (<em>sparsity</em>) y permite a Lasso realizar selección de variables.</p></li>
</ul>
</details>
</section>
<section id="ejercicio-5-la-familia-exponencial-y-los-glm" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="ejercicio-5-la-familia-exponencial-y-los-glm"><span class="header-section-number">6.5</span> Ejercicio 5: La Familia Exponencial y los GLM</h2>
<p>La teoría de los Modelos Lineales Generalizados (GLM) se basa en que distribuciones como la Normal, Binomial o Poisson pertenecen a la <strong>familia exponencial</strong>. La forma canónica de esta familia establece una relación directa entre la media y la varianza a través de la <strong>función de varianza</strong> <span class="math inline">\(V(\mu)\)</span>. Explica cuál es la función de varianza para un modelo de <strong>Poisson</strong> y para un modelo <strong>Binomial</strong>. ¿Qué implicaciones tiene la forma de <span class="math inline">\(V(\mu)\)</span> en cada caso sobre el comportamiento de los datos y los supuestos del modelo?</p>
<details>
<summary>
</summary>
<p>La <strong>función de varianza</strong> <span class="math inline">\(V(\mu)\)</span> es la “firma” de cada distribución dentro de la familia exponencial, ya que define la relación teórica entre la media <span class="math inline">\(\mu\)</span> y la varianza de la variable respuesta.</p>
<ul>
<li><strong>Modelo de Poisson:</strong>
<ul>
<li><strong>Función de Varianza:</strong> <span class="math inline">\(V(\mu) = \mu\)</span>.</li>
<li><strong>Implicación:</strong> Esto implica que la varianza de la variable respuesta es teóricamente igual a su media: <span class="math inline">\(\text{Var}(Y) = \mu\)</span>. Este supuesto se conoce como <strong>equidispersión</strong>. La implicación más importante para el modelado es que, si los datos reales muestran una varianza significativamente mayor que la media (un fenómeno muy común llamado <strong>sobredispersión</strong>), el modelo de Poisson será inadecuado. Los errores estándar de los coeficientes estarán subestimados, llevando a p-valores incorrectamente bajos y a una inferencia errónea.</li>
</ul></li>
<li><strong>Modelo Binomial:</strong>
<ul>
<li><strong>Función de Varianza:</strong> <span class="math inline">\(V(\mu) = \mu(1-\mu)\)</span>.</li>
<li><strong>Implicación:</strong> En este caso, la varianza no es constante, sino que es una función cuadrática de la media (la probabilidad de éxito). La varianza es mínima cuando <span class="math inline">\(\mu\)</span> se acerca a 0 o 1, y es máxima cuando <span class="math inline">\(\mu = 0.5\)</span>. Esta es la heterocedasticidad inherente a los datos de proporciones. El modelo GLM maneja esto de forma natural a través del algoritmo de estimación (IRLS), que da más peso a las observaciones con menor varianza (aquellas con probabilidades predichas cercanas a 0 o 1) y menos peso a las más inciertas (aquellas con probabilidades cercanas a 0.5).</li>
</ul></li>
</ul>
</details>
</section>
<section id="ejercicio-6-el-problema-de-la-inferencia-en-métodos-stepwise" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="ejercicio-6-el-problema-de-la-inferencia-en-métodos-stepwise"><span class="header-section-number">6.6</span> Ejercicio 6: El Problema de la Inferencia en Métodos Stepwise</h2>
<p>Los apuntes advierten que los p-valores de un modelo final obtenido mediante selección por pasos (stepwise) están <strong>sesgados y son excesivamente optimistas</strong>. Explica el razonamiento estadístico detrás de esta advertencia. ¿Por qué el proceso iterativo de “buscar y seleccionar” la variable más significativa en cada paso invalida los supuestos teóricos del test t estándar?</p>
<details>
<summary>
</summary>
<p>La advertencia se debe a que los métodos stepwise violan un principio fundamental de la prueba de hipótesis: el modelo y las hipótesis deben ser especificados <strong>a priori</strong>, antes de examinar las relaciones en los datos. Los métodos stepwise hacen exactamente lo contrario.</p>
<ol type="1">
<li><p><strong>Problema de Múltiples Comparaciones:</strong> En cada paso, un algoritmo como la selección <em>forward</em> realiza múltiples tests (un test t para cada variable candidata a entrar) y selecciona la variable “ganadora”, que es la que tiene el p-valor más pequeño. Al elegir el valor mínimo de un conjunto de pruebas, estamos seleccionando un valor extremo de la distribución de p-valores bajo la hipótesis nula. El p-valor reportado para esa variable (p.&nbsp;ej., 0.03) no refleja la probabilidad de observar un resultado tan extremo en un solo intento, sino la probabilidad de que <em>el mejor de varios intentos</em> sea tan extremo, lo cual es una probabilidad mucho mayor.</p></li>
<li><p><strong>Invalidez de la Distribución Teórica:</strong> El p-valor de un test t se calcula asumiendo que el coeficiente sigue una distribución t de Student. Sin embargo, el coeficiente de una variable seleccionada por un algoritmo stepwise no sigue esta distribución. Sigue una distribución más compleja (una “distribución de un estadístico de orden”), porque ha sido seleccionado condicionalmente por ser el mejor.</p></li>
<li><p><strong>Sesgo de Selección:</strong> El proceso está diseñado para encontrar relaciones, incluso en datos puramente aleatorios. Si tenemos muchas variables de ruido, la probabilidad de que una de ellas parezca significativa por puro azar es alta. El método stepwise seleccionará esa variable y reportará un p-valor bajo y engañoso.</p></li>
</ol>
<p>En resumen, los p-valores de un modelo stepwise están <strong>sesgados a la baja (son demasiado pequeños)</strong> porque no tienen en cuenta el proceso de búsqueda y selección que los ha producido. Esto lleva a una <strong>inflación de la tasa de error de Tipo I</strong>, haciendo que concluyamos que ciertas variables son significativas cuando en realidad no lo son.</p>
</details>
</section>
<section id="ejercicio-7-propiedades-de-los-estimadores-mco" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="ejercicio-7-propiedades-de-los-estimadores-mco"><span class="header-section-number">6.7</span> Ejercicio 7: Propiedades de los Estimadores MCO</h2>
<p>El <strong>Teorema de Gauss-Markov</strong> establece que, bajo ciertos supuestos, los estimadores de Mínimos Cuadrados Ordinarios (MCO) son <strong>MELI (Mejores Estimadores Lineales Insesgados)</strong>. Demuestra la propiedad de <strong>insesgadez</strong> para el estimador <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> en notación matricial. Es decir, demuestra que <span class="math inline">\(E[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\)</span>. Muestra todos los pasos y menciona qué supuestos del modelo estás utilizando en cada paso.</p>
<details>
<summary>
</summary>
<ol type="1">
<li><p>Comenzamos con la fórmula del estimador MCO en notación matricial: <span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\]</span></p></li>
<li><p>Sustituimos el modelo poblacional verdadero para el vector <span class="math inline">\(\mathbf{y}\)</span>, que es <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}\)</span>: <span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})
\]</span></p></li>
<li><p>Aplicamos el operador de valor esperado <span class="math inline">\(E[\cdot]\)</span> a ambos lados. Tratamos la matriz de diseño <span class="math inline">\(\mathbf{X}\)</span> como fija (no aleatoria): <span class="math display">\[
E[\hat{\boldsymbol{\beta}}] = E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]
\]</span></p></li>
<li><p>Distribuimos el término <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span> dentro del paréntesis: <span class="math display">\[
E[\hat{\boldsymbol{\beta}}] = E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\boldsymbol{\varepsilon}]
\]</span></p></li>
<li><p>Usamos la propiedad de linealidad del valor esperado (<span class="math inline">\(E[A+B] = E[A] + E[B]\)</span>): <span class="math display">\[
E[\hat{\boldsymbol{\beta}}] = E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}] + E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\boldsymbol{\varepsilon}]
\]</span></p></li>
<li><p>Analizamos cada término por separado:</p>
<ul>
<li>En el primer término, todo es constante excepto el operador de valor esperado, y <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\)</span> es la matriz identidad <span class="math inline">\(\mathbf{I}\)</span>. Por lo tanto, <span class="math inline">\(E[\mathbf{I}\boldsymbol{\beta}] = \boldsymbol{\beta}\)</span>.</li>
<li>En el segundo término, podemos sacar las constantes del valor esperado: <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T E[\boldsymbol{\varepsilon}]\)</span>.</li>
</ul></li>
<li><p>Aplicamos el supuesto de <strong>exogeneidad</strong> (o media del error nula), que establece que el valor esperado del término de error es cero: <span class="math inline">\(E[\boldsymbol{\varepsilon}] = \mathbf{0}\)</span>. <span class="math display">\[
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T E[\boldsymbol{\varepsilon}] = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{0} = \mathbf{0}
\]</span></p></li>
<li><p>Uniendo los resultados, concluimos: <span class="math display">\[
E[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta} + \mathbf{0} = \boldsymbol{\beta}
\]</span> Esto demuestra que el estimador MCO <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> es insesgado, ya que su valor esperado es el verdadero parámetro poblacional <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p></li>
</ol>
</details>
</section>
<section id="ejercicio-8-intervalos-de-confianza-vs.-predicción" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="ejercicio-8-intervalos-de-confianza-vs.-predicción"><span class="header-section-number">6.8</span> Ejercicio 8: Intervalos de Confianza vs.&nbsp;Predicción</h2>
<p>La fórmula para el intervalo de predicción para una nueva observación en regresión lineal simple es: <span class="math display">\[\hat{y}_0 \pm t_{\alpha/2, n-2} \cdot \sqrt{\text{MSE} \left( 1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}} \right)}\]</span> Explica el origen y el significado de cada uno de los <strong>tres términos</strong> que se encuentran dentro del paréntesis bajo la raíz cuadrada. ¿Qué fuente de incertidumbre representa cada término y por qué la suma de los tres es necesaria para un intervalo de predicción?</p>
<details>
<summary>
</summary>
<p>La fórmula cuantifica la incertidumbre total de predecir una <em>única</em> nueva observación. Esta incertidumbre proviene de dos fuentes: la incertidumbre sobre la posición de la verdadera línea de regresión y la variabilidad inherente de un punto individual alrededor de esa línea. Los tres términos dentro del paréntesis representan estas fuentes de varianza (escaladas por MSE, que estima <span class="math inline">\(\sigma^2\)</span>):</p>
<ol type="1">
<li><p><strong>Término <code>1</code></strong>: Esta es la componente más importante y la que distingue al intervalo de predicción. Representa la <strong>varianza del error aleatorio de la nueva observación</strong>, <span class="math inline">\(\text{Var}(\varepsilon_0) = \sigma^2\)</span>. Es la incertidumbre irreducible o inherente de un solo punto, que siempre se desviará de la media. Esta es la razón principal por la que un intervalo de predicción es siempre más ancho que uno de confianza.</p></li>
<li><p><strong>Término <code>1/n</code></strong>: Esta componente está relacionada con la <strong>incertidumbre en la estimación del intercepto <span class="math inline">\(\hat{\beta}_0\)</span></strong>. Representa la incertidumbre sobre la “altura” general de la línea de regresión. A medida que el tamaño de la muestra (<span class="math inline">\(n\)</span>) aumenta, nuestra confianza en la posición de la línea mejora, y este término de incertidumbre se hace más pequeño.</p></li>
<li><p><strong>Término <code>(x_0 - \bar{x})^2 / S_{xx}</code></strong>: Esta componente representa la <strong>incertidumbre debida a la estimación de la pendiente <span class="math inline">\(\hat{\beta}_1\)</span></strong>. La incertidumbre en la pendiente tiene un mayor impacto cuanto más nos alejamos del centro de los datos (<span class="math inline">\(\bar{x}\)</span>). Si predecimos en el punto medio de nuestros datos (<span class="math inline">\(x_0 = \bar{x}\)</span>), este término se anula. A medida que <span class="math inline">\(x_0\)</span> se aleja de <span class="math inline">\(\bar{x}\)</span>, el efecto de un pequeño error en la estimación de la pendiente se magnifica, ensanchando el intervalo.</p></li>
</ol>
<p>En resumen, los términos <code>1/n</code> y <code>(x_0 - \bar{x})^2 / S_{xx}</code> juntos cuantifican la incertidumbre sobre dónde está la <strong>línea de regresión verdadera</strong> (lo que cubre el intervalo de confianza). El término <code>1</code> añade la incertidumbre de <strong>un nuevo punto individual</strong> alrededor de esa línea.</p>
</details>
</section>
<section id="ejercicio-9-estimación-por-máxima-verosimilitud" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="ejercicio-9-estimación-por-máxima-verosimilitud"><span class="header-section-number">6.9</span> Ejercicio 9: Estimación por Máxima Verosimilitud</h2>
<p>Para un modelo de regresión logística, la función de log-verosimilitud es: <span class="math display">\[\ell(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right]\]</span> donde <span class="math inline">\(p_i = \frac{1}{1 + e^{-\mathbf{x}_i^T\boldsymbol{\beta}}}\)</span>. Deriva la <strong>ecuación de puntuación (score equation)</strong> para un coeficiente <span class="math inline">\(\beta_j\)</span> (es decir, calcula <span class="math inline">\(\frac{\partial \ell}{\partial \beta_j}\)</span>) y demuestra que se iguala a cero cuando <span class="math inline">\(\sum_{i=1}^{n} x_{ij}(y_i - p_i) = 0\)</span>. Interpreta el significado de esta condición final.</p>
<details>
<summary>
</summary>
<ol type="1">
<li><p>La función de log-verosimilitud para la regresión logística es: <span class="math display">\[
\ell(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[y_i \mathbf{x}_i^T\boldsymbol{\beta} - \log(1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}})\right]
\]</span></p></li>
<li><p>La ecuación de puntuación (<em>score equation</em>) se obtiene al calcular la primera derivada de la log-verosimilitud con respecto a un parámetro, en este caso <span class="math inline">\(\beta_j\)</span>. Debemos calcular <span class="math inline">\(\frac{\partial \ell}{\partial \beta_j}\)</span> y igualarla a cero.</p></li>
<li><p>La derivada de una suma es la suma de las derivadas, por lo que podemos analizar el término dentro del sumatorio para una observación <span class="math inline">\(i\)</span>: <span class="math display">\[
\frac{\partial}{\partial \beta_j} \left[y_i \mathbf{x}_i^T\boldsymbol{\beta} - \log(1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}})\right]
\]</span></p></li>
<li><p>La derivada del primer término, <span class="math inline">\(y_i \mathbf{x}_i^T\boldsymbol{\beta} = y_i(\beta_0 + \beta_1x_{i1} + \dots + \beta_jx_{ij} + \dots)\)</span>, con respecto a <span class="math inline">\(\beta_j\)</span> es simplemente <span class="math inline">\(y_i x_{ij}\)</span>.</p></li>
<li><p>La derivada del segundo término, <span class="math inline">\(\log(1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}})\)</span>, requiere la regla de la cadena. Sea <span class="math inline">\(u = 1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}}\)</span>. <span class="math display">\[
\frac{\partial}{\partial \beta_j} \log(u) = \frac{1}{u} \cdot \frac{\partial u}{\partial \beta_j} = \frac{1}{1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}}} \cdot \frac{\partial}{\partial \beta_j} (1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}})
\]</span> <span class="math display">\[
= \frac{1}{1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}}} \cdot (e^{\mathbf{x}_i^T\boldsymbol{\beta}} \cdot x_{ij}) = \left(\frac{e^{\mathbf{x}_i^T\boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^T\boldsymbol{\beta}}}\right) x_{ij}
\]</span></p></li>
<li><p>Reconocemos que el término entre paréntesis es la definición de la probabilidad <span class="math inline">\(p_i\)</span> en el modelo logístico (<span class="math inline">\(p_i = \frac{1}{1+e^{-\mathbf{x}_i^T\boldsymbol{\beta}}}\)</span>). Por lo tanto, la derivada del segundo término es <span class="math inline">\(p_i x_{ij}\)</span>.</p></li>
<li><p>Uniendo ambos resultados, la derivada para la observación <span class="math inline">\(i\)</span> es <span class="math inline">\(y_i x_{ij} - p_i x_{ij}\)</span>.</p></li>
<li><p>La ecuación de puntuación completa es la suma sobre todas las observaciones: <span class="math display">\[
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^{n} (y_i x_{ij} - p_i x_{ij}) = \sum_{i=1}^{n} x_{ij}(y_i - p_i)
\]</span></p></li>
<li><p>Los estimadores de máxima verosimilitud se encuentran al igualar esta ecuación a cero: <span class="math display">\[
\sum_{i=1}^{n} x_{ij}(y_i - p_i) = 0
\]</span></p></li>
</ol>
<p><strong>Interpretación:</strong> Esta condición final significa que los estimadores de máxima verosimilitud se encuentran cuando los residuos del modelo (<span class="math inline">\(y_i - p_i\)</span>, la diferencia entre lo observado y la probabilidad predicha) son <strong>ortogonales</strong> (no están correlacionados) a los predictores <span class="math inline">\(x_{ij}\)</span>. Esto es análogo a las ecuaciones normales de MCO y significa que el modelo ha extraído toda la información linealmente asociable a los predictores, no quedando ningún patrón relacionado con ellos en los errores.</p>
</details>
</section>
<section id="ejercicio-10-el-coeficiente-de-regresión-parcial" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="ejercicio-10-el-coeficiente-de-regresión-parcial"><span class="header-section-number">6.10</span> Ejercicio 10: El Coeficiente de Regresión Parcial</h2>
<p>El texto afirma que el coeficiente <span class="math inline">\(\hat{\beta}_j\)</span> de una regresión múltiple puede entenderse como el coeficiente de una regresión simple entre dos conjuntos de residuos. Explica con detalle este concepto de <strong>regresión parcial</strong>. ¿Qué se está “parcializando” o “eliminando” de la variable respuesta <span class="math inline">\(Y\)</span> y del predictor <span class="math inline">\(X_j\)</span> antes de calcular su relación? ¿Por qué este concepto es fundamental para entender la interpretación <em>ceteris paribus</em>?</p>
<details>
<summary>
</summary>
<p>El concepto de regresión parcial es fundamental para entender la interpretación <em>ceteris paribus</em> de un coeficiente en regresión múltiple. Afirma que el coeficiente <span class="math inline">\(\hat{\beta}_j\)</span> del predictor <span class="math inline">\(X_j\)</span> en un modelo múltiple es matemáticamente idéntico a la pendiente de una regresión simple entre dos conjuntos de residuos.</p>
<p>El proceso de “parcialización” consiste en eliminar la influencia de todos los demás predictores (denotados como <span class="math inline">\(X_{-j}\)</span>) tanto de la variable respuesta <span class="math inline">\(Y\)</span> como del predictor de interés <span class="math inline">\(X_j\)</span>.</p>
<ol type="1">
<li><p><strong>Parcialización de Y:</strong> Se ajusta un modelo de regresión de <span class="math inline">\(Y\)</span> en función de todos los demás predictores: <span class="math inline">\(Y \sim X_{-j}\)</span>. Los residuos de este modelo, <span class="math inline">\(e_{Y|X_{-j}}\)</span>, representan la parte de la variabilidad de <span class="math inline">\(Y\)</span> que <strong>no puede ser explicada</strong> por el resto de variables del modelo. Es la “información única” de Y.</p></li>
<li><p><strong>Parcialización de <span class="math inline">\(X_j\)</span>:</strong> Se ajusta un modelo de regresión de <span class="math inline">\(X_j\)</span> en función de todos los demás predictores: <span class="math inline">\(X_j \sim X_{-j}\)</span>. Los residuos de este modelo, <span class="math inline">\(e_{X_j|X_{-j}}\)</span>, representan la parte de la variabilidad de <span class="math inline">\(X_j\)</span> que es <strong>única</strong> y no está correlacionada con el resto de variables. Es la “información única” que <span class="math inline">\(X_j\)</span> aporta.</p></li>
<li><p><strong>Relación entre los residuos:</strong> Si ahora ajustamos una regresión lineal simple entre estos dos conjuntos de residuos: <span class="math display">\[
e_{Y|X_{-j}} \sim e_{X_j|X_{-j}}
\]</span> La pendiente de esta regresión simple es <strong>exactamente igual</strong> al coeficiente de regresión múltiple <span class="math inline">\(\hat{\beta}_j\)</span> del modelo original completo.</p></li>
</ol>
<p><strong>Conclusión:</strong> Esto demuestra que <span class="math inline">\(\hat{\beta}_j\)</span> no mide la relación “bruta” entre Y y <span class="math inline">\(X_j\)</span>, sino la relación entre la parte de Y que no es explicada por los otros predictores y la parte de <span class="math inline">\(X_j\)</span> que es única. Es la asociación “limpia” entre Y y <span class="math inline">\(X_j\)</span> después de haber controlado estadísticamente por la influencia de todas las demás variables en el modelo. Esto es, precisamente, la formalización matemática del principio <em>ceteris paribus</em>.</p>


</details></section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiado");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiado");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./tema5_glm_soluciones.html" class="pagination-link" aria-label="Modelos de Regresión Generalizada">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelos de Regresión Generalizada</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>