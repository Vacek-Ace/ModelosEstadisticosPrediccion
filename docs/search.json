[
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "5.1 Modelo con un factor\nEl modelo de ANOVA de un factor se utiliza cuando se estudia el efecto de un solo factor (variable independiente) en una variable dependiente continua. Este modelo permite comparar las medias de varios grupos para determinar si existen diferencias significativas entre ellos.\nTenemos una variable aleatoria \\(Y\\) que toma valores reales y una variable cualitativa o factor \\(X\\) con \\(k\\) niveles \\(1,2,\\ldots,i,\\ldots,k\\). La variable \\(Y\\) toma valores \\(Y_{ij}, j=1,\\ldots,n_i\\) en el nivel \\(i\\) del factor \\(X\\), siendo \\(n_i\\) el número de observaciones en el nivel \\(i\\) del factor \\(X\\).\nTenemos los siguientes supuestos:\nEscribimos el modelo como sigue: \\[\n  Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\] Donde:\nLa suma de las diferencias al cuadrado de cada dato respecto a la media general se calcula como sigue: \\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2        \n\\] donde \\(\\bar{Y}_{..}\\) es la media general de todas las observaciones.\nTeniendo en cuenta que: \\[\nY_{ij} - \\bar{Y}_{..}= Y_i  + \\epsilon_{ij} - \\bar{Y}_{..}\n\\] Podemos descomponer la suma de cuadrados, como sigue:\n\\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2=  \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2+\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2      \n\\]\nDonde: La varianza entre grupos se calcula como la suma de las diferencias al cuadrado de las medias de los grupos respecto a la media general, ponderada por el tamaño de los grupos: \\[\n\\text{SSB} = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2\n\\] donde \\(\\bar{Y}_i\\) es la media del grupo \\(i\\).\nAdemás, la varianza dentro de los grupos, se calcula como la suma de las diferencias al cuadrado de cada dato respecto a la media de su grupo se obtiene como: \\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\] Esto es, se descompone la variabilidad total de los datos en dos componentes, SSB que refleja la diferencia de cada grupo respecto a la media global y SSW que refleja la variabilidad intrínseca dentro de cada grupo: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW}\n\\]\nCálculo del Estadístico F: \\[\nF = \\frac{\\text{Varianza Entre Grupos (MSB)}}{\\text{Varianza Dentro de los Grupos (MSW)}}\n\\] Donde:\nEsto es: \\[\nF=\\frac{SSB/df_B}{SSW/df_W}\n\\] donde:\nUna vez se dispone de toda esta información, es común representarla en forma de tabla, en la llamada Tabla ANOVA:\nEl estadístico de prueba \\(F \\sim F_{df_B,df_W}\\) bajo la hipótesis nula de igualdad de medias.\nEl \\(p-valor\\) se obtiene a partir de la distribución \\(F\\), considerando los grados de libertad de numerador y denominador. Esto es: \\[\np-valor=P(F_{df_b,df_W}&gt;F_{muestral})\n\\]\nComo en otros contrastes, si el \\(p-valor\\) p es menor que el nivel de significancia (\\(\\alpha\\)), se rechaza la hipótesis nula, concluyendo que al menos una de las medias de los grupos es diferente.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-un-factor",
    "href": "anova.html#modelo-con-un-factor",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "Hipótesis:\n\nHipótesis Nula (\\(H_0\\)): Todas las medias de los grupos son iguales (\\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)).\nHipótesis Alternativa (\\(H_a\\)): Al menos una de las medias de los grupos es diferente.\n\n\n\n\n\nNormalidad: Las distribuciones de las poblaciones de las que provienen las muestras son normales.\nHomogeneidad de varianzas: Las varianzas de las poblaciones son iguales.\nIndependencia: Las observaciones son independientes entre sí.\n\n\n\n\\(Y_{ij}\\) es la observación \\(j\\)-ésima del grupo \\(i\\)-ésimo.\n\\(\\mu\\) es la media general.\n\\(\\epsilon_{ij}\\) es el término de error aleatorio.\n\\(\\tau_i\\) es el efecto del grupo i-ésimo en la media de la variable respuesta \\(Y\\). Esto es, cuánto aumenta o disminuye la media de \\(Y\\) por pertenecer la observación a la categoría \\(i\\). De modo que podemos llamar \\[\nY_i=\\mu+\\tau_i\n\\] al efecto medio del grupo \\(i\\)-esimo.\n\n\n\n\n\n\n\n\nMSB (Mean Square Between): Media cuadrática entre grupos.\nMSW (Mean Square Within): Media cuadrática dentro de los grupos.\n\n\n\n\\(df_B=k-1\\) son los grados de libertad entre los grupos\n\\(df_W=N-k\\) son los grado sd elibertado dentro de los grupos, siendo \\(N\\) el número total de observaciones.\n\n\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDisferencias entre grupos\nSSB\nk-1\nMSB\n\n\nDiferencias dentro de los grupos, Residual o Error\nSSW\nN-k\nMSW\n\n\nTotal\nSST\nN-1\n\n\n\n\n\n\n\n\n\n\nPara el futuro\n\n\n\nLa proporción de variabilidad explicada por los grupos se calcula como: \\[\nR^2=1-SSW/SST\n\\] Este valor, será muy importante en la asignatura de Regresión del Grado en Ciencia e Ingeniería de Datos.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de un factor\n\n\n\n\n\nVamos a realizar un ejemplo completo de ANOVA de un factor, donde calcularemos todos los pasos del contraste, incluidos el valor de F y el p-valor.\nSupongamos que tenemos tres tratamientos (A, B y C) y sus correspondientes muestras de datos son:\n\nGrupo A: \\([5, 7, 6, 9, 6]\\)\nGrupo B: \\([8, 12, 9, 11, 10]\\)\nGrupo C: \\([14, 10, 13, 15, 12]\\)\n\nNuestro objetivo es determinar si existe una diferencia significativa entre las medias de estos tres grupos.\nEn primer lugar calculamos la media de cada grupo y la media general:\n\nMedia de Grupo A (\\(\\bar{Y}_A\\)): \\[\n\\bar{Y}_A = \\frac{5 + 7 + 6 + 9 + 6}{5} = \\frac{33}{5} = 6.6\n\\]\nMedia de Grupo B (\\(\\bar{Y}_B\\)): \\[\n\\bar{Y}_B = \\frac{8 + 12 + 9 + 11 + 10}{5} = \\frac{50}{5} = 10.0\n\\]\nMedia de Grupo C (\\(\\bar{Y}_C\\)): \\[\n\\bar{Y}_C = \\frac{14 + 10 + 13 + 15 + 12}{5} = \\frac{64}{5} = 12.8\n\\]\nMedia General (\\(\\bar{Y}\\)): \\[\n\\bar{Y} = \\frac{6.6 + 10.0 + 12.8}{3} = \\frac{29.4}{3} \\approx 9.8\n\\]\n\nCalculamos la Suma de Cuadrados entre Grupos (SSB)\n\\[\n\\text{SSB} = n_A (\\bar{Y}_A - \\bar{Y})^2 + n_B (\\bar{Y}_B - \\bar{Y})^2 + n_C (\\bar{Y}_C - \\bar{Y})^2\n\\] donde \\(n_A = n_B = n_C = 5\\) (número de observaciones en cada grupo). Fíjate que el número de observaciones en cada grupo podría ser diferente. En este ejemplo, son iguales.\n\\[\n\\text{SSB} = 5 (6.6 - 9.8)^2 + 5 (10.0 - 9.8)^2 + 5 (12.8 - 9.8)^2 =96.4\n\\]\nA continuación calculamos la Suma de Cuadrados Dentro de los Grupos (SSW)\n\\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\]\nPara cada grupo, calculamos la suma de las diferencias al cuadrado entre cada dato y la media del grupo:\n\nGrupo A: \\[\n(5 - 6.6)^2 + (7 - 6.6)^2 + (6 - 6.6)^2 + (9 - 6.6)^2 + (6 - 6.6)^2 = 9.2\n\\]\nGrupo B: \\[\n(8 - 10.0)^2 + (12 - 10.0)^2 + (9 - 10.0)^2 + (11 - 10.0)^2 + (10 - 10.0)^2 = 10.0\n\\]\nGrupo C: \\[\n(14 - 12.8)^2 + (10 - 12.8)^2 + (13 - 12.8)^2 + (15 - 12.8)^2 + (12 - 12.8)^2 = 14.8\n\\]\n\nEntonces, la Suma de Cuadrados Dentro de los Grupos (SSW) es: \\[\n\\text{SSW} = 9.2 + 10.0 + 14.8 = 34.0\n\\]\nTenemos, por tanto que la Suma Total de Cuadrados (SST) es: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW} = 96.4 + 34.0 = 130.4\n\\]\nPara realizar el contraste, necesitamos calcular los Grados de Libertad del estadístico:\n\nGrados de libertad entre los grupos (\\(\\text{df}_B = k-1=3-1=2\\)):\nGrados de libertad dentro de los grupos (\\(\\text{df}_W=N-k=15-3=12\\)):\n\nCalcularemos las varianzas como sigue:\n\nVarianza entre los grupos (MSB): \\[\n\\text{MSB} = \\frac{\\text{SSB}}{\\text{df}_B} = \\frac{96.4}{2} = 48.2\n\\]\nVarianza dentro de los grupos (MSW): \\[\n\\text{MSW} = \\frac{\\text{SSW}}{\\text{df}_W} = \\frac{34.0}{12} \\approx 2.8333\n\\]\n\nYa estamos en disposición de calcular el estadístico de contraste : \\[\nF = \\frac{\\text{MSB}}{\\text{MSW}} = \\frac{48.2}{2.8333} \\approx 17.01\n\\]\nEl p-valor se obtiene utilizando la distribución \\(F\\) con \\(\\text{df}_B = 2\\) y \\(\\text{df}_W = 12\\). Para este ejemplo, podemos utilizar software estadístico o tablas de distribución F.\nUsando R:\n\npf(17.01, df1 = 2, df2 = 12, lower.tail = FALSE)\n\n[1] 0.0003143459\n\n\nEl p-valor es muy pequeño, mucho menor que el grado de significancia \\(\\alpha=0.05\\), indicando una diferencia significativa entre los grupos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "href": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "title": "5  Análisis de varianza",
    "section": "5.2 Modelo con dos factores con y sin interacción",
    "text": "5.2 Modelo con dos factores con y sin interacción\nEl modelo de ANOVA de dos factores se utiliza cuando se estudian dos factores simultáneamente para evaluar su efecto individual y conjunto en una variable dependiente. Podemos verlo como una generalización del caso de ANOVA con un único factor. Este modelo es más complejo y permite entender no solo los efectos principales de cada factor, sino también si existe una interacción entre ellos.\nSean A y B dos factores que se desean estudiar, con \\(m_A\\) y \\(m_B\\) niveles. Trabajaremos con las siguientes hipótesis nulas:\n\nHipótesis:\n\nHipótesis Nula para los efectos principales (\\(H_0\\)):\n\nNo hay efecto del primer factor.\nNo hay efecto del segundo factor.\n\nHipótesis Nula para la interacción (\\(H_0\\)): No hay interacción entre los dos factores.\n\n\nModelo sin Interacción:\n\\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk} \\]\nDonde:\n\n\\(Y_{ijk}\\) es la observación \\(k\\)-ésima del nivel \\(j\\)-ésimo del factor B y nivel \\(i\\)-ésimo del factor A. - \\(\\mu\\) es la media general.\n\\(\\alpha_i\\) es el efecto del nivel \\(i\\)-ésimo del factor A.\n\\(\\beta_j\\) es el efecto del nivel \\(j\\)-ésimo del factor B. - \\(\\epsilon_{ijk}\\) es el término de error aleatorio.\n\nEn este caso, la tabla ANOVA queda como sigue:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A-m_B+1\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de cada factor se calcula el estadístico \\(F\\) particular para cada uno de ellos como sigue: \\[\nF_A=\\frac{MSB_A}{MSW} \\sim F_{m_A-1,N-m_A-m_B+1}\n\\] y \\[\nF_B=\\frac{MSB_B}{MSW}\\sim F_{m_B-1,N-m_A-m_B+1}\n\\] A partir de estos estadísticos de prueba podemos contrastar las hipótesis nulas de no existencia de efectos asociados a los factores \\(A\\) y \\(B\\) respectivamente.\nModelo con Interacción: \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk} \\]\nDonde: \\((\\alpha\\beta)_{ij}\\) representa el efecto de interacción entre el nivel \\(i\\)-ésimo del factor A y el nivel \\(j\\)-ésimo del factor B.\nEn este caso, la tabla ANOVA añade el factor de interacción:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nDiferencias debidas la interacción\n\\(SSB_{AB}\\)\n\\((m_A-1)*(m_B-1)\\)\n\\(MSB_{AB}\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A*m_B\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de la interacción se calcula el estadístico \\(F\\) correspondiente:\n\\[\nF_{AB}=\\frac{MSB_{AB}}{MSW}\\sim F_{(m_A-1)*(m_B-1),N-m_A*m_B}\n\\]\nA partir de este estadísticos de prueba podemos contrastar la hipótesis nula de no existencia de interacción entre los dos factores \\(A\\), \\(B\\). Si podemos rechazar esa hipótesis, es decir, si existe interacción entre los factores, entonces hemos terminado. Es decir, no podemos eliminar ningún factor del modelo. En cambio, si no rechazamos la hipótesis nula, es decir, si no existe interacción entre los factores, podemos eliminar dicho efecto (la interacción) del modelo y pasar a un modelos sin interacción como el anteriormente descrito.\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores con interacción\n\n\n\n\n\nSupongamos que estamos estudiando el efecto de dos factores sobre el rendimiento de los estudiantes: el método de enseñanza (con dos niveles: Tradicional y Experimental) y el tipo de material de estudio (con tres niveles: Libro, Video, y Online). Queremos saber si estos factores, y su posible interacción, tienen un efecto significativo en el rendimiento.\n\nrendimiento &lt;- c(85, 88, 90, 83, 87, 85, 86, 89, 91, 84, 88, 87,\n                 78, 79, 80, 76, 77, 75, 78, 79, 81, 76, 77, 76,\n                 90, 92, 91, 93, 90, 89, 91, 92, 91, 93, 92, 91)\nmetodo &lt;- factor(rep(c(\"Tradicional\", \"Experimental\"), each=18))\nmaterial &lt;- factor(rep(c(\"Libro\", \"Video\", \"Online\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(rendimiento, metodo, material)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"material\", y = \"rendimiento\", color = \"metodo\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nAhora que tenemos nuestros datos, vamos a realizar el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(rendimiento ~ metodo * material, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmetodo           1   81.0    81.0   21.83 5.88e-05 ***\nmaterial         2  309.7   154.9   41.73 2.16e-09 ***\nmetodo:material  2  771.2   385.6  103.90 3.26e-14 ***\nResiduals       30  111.3     3.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLos resultados del ANOVA se interpretan observando los \\(p-valores\\) para cada uno de los componentes del modelo:\n\nmetodo: Efecto principal del método de enseñanza.\nmaterial: Efecto principal del tipo de material de estudio.\nmetodo:material: Interacción entre el método de enseñanza y el tipo de material de estudio.\n\nEl \\(p-valor\\) asociado a la interacción es menor que \\(0.05\\), lo que indica que la interacción entre el método de enseñanza y el tipo de material de estudio es significativa y por lo tanto no debe ser eliminada del modelo.\nEfectivamente, viendo la figura anterior concluimos que el rendimiento depende de la interacción entre los dos factores. Así, por ejemplo, cuando el matería es proporcionado de modo “online” o en “vídeo” el rendimiento es más elevado en el método experimental que en el método tradicional. La diferencia entre los dos métodos es especialmente notable cuando el material es “online”. Sin embargo, cuando el material se ofrece en modo “libro” el método tradicional ofrece mejores resultados que el método experimental.\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores no significativos\n\n\n\n\n\nSupongamos que estamos estudiando el efecto del tipo de fertilizante (con dos niveles: \\(A\\) y \\(B\\)) y el tipo de riego (con tres niveles: Goteo, Aspersión, Manual) sobre el crecimiento de las plantas.\nEn primer lugar observamos los datos:\n\n# Crear datos simulados\ncrecimiento &lt;- c(20, 21, 23, 22, 24, 25, 26, 27, 28, 29, 30, 31,\n                 22, 23, 25, 24, 26, 27, 28, 29, 30, 31, 32, 33,\n                 21, 23, 22, 24, 23, 25, 26, 28, 27, 29, 28, 30)\nfertilizante &lt;- factor(rep(c(\"A\", \"B\"), each=18))\nriego &lt;- factor(rep(c(\"Goteo\", \"Aspersión\", \"Manual\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(crecimiento, fertilizante, riego)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"riego\", y = \"crecimiento\", color = \"fertilizante\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante * riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfertilizante        1   36.0   36.00  12.000  0.00162 ** \nriego               2    3.5    1.75   0.583  0.56424    \nfertilizante:riego  2  283.5  141.75  47.250 5.36e-10 ***\nResiduals          30   90.0    3.00                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEl \\(p-valor\\) asociado a la interaccióni es mayor que \\(0.05\\) lo, lo que indica que la interacción entre el tipo de fertilizante y el tipo de riego no es significativa y debe de ser eliminada del modelo.\nPor tanto, obtenemos la tabla ANOVA para los dos factores sin interacción:\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante + riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1   36.0   36.00   3.084 0.0886 .\nriego         2    3.5    1.75   0.150 0.8614  \nResiduals    32  373.5   11.67                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUna vez eliminado el efecto de la interacción del modelo, podemos observar como ninguno de los dos factores para estadísticamente significativo, puesto que los \\(p-valores\\) asociados son mayores que \\(0.05\\). Sin embargo, hemos de actuar con cautela. Veamos el modelo cuando se elimina el factor menos significativo riego:\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(crecimiento ~ fertilizante, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1     36   36.00   3.247 0.0804 .\nResiduals    34    377   11.09                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAl nivel de significancia estadística de \\(0.05\\) podríamos decir que el factor fertilizante no es estadísticamente significativo y que ninguno de los dos factores influye en el crecimiento de las planteas.\nAhora bien, si tuvieras que elegir un método de riego y un fertilizando, ¿cuál recomendarías al cliente? ¿por qué?\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores sin interacción\n\n\n\n\n\nSupongamos que estamos estudiando del tiempo de estudio y del tipo de dieta en el rendimiento académico de un grupo de estudiantes. El tiempo de estudio tiene \\(3\\) niveles (Alto, Medio y Bajo). El tipo de dieta tiene \\(3\\) niveles (Normal, Vegana y Vegetariana).\nEn primer lugar observamos los datos:\n\n# Datos\ndatos &lt;- data.frame(\n  Tiempo_estudio = factor(rep(c(\"1. Alto\", \"2. Medio\", \"3. Bajo\"), each = 9)),\n  Tipo_dieta = factor(rep(c(\"Vegetariana\", \"Normal\", \"Vegana\"), times = 9)),\n  Rendimiento = c(81, 71, 90, 75, 84, 81, 91, 100, 98, # Datos para Nivel1 de Factor1\n                60, 83, 70, 54, 65, 73, 82, 92, 73, # Datos para Nivel2 de Factor1\n                47, 63, 44, 61, 55, 52, 73, 63, 53) # Datos para Nivel3 de Factor1\n)\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\", color = \"Tipo_dieta\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\",\"#c94545\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio * Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nTiempo_estudio             2   3765  1882.3  17.410 6.2e-05 ***\nTipo_dieta                 2    169    84.6   0.782   0.472    \nTiempo_estudio:Tipo_dieta  4    465   116.1   1.074   0.398    \nResiduals                 18   1946   108.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa interacción entre los dos factores no es estadísticamente significativa a nivel \\(\\alpha=0.05\\) y por lo tanto eliminamos dicha fuente de variabilidad del modelo.\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio + Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3  17.178 3.21e-05 ***\nTipo_dieta      2    169    84.6   0.772    0.474    \nResiduals      22   2411   109.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPodemos observar que el tipo de dieta no es un factor significativo en el rendimiento académico. Por tanto, eliminamos dicho factor del modelo.\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio , data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3   17.51 2.04e-05 ***\nResiduals      24   2580   107.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn cambio, el tiempo de estudio sí es un factor determinante en el rendimiento académico. Su \\(p-valor\\) asociado es claramente inferior al nivel de significancia \\(0.05\\).\nPodemos visualizar el resultado:\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\",\n       add = c(\"mean_se\", \"dotplot\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#comparaciones-múltiples",
    "href": "anova.html#comparaciones-múltiples",
    "title": "5  Análisis de varianza",
    "section": "5.3 Comparaciones múltiples",
    "text": "5.3 Comparaciones múltiples\nEn el último ejemplo del apartado anterior hemos determinado que un factor con \\(3\\) niveles era estadísticamente significativo. Es decir, podemos rechazar la siguiente hipótesis nula:\n\\[H_O: \\mu_1=\\mu_2\\ldots = \\mu_k\\] (con \\(k\\) igual al número de niveles en el factor). Podemos plantearnos la pregunta siguiente: ¿En qué niveles del factor se encuentran las principales diferencias? Es decir, qué hipotesis (una o varias) de las siguientes son rechazadas: \\[H_O: \\mu_1=\\mu_2\\] \\[H_O: \\mu_1=\\mu_3\\] \\[\\ldots\\] \\[H_O: \\mu_{k-1} = \\mu_k\\]\nEn un ejemplo con \\(k=3\\) niveles en el factor, es posible plantear \\(3\\) contrastes de igualdad de medias, como los estudiados en capítulos anteriores. En un ejemplo con \\(k\\) niveles en el factor, es posible plantear \\(k*(k-1)/2\\) posibles contrastes de igualda de medias. Sin embargo, si realizamos todos estos contrastes, aumenta la probabilidad de cometer errores de tipo I (rechazar incorrectamente la hipótesis nula). Este fenómeno se conoce como el problema de las pruebas múltiples.\nComo hemos venido viendo, cuando realizamos una sola prueba de hipótesis (por ejemplo, una prueba \\(t\\) de Student o un ANOVA), generalmente establecemos un nivel de significancia predeterminado, como \\(\\alpha = 0.05\\). Esto significa que estamos dispuestos a aceptar una probabilidad de error de tipo I del \\(5\\%\\), es decir, hay un \\(5\\%\\) de probabilidad de rechazar incorrectamente la hipótesis nula cuando es verdadera.\nSin embargo, cuando realizamos múltiples pruebas de hipótesis, la probabilidad acumulada de cometer al menos un error de tipo I aumenta significativamente con cada prueba adicional. Por ejemplo, si realizamos \\(10\\) pruebas de hipótesis independientes, cada una con un nivel de significancia de \\(\\alpha = 0.05\\), la probabilidad de cometer al menos un error de tipo I aumenta a más del \\(40\\%\\) (\\(1-(1-0.05)^{10}\\approx 0.401\\).\nExiste una solución, las comparaciones múltiples necesarias para controlar este aumento en el riesgo de error. Existen varios métodos para controlar el problema de las pruebas múltiples, como los ajustes de Bonferroni, Holm-Bonferroni, Holm, Hochberg, Benjamini-Hochberg (FDR), entre otros. Estos métodos controlan la tasa global de error de tipo I para todas las comparaciones realizadas, manteniendo un nivel de significancia general específico.\n\n5.3.1 Método de Bonferroni\nEl método de Bonferroni es una técnica comúnmente utilizada para corregir el problema de las comparaciones múltiples y controlar el riesgo de error de tipo I. Este método es relativamente simple y conservador, lo que lo hace popular en muchas aplicaciones estadísticas.\nLa idea principal detrás del método de Bonferroni es ajustar el nivel de significancia individual para cada prueba de hipótesis realizada. En lugar de utilizar un nivel de significancia estándar (por ejemplo, \\(\\alpha = 0.05\\)), dividimos el nivel de significancia global deseado (generalmente \\(0.05\\)) por el número total de pruebas realizadas (\\(m\\)): \\[\n\\alpha' = \\frac{\\alpha}{m}\n\\] Esta división produce un nivel de significancia más estricto (\\(\\alpha'\\)) para cada prueba individual, lo que ayuda a controlar el riesgo global de error de tipo I.\nUtilizamos el nivel de significancia individual ajustado para cada prueba de hipótesis. Si el \\(p-valor\\) de una prueba es menor que el nivel de significancia ajustado, rechazamos la hipótesis nula de la prueba.\nEl método de Bonferroni es fácil de entender e implementar, y proporciona un control conservador sobre el error de tipo I en comparaciones múltiples. No obstante puede ser un método demasiado conservador en situaciones donde se realizan muchas comparaciones, lo que puede resultar en una pérdida de potencia estadística. Además, no tiene en cuenta la correlación entre las pruebas realizadas.\n\n\n\n\n\n\nEjemplo. Comparaciones múltiples\n\n\n\n\n\nContinuamos con el ejemplo anterior anterior sobre la influencia del tiempo de estudio en el rendimiento académico. Hemos visto que existe una relación entre ambos factores. En otras palabras, hemos rechazado la siguiente hipótesis nula:\n\\[\nH_0: \\mu_{Alto}=\\mu_{Medio}=\\mu_{Bajo}\n\\] Pero, ¿dónde se encuentran las diferencias relevantes?. Calculamos las tres medias muestrales\n\nmean(datos[datos$Tiempo_estudio==\"1. Alto\",]$Rendimiento)\n\n[1] 85.66667\n\nmean(datos[datos$Tiempo_estudio==\"2. Medio\",]$Rendimiento)\n\n[1] 72.44444\n\nmean(datos[datos$Tiempo_estudio==\"3. Bajo\",]$Rendimiento)\n\n[1] 56.77778\n\n\nAplicamos el método de Bonferroni, y obtenemos:\n\npairwise.t.test(datos$Rendimiento, g=datos$Tiempo_estudio,p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  datos$Rendimiento and datos$Tiempo_estudio \n\n         1. Alto 2. Medio\n2. Medio 0.037   -       \n3. Bajo  1.3e-05 0.011   \n\nP value adjustment method: bonferroni \n\n\nDado que todos los \\(p-valores\\) ajustados son menores que \\(0.05\\), podemos rechazar las \\(3\\) hipótesis nulas. Es decir, rechazamos que el rendimiento sea el mismo para los diferentes niveles de tiempos de estudio.\nY en R podemos pintarlo como sigue (aquí empleamos otro método de corrección diferente al de Bonferroni, revísalo)\n\ncomparaciones_mult &lt;- TukeyHSD(anova_result)\nplot(comparaciones_mult)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Ciencia de datos\nLa última revolución asociada a la IA ha estado enmarcada por el crecimiento en el uso del Aprenzaje Automático dentro del contexto de la ciencia de datos (Kelleher, Mac Namee, y D’arcy 2020).\nPero, ¿qué áreas, métodos y técnicas están implicados en la ciencia de datos?. En primer lugar, presentemos los aspectos teórico y prácticos que sustentan un proyecto real de ciencia de datos. Para ellos recurrimos a la Figura Figura 1.1 (a) que representa el clásico diagrama de la ciencia de datos, como una disciplina en la intersección de tres aspectos fundamentales. Para saber más sobre estos aspectos, desplegad los paneles siguientes:\nLa Figura Figura 1.1 (b) basada en CRISP-DM: “Cross Industry Standard Process for Data Mining” (Wirth y Hipp 2000) presenta el ciclo de vida que todo proyecto de ciencia de datos debería seguir. El inicio del proyecto viene dado por la definición de los objetivos de la organización. A continuación, se recogen y gestionan los datos. Como siguiente paso, se desarrollan y evalúan algoritmos matemáticos sobre los datos. Los resultados de estos modelos se presentan a los expertos en el dominio de aplicación para su posterior integración dentro de la organización. Nótese que el proyecto puede tener varias iteraciones, volviendo a alguna de las etapas anteriores siempre que una etapa posterior así lo requiera. Para saber más detalles sobre estas etapas investigad los paneles siguientes:\nEn este libro vamos a tratar en profundidad la Inferencia Estadística. Pero, ¿qué relación tiene con la Estadística. Vamos a verlo.\nLa estadística es la ciencia que se encarga de recolectar, organizar, analizar e interpretar datos para tomar decisiones informadas. Su objetivo principal es comprender y describir la variabilidad inherente en los datos y utilizar esta comprensión para hacer predicciones y tomar decisiones bajo condiciones de incertidumbre. La estadística se divide en dos ramas principales:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#ciencia-de-datos",
    "href": "intro.html#ciencia-de-datos",
    "title": "1  Introducción",
    "section": "",
    "text": "Ciencia de datos\n\n\n\nLa ciencia de datos es un área interdisciplinar que abarca un conjunto de principios, problemas, definiciones, algoritmos y procesos cuyo objetivo es extraer conocimiento no obvio y útil a partir de un conjunto de datos.\n\n\n\n\n\n\n\n\n\nMatemáticas y Estadística\n\n\n\n\n\nConocimientos de Matemáticas, y más concretamente de Estadística, son necesarios para analizar correctamente los datos disponibles. Conceptos como intervalo de confianza, histograma de frecuencias, contraste de hipótesis, espacio de características, métrica, hiperplano separador, error de clasificación, p_valor, etc. han de formar parte del conocimiento de todo científico de datos. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Matemáticas y Estadística. Un buen libro de referencia para dominar los conceptos fundamentales en el ámbito matemático y estadístico que son necesarios en ciencia de datos es (Hastie et al. 2009). Además disponéis de esta versión online (James et al. 2013), similar pero más enfocada al análisis de datos. ¿Lo conocías ya?\n\n\n\n\n\n\n\n\n\nCiencias de la Computación\n\n\n\n\n\nEstudio del diseño y la arquitectura de los ordenadores y su aplicación en el campo de la ciencia y la tecnología, incluyendo el hardware, el software y las redes de comunicación. Un experto en ciencias de la computación ha de dominar lenguajes de programación como Python, JavaScript, C++, así como los elementos fundamentales que hacen que estos lenguajes funcionen. Algunas referencias útiles para estudiar estos lenguajes son (Hao y Ho 2019), (Osmani 2012) y (Oualline 2003). De igual modo, el científico de datos, ha de conocer ámbitos como los diferentes sistemas operativos, redes, seguridad, algoritmos y arquitectura de ordenadores. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Ciencias de la Computación.\n\n\n\n\n\n\n\n\n\nConocimiento del Dominio\n\n\n\n\n\nRepresenta el problema que deseamos estudiar, la organización que lo proporciona y su dominio de aplicación. Existen casos de éxito de la ciencia de datos en prácticamente todos los dominios de interés que podamos mencionar: medicina, ciudades inteligentes, energía, telecomunicaciones, finanzas, seguros, ganadería, agricultura, ciencias sociales, ciberseguridad, etc. Un equipo de ciencia de datos ha de contar con uno o varios expertos en el dominio de aplicación. Estos expertos han de implicarse, fuertemente, en el problema que se quiere resolver. En (Kelleher, Mac Namee, y D’arcy 2020) podéis encontrar ejemplos muy interesantes de cómo la ciencia de datos es aplicada en diferentes dominios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Fundamentos\n\n\n\n\n\n\n\n\n\n\n\n(b) Aplicaciones\n\n\n\n\n\n\n\nFigura 1.1: Ciencia de datos\n\n\n\n\n\n\n\n\n\n\nDefinir objetivos\n\n\n\n\n\nEntender el negocio es el primer paso en el proceso. Con ayuda de expertos en el dominio, se definen las preguntas a responder con el proyecto (definición de objetivos de la entidad responsable del proyecto). Una vez comprendido el negocio se designa una solución analítica para abordar el problema. En esta etapa las reuniones entre los matemáticos, informáticos y los expertos del dominio (habitualmente trabajadores con grandes conocimiento del problema que se trata de abordar) son frecuentes, necesarias y (casi) nunca, suficientes.\n\n\n\n\n\n\n\n\n\nObtener, preparar y gestionar los datos\n\n\n\n\n\nMediante técnicas informáticas, se recopilan y se preparan los datos para su posterior análisis. Podremos hablar de Big Data si los datos se caracterizan por su volumen, variedad o velocidad de procesamiento. Todo proceso de Ciencia de Datos es un proceso de aprendizaje en torno al dato. Las preguntas no surgen de los datos, pero se necesitan datos para responderlas. Es esta la etapa en la que trataremos una parte fundamental de todo el proceso: el análisis exploratorio de datos. Podrás estudiar más sobre cómo preparar los datos para etapas posteriores en el tema 3 de este libro.\n\n\n\n\n\n\n\n\n\nConstruir un modelo\n\n\n\n\n\nA través de métodos matemáticos y estadísticos se estudian y analizan los datos, se construyen algoritmos y se aplican modelos. Es la etapa asociada a los modelos de ML que trataremos en los temas 5,7 y 8.\n\n\n\n\n\n\n\n\n\nEvaluar y criticar el modelo\n\n\n\n\n\nSe definen medidas de rendimiento de los modelos que permitan su evaluación tanto por parte del desarrollador como por parte del cliente. Trataremos estas medidas en el tema 6 de nuestro curso.\n\n\n\n\n\n\n\n\n\nVisualización y presentación\n\n\n\n\n\nSe presentan los resultados buscando la comprensión por parte del cliente. No se trata únicamente de aplicar modelos complejos que nadie, más allá del desarrollador o experto matemático, pueda comprender. Muy al contrario, existe en la actualidad una corriente de investigación orientada a construir métodos capaces de ser explicables, junto con otras técnicas para convertir en entendibles los resultados obtenidos por los más complejos algoritmos matemáticos. Hablaremos de explicabilidad de modelos en el último tema del curso.\n\n\n\n\n\n\n\n\n\nDespliegue en producción\n\n\n\n\n\nLa solución final se convierte en un producto que podrá ser comercializado. Los modelos de ML se construyen para un propósito dentro de una organización. La integración de este modelo dentro del proceso de la organización debería de ser el último paso del proyecto de ciencia de datos.\n\n\n\n\n\n\n\n\n\n\n\nEstadística Descriptiva\n\n\n\n\n\nSe ocupa de resumir y describir las características de un conjunto de datos mediante herramientas gráficas y numéricas, como tablas, gráficos, medias, medianas, varianzas, etc. Su objetivo es proporcionar una visión clara y comprensible de la estructura y características de los datos.\n\n\n\n\n\n\n\n\n\nEstadística Inferencial\n\n\n\n\n\nUtiliza muestras de datos para hacer generalizaciones o inferencias sobre una población más amplia. Involucra el uso de métodos como la estimación de parámetros, pruebas de hipótesis y la construcción de intervalos de confianza. La inferencia estadística permite tomar decisiones y hacer predicciones basadas en datos muestreados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-inferencia",
    "href": "intro.html#sec-inferencia",
    "title": "1  Introducción",
    "section": "1.2 Inferencia Estadística",
    "text": "1.2 Inferencia Estadística\nLa inferencia estadística se refiere a los métodos y procesos utilizados para extraer conclusiones acerca de una población a partir de una muestra de datos. A diferencia de la mera descripción de datos, la inferencia permite ir más allá de lo observado y hacer generalizaciones, estimaciones y decisiones en presencia de incertidumbre. Esto es fundamental para cualquier análisis de datos que aspire a ser predictivo o que busque comprender fenómenos más amplios que los capturados por los datos disponibles.\nAlgunos ejemplos del uso de la inteferencia estadística son:\n\nProporción de votantes a un determinado partido\nProporción de elementos defectuosos en una partida de productos\nProporción de paquetes que llegan tarde\nSalario medio\nAltura media\nConcentración media de un componente\nDuración media de un viaje en tren\nEspera media entre dos trenes consecutivos\n\nEn el contexto de la Ciencia de Datos, la inferencia estadística permite abordar preguntas críticas como:\n\n¿Cuál es la efectividad de un nuevo medicamento?\n¿Qué factores influyen en la satisfacción del cliente?\n¿Cómo se puede predecir el comportamiento futuro de los mercados financieros?\n\nEstas preguntas no solo requieren una recopilación cuidadosa de datos, sino también un análisis riguroso que tenga en cuenta la variabilidad inherente y las posibles fuentes de error.\nLa relevancia de la inferencia estadística en la Ciencia de Datos se manifiesta en varias áreas clave. Despliega el panel para averiguarlas.\n\n\n\n\n\n\nÁreas clave\n\n\n\n\n\nEstas son algunas de las áreas clave donde la inferencia estadística cobra valor.\n\nModelado predictivo: La inferencia estadística es fundamental para construir modelos que pueden predecir resultados futuros basándose en datos históricos. Estos modelos se utilizan en una variedad de campos, desde el marketing hasta la medicina y las finanzas.\nAnálisis experimental: En muchos dominios, como la biomedicina y la psicología, los experimentos controlados son esenciales para determinar causalidad y no solo correlación. La inferencia estadística proporciona el marco para diseñar estos experimentos y analizar los resultados de manera adecuada.\nDecisiones basadas en datos: En el ámbito empresarial y gubernamental, las decisiones basadas en datos permiten optimizar procesos, asignar recursos de manera eficiente y mejorar los servicios. La inferencia estadística permite que estas decisiones sean informadas y respaldadas por evidencia cuantitativa.\nManejo de la incertidumbre: En cualquier análisis de datos, es crucial manejar y comunicar la incertidumbre. La inferencia estadística ofrece métodos para cuantificar esta incertidumbre y tomar decisiones informadas pese a la presencia de variabilidad y error.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#datos-y-variables",
    "href": "intro.html#datos-y-variables",
    "title": "1  Introducción",
    "section": "1.3 Datos y variables",
    "text": "1.3 Datos y variables\nLos datos son las observaciones o medidas que recopilamos del mundo que nos rodea. Estos pueden ser números, categorías o cualquier tipo de información cuantificable. Llamaremos elementos a los individuos, los sujetos, las observaciones sobre las que se recojen un conjunto de variables.\n\n1.3.1 Datos en R\nR incluye en sus librerías numerosos conjuntos de datos. Para acceder a ellos, basta con cargar la librería correspondiente.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nEs posible leer datos desde una cuenta de git. Y podemos tener una primera visión de los datos con sentencias comohead que nos enseña las primeras observaciones y sus variables.\n\nlibrary (readr)\n\nurlfile=\"https://raw.githubusercontent.com/IsaacMartindeDiego/IA/master/datasets/california_housing.csv\"\n\nmydata&lt;-read_csv(url(urlfile))\n\nhead(mydata)\n\n# A tibble: 6 × 10\n  longitude latitude housing_median_age total_rooms total_bedrooms population\n      &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1     -122.     37.9                 41         880            129        322\n2     -122.     37.9                 21        7099           1106       2401\n3     -122.     37.8                 52        1467            190        496\n4     -122.     37.8                 52        1274            235        558\n5     -122.     37.8                 52        1627            280        565\n6     -122.     37.8                 52         919            213        413\n# ℹ 4 more variables: households &lt;dbl&gt;, median_income &lt;dbl&gt;,\n#   median_house_value &lt;dbl&gt;, ocean_proximity &lt;chr&gt;\n\ndim(mydata)\n\n[1] 20640    10\n\nsummary(mydata)\n\n   longitude         latitude     housing_median_age  total_rooms   \n Min.   :-124.3   Min.   :32.54   Min.   : 1.00      Min.   :    2  \n 1st Qu.:-121.8   1st Qu.:33.93   1st Qu.:18.00      1st Qu.: 1448  \n Median :-118.5   Median :34.26   Median :29.00      Median : 2127  \n Mean   :-119.6   Mean   :35.63   Mean   :28.64      Mean   : 2636  \n 3rd Qu.:-118.0   3rd Qu.:37.71   3rd Qu.:37.00      3rd Qu.: 3148  \n Max.   :-114.3   Max.   :41.95   Max.   :52.00      Max.   :39320  \n                                                                    \n total_bedrooms     population      households     median_income    \n Min.   :   1.0   Min.   :    3   Min.   :   1.0   Min.   : 0.4999  \n 1st Qu.: 296.0   1st Qu.:  787   1st Qu.: 280.0   1st Qu.: 2.5634  \n Median : 435.0   Median : 1166   Median : 409.0   Median : 3.5348  \n Mean   : 537.9   Mean   : 1425   Mean   : 499.5   Mean   : 3.8707  \n 3rd Qu.: 647.0   3rd Qu.: 1725   3rd Qu.: 605.0   3rd Qu.: 4.7432  \n Max.   :6445.0   Max.   :35682   Max.   :6082.0   Max.   :15.0001  \n NA's   :207                                                        \n median_house_value ocean_proximity   \n Min.   : 14999     Length:20640      \n 1st Qu.:119600     Class :character  \n Median :179700     Mode  :character  \n Mean   :206856                       \n 3rd Qu.:264725                       \n Max.   :500001                       \n                                      \n\n\nLa estructura de datos más habitual para realizar análisis de datos es el data frame. ¿Has estudiado este concepto en cursos anteriores?. Los data frame son estructuras de datos de dos dimensiones (como una matriz) que pueden contener datos de diferentes tipos. Normalmente nos referimos a las filas de un data frame como observaciones o registros, mientras que las columnas reciben el nombre de campos, variables, o características.\n\nL3 &lt;- LETTERS[1:3]\nchar &lt;- sample(L3, 10, replace = TRUE)\ndatos &lt;- data.frame(x = 1, y = 1:10, char = char)\ndatos\n\n   x  y char\n1  1  1    A\n2  1  2    A\n3  1  3    C\n4  1  4    A\n5  1  5    B\n6  1  6    A\n7  1  7    B\n8  1  8    B\n9  1  9    A\n10 1 10    A\n\nis.data.frame(datos)\n\n[1] TRUE\n\n\nUn tibble, o tbl_df, es una actualización del concepto del data frame. Los tibbles son data.frames perezosos. Esto le obliga a enfrentarse a los problemas antes, lo que normalmente conduce a un código más limpio y expresivo. Los tibbles también tienen un método print() mejorado que facilita su uso con grandes conjuntos de datos que contienen objetos complejos.\n\nlibrary(tidyverse)\nas.tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n\n\n\n\n\n\nPráctica\n\n\n\nEs importante que realices algún ejercicio en R, leyendo datos de diferentes fuentes y familiarizándote con las diferentes estructuras de datos que R proporciona.\n\n\n\n\n1.3.2 Tipo de variables\nUna variable es una característica o atributo que se pueden observar en los elementos y que puede tomar diferentes valores. Llamaremos valores a los resultados que se pueden observar de la característica en el elemento. Las variables siguen una distribución de probabilidad. Las variables pueden ser de varios tipos:\n\nCualitativas: También conocidas como categóricas, estas variables describen atributos o cualidades y se dividen en nominales (sin orden específico, como colores) y ordinales (con un orden, como niveles de satisfacción).\nCuantitativas: Estas variables son numéricas y pueden ser discretas (valores contables, como el número de hijos) o continuas (valores dentro de un rango, como la altura o el peso).\nMarcas de tiempo o identificadores: Como por ejemplo la fecha y hora de una transacción o el código de un producto o el número de identidad.\n\nEn este tema vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo del problema en cuestión es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). Abordaremos este problema en próximos cursos. En este curso nos conformamos con entender los datos y proponer ciertas hipótesis en base a ellos.\nLas variables que debemos estudiar son:\nVariables de entrada:\n# datos del cliente bancario:\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n# otros atributos\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n# Variable de salida (objetivo deseado):\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla!\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nIdentifica el tipo de cada variable y las unidades de medida.\n\n\n\n\n1.3.3 Escalas de medición\nUna escala de medición define cómo se cuantifican o categorizan las variables recogidas sobre un conjunto de datos, influyendo en el análisis estadístico aplicable.\n\nNominal: categorización sin orden inherente. Por ejemplo, el género, la nacionalidad o el tipo de sangre.\nOrdinal: categorización con un orden lógico. Por ejemplo, el nivel educativo, o una clasificación de hoteles.\nMétrica: medición de diferentes escalas:\n\nIntervalo: sin cero verdadero, por ejemplo la temperatura en Celsius.\nRazón: con cero verdadero, por ejemplo los ingresos o la distancia.\n\n\nEn estadística y análisis de datos, es muy común recurrir a conversión entre diferentes escalas de medición. Las más comunes son:\n\nFechas a categóricas: convertir fechas exactas en mes, día de la semana, etc.\nCuantitativas a cualitativas: crear clases o rangos a partir de datos numéricos. Por ejemplo convertir el nivel de ingresos en “bajo”, “medio” y “alto”.\nOrdinales como numéricas: pasar de un orden a unos valores numéricos puede ser peligroso. Esta transformación ha de ser empleada con precaución, especialmente cuando se trabaja con pocos datos (\\(&lt;100\\)). Ten en cuenta que combinar en índices puede ser más informativo.\nVariables calculadas: creación de nuevas variables a partir de las existentes es una práctica muy habitual en análisis de datos. Hay una gran cantidad de situaciones donde esta tarea será muy beneficiosa. Por ejemplo, se crea el Índice de Masa Corporal (IMC) a partir de peso y altura.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#población-y-muestra",
    "href": "intro.html#población-y-muestra",
    "title": "1  Introducción",
    "section": "1.4 Población y muestra",
    "text": "1.4 Población y muestra\nLa población es el conjunto completo de todos los elementos o individuos que se desean estudiar. Puede ser una colección de personas, objetos, eventos o cualquier unidad de observación que sea de interés en un estudio. Por ejemplo, la población podría ser todos los estudiantes de una universidad, todos los árboles en un bosque, o todos los productos fabricados en una planta.\nDado que es a menudo impráctico o imposible estudiar toda la población, se selecciona una muestra, que es un subconjunto de la población. El estudio de las observaciones de una población podría implicar destruir dichas observaciones (vida útil del componente). El coste del estudio de las características de las observaciones podría ser muy elevado (experimentos biológicos). A veces el tamaño de la población es tan elevado (very big data!) que es obligatorio utilizar métodos de muestreo. Eso sí, es importante que la muestra sea representativa para que las conclusiones que se extraen de ella sean válidas para la población completa.\n\n\n\n\n\n\nEjemplo. Sondeo Electoral de 1936\n\n\n\n\n\nEl caso del sondeo electoral de 1936, realizado por la revista “Literary Digest” en Estados Unidos, es un ejemplo clásico que ilustra la importancia de la representatividad de una muestra en la investigación estadística y en particular en la realización de encuestas.\nEn 1936, la revista “Literary Digest” llevó a cabo un sondeo para predecir el resultado de la elección presidencial entre el candidato demócrata Franklin D. Roosevelt y el candidato republicano Alf Landon. La revista enviaba millones de encuestas a sus lectores y a otras listas de individuos, como propietarios de automóviles y personas que aparecían en directorios telefónicos. En ese año, “Literary Digest” envió unos 10 millones de encuestas y recibió alrededor de 2.4 millones de respuestas, lo que constituía una muestra enorme para los estándares de la época.\nLa predicción del sondeo fue que Alf Landon ganaría la elección con un margen significativo. Sin embargo, el resultado real fue una victoria aplastante para Franklin D. Roosevelt, quien ganó con más del 60% del voto popular y el 98.5% del voto electoral. Este error de predicción se debió principalmente a problemas relacionados con la representatividad de la muestra.\nProblemas de Representatividad\n\nSesgo de selección: La muestra del sondeo no era representativa de la población general. La lista de encuestados estaba basada en suscriptores de la revista, propietarios de automóviles y personas que aparecían en directorios telefónicos. En la década de 1930, estos grupos eran, en su mayoría, personas de ingresos más altos y de tendencia republicana, que no representaban adecuadamente a la población estadounidense en general, que incluía una proporción significativa de votantes de ingresos más bajos y de tendencia demócrata.\nTasa de respuesta: Aunque el tamaño de la muestra era grande (2.4 millones de respuestas), la tasa de respuesta fue baja en relación al número de encuestas enviadas (10 millones). Esto puede introducir un sesgo adicional, ya que las personas que respondieron pueden no ser representativas de la población objetivo. Es posible que aquellos más inclinados a responder fueran también más inclinados a votar por Landon.\nMuestreo no aleatorio: El método de selección de la muestra no era aleatorio. Las listas utilizadas para enviar las encuestas no cubrían todas las demografías de manera equitativa. Un muestreo aleatorio hubiera asegurado que cada individuo de la población tuviera una probabilidad conocida y no nula de ser seleccionado, lo cual es crucial para la representatividad.\n\n\n\n\nLa probabilidad juega un papel crucial en el diseño del muestreo y en la inferencia estadística. En el contexto del muestreo, la probabilidad asegura que cada miembro de la población tiene una oportunidad conocida y, generalmente, igual de ser incluido en la muestra. Esto permite que las muestras sean representativas y que los resultados sean generalizables. La teoría de la probabilidad también se utiliza para modelar la incertidumbre y el comportamiento aleatorio inherente en los datos.\n\n\n\n\n\n\nAdvertencia\n\n\n\nProbablemente has estudiado estos conceptos en la asignatura de Probabilidad y Simulación.\n\n\nUna vez que se ha recolectado una muestra, la estadística descriptiva se utiliza para organizar, resumir y presentar los datos de manera comprensible. Las herramientas de estadística descriptiva incluyen:\n\nMedidas de tendencia central: como la media, mediana y moda, que resumen el centro de los datos.\nMedidas de dispersión: como el rango, la varianza y la desviación estándar, que describen la variabilidad de los datos.\nGráficos: como histogramas, gráficos de caja y gráficos de dispersión, que visualizan los datos.\n\nLa estadística descriptiva se centra en describir lo que los datos muestran, sin hacer inferencias o generalizaciones sobre la población.\nLa inferencia estadística utiliza los datos de la muestra para hacer estimaciones, predicciones y generalizaciones sobre la población completa. Esto incluye dos aspectos principales:\n\nEstimación: Utilizar los datos de la muestra para estimar parámetros de la población, como la media o la proporción. Las estimaciones pueden ser puntuales (un solo valor) o por intervalo (un rango de valores con un nivel de confianza asociado).\nContraste de hipótesis: Probar afirmaciones sobre la población utilizando los datos de la muestra. Esto implica formular una hipótesis nula y una hipótesis alternativa, y usar pruebas estadísticas para decidir cuál es más consistente con los datos observados.\n\nLa inferencia estadística se basa en la teoría de la probabilidad para evaluar la incertidumbre y la variabilidad en las estimaciones y pruebas.\nLa Figura 1.2 muestra la relación entre los conceptos de Población, Muestra, Inferencia Estadística, Probabilida y Estadística Descriptiva. Estos conceptos permiten a los estadísticos recolectar datos de manera eficiente, describir los datos recolectados y hacer inferencias significativas y precisas sobre la población a partir de la muestra.\n\n\n\n\n\n\nFigura 1.2: La esencia de la Estadística\n\n\n\nExiste un principio fundamental en el análisis de datos que podríamos simplificar así:\n\\[DATOS = MODELO + ERROR\\]\n\nLos datos representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.\nEl modelo es una representación simplificada de la realidad que proponemos para describirla e interpretarla más fácilmente.\nEl error refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.\n\nBuscaremos, especialmente cuando estudiemos Apendizaje Automático, encontrar el modelo que explique los datos minimizando al máximo el error.\n\n1.4.1 Muestreo\nEl muestreo estadístico es una técnica fundamental en la estadística que permite extraer conclusiones sobre una población basándose en el análisis de una parte más pequeña de dicha población, conocida como muestra. A continuación, presentamos algunas de las principales técnicas de muestreo estadístico. Cada una de estas técnicas tiene sus propias ventajas y limitaciones, y la elección de la técnica adecuada depende del objetivo del estudio, las características de la población y los recursos disponibles.\n\n\n\n\n\n\nMuestreo aleatorio simple\n\n\n\n\n\nEn el muestreo aleatorio simple, cada miembro de la población tiene la misma probabilidad de ser seleccionado. Diremos que la muestra es aletoria simple (m.a.s.) cuando empleamos este tipo de muestreo. Se suele realizar utilizando métodos aleatorios, como sorteo, tablas de números aleatorios o generadores de números aleatorios. Este método es simple y fácil de entender, pero puede no ser siempre el más eficiente, especialmente si la población es muy grande. Podemos realizar el muestro con o sin reemplazamiento. Diremos que un muestreo es con reemplazamiento cuando una observación poblacional puede ser elegida varias veces para formar parte de la muestra. La misma observación puede aparecer repetida. Habitualmente se recurre al muestro sin reemplazamiento, donde una observación de la población, una vez elegida para formar parte de la muestra, es eliminada de la población y no puede volver a ser elegida.\nLa siguiente sentencia de R elige \\(5\\) observaciones de la base de datos bank mediante un muestreo aleatorio simple.\n\nn=dim(bank)[1] # número de observaciones en la base de datos\nindices=sample(1:n,size=5,replace=FALSE)\nindices # indices de las observaciones elegidas\n\n[1]  9871  2846 10505  1818   200\n\nmuestra=bank[indices,] # muestra de observaciones \n\nEsta técnica será empleada en la asignatura de Aprendizaje Automático para crear las particiones de entrenamiento, test y validación.\n\n\n\n\n\n\n\n\n\nMuestreo sistemático\n\n\n\n\n\nEn el muestreo sistemático, se selecciona un punto de inicio al azar y luego se elige a cada n-ésimo individuo de la lista de la población. Por ejemplo, si se quiere una muestra del 10% de una población de 1000 individuos, se seleccionaría un punto de inicio al azar entre los primeros 10 individuos y luego se seleccionaría cada décimo individuo a partir de ese punto. Este método es más fácil de ejecutar que el muestreo aleatorio simple, pero puede introducir sesgos si hay un patrón en la lista de la población.\n\n\n\n\n\n\n\n\n\nMuestreo estratificado\n\n\n\n\n\nEl muestreo estratificado implica dividir la población en subgrupos o estratos homogéneos y luego tomar una muestra aleatoria de cada estrato. Los estratos se forman en base a una característica específica como la edad, el género, el nivel socioeconómico, etc. Este método asegura que todas las subpoblaciones importantes estén representadas en la muestra y puede proporcionar estimaciones más precisas que el muestreo aleatorio simple. En el Aprendizaje Automático la variable elegida será la variable respuesta, o variable objetivo. De este modo, cuando los datos están desequilibrados, es decir, cuando tenemos más observaciones de una clase que de otra en los valores de la variable respuesta, aseguramos que todas los grupos están igualmente representados en la muestra.\n\n\n\n\n\n\n\n\n\nMuestreo por conglomerados\n\n\n\n\n\nEn el muestreo por conglomerados, la población se divide en grupos o conglomerados, y se seleccionan algunos de estos conglomerados al azar. Luego, todos (o una muestra de) los individuos dentro de los conglomerados seleccionados se incluyen en la muestra. Este método es útil cuando es difícil o costoso crear una lista completa de la población, pero puede ser menos preciso si los conglomerados no son homogéneos.\n\n\n\n\n\n\n\n\n\nMuestreo por cuotas\n\n\n\n\n\nEl muestreo por cuotas es un tipo de muestreo no probabilístico en el que se selecciona una muestra que cumple con ciertas cuotas preestablecidas basadas en características específicas. Por ejemplo, se puede querer que la muestra tenga un cierto número de individuos de diferentes edades o géneros. Aunque este método es práctico y rápido, no permite estimaciones estadísticas precisas de la población porque no todos los individuos tienen la misma probabilidad de ser seleccionados.\n\n\n\n\n\n\n\n\n\nMuestreo de bola de nieve\n\n\n\n\n\nEl muestreo de bola de nieve es otra técnica no probabilística, utilizada principalmente cuando es difícil acceder a los miembros de la población. Se empieza con unos pocos individuos conocidos de la población, quienes a su vez refieren a otros individuos, y así sucesivamente. Este método es útil para estudios de poblaciones ocultas o difíciles de alcanzar, como personas sin hogar o usuarios de drogas, pero introduce un alto riesgo de sesgo. También es muy común utilizar este tipo de muestreo cuando se realizan encuestas en redes sociales puesto que, habitualmente, la población objetivo está oculta o es complejo acceder a ella.\n\n\n\n\n\n\n\n\n\nMuestreo intencional o dirigido\n\n\n\n\n\nEn el muestreo intencional o dirigido, se seleccionan individuos que cumplen con ciertos criterios específicos de la investigación. Es un método no probabilístico donde la selección se basa en el juicio del investigador. Es útil cuando se busca estudiar casos específicos, pero no permite hacer generalizaciones precisas sobre la población completa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#parámetros-y-estadísticos",
    "href": "intro.html#parámetros-y-estadísticos",
    "title": "1  Introducción",
    "section": "1.5 Parámetros y estadísticos",
    "text": "1.5 Parámetros y estadísticos\nLos parámetros de una población son (casi) siempre desconocidos. Son valores teóricos que se definen sobre la población y que son de interés para el investigador. Son sobre los que haremos inferencia. Normalmente se representan con letras griegas.\nDiremos que un estadístico (además de una persona que hace estadística) es una función definida sobre los datos de una muestra (valores de una o más variables). En cada muestra serán distintos, debido a la variabilidad inherente a la extracción de una muestra representativa de una población. Los estadísticos siguen una distribución en el muestro y se representa con letras latinas.\n\n\n\n\n\n\nFigura 1.3: Parámetros vs. Estadísticos\n\n\n\nEs decir, una vez obtenida una muestra, es necesario extraer información útil de la misma. Esto se hace a través de los estadísticos.\nDiremos que un estadístico \\(T= T(x_1,\\ldots,x_n)\\) es una función real de la muestra aleatoria \\((x_1,\\ldots,x_n)\\).\nUn estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución. La ditribución de probabilidad correspondiente a un estadístico se denomina distribución muestral.\nPor ejemplo, sea \\((X_1,\\ldots,X_n)\\) una muestra aletoria de una población \\(X\\) con esperanza \\(\\mu\\) y vaianza \\(\\sigma^2\\). Consideremos los siguientes ejemplos de estadísticos:\n\nMedia Muestral (\\(\\bar{X}\\)): Utilizada para estimar la media poblacional. \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nVarianza Muestral (\\(V\\)): Utilizada para estimar la varianza poblacional. \\[\nV=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2\n\\]\nCuasivarianza muestral (\\(S^2\\)) \\[\nS^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2=\\frac{1}{n-1}\\left [ \\sum_{i=1}^nX_i^2-n\\bar{X}^2\\right]\n\\] Entonces se tienen las siguientes propiedades:\n\n\\[\nE(\\bar{X})=\\mu\n\\] \\[\nVar(\\bar{X})=\\frac{\\sigma^2}{n}\n\\]\n\\[\nE(V)=\\frac{n-1}{n}\\sigma^2\n\\] \\[\nE(S^2)=\\sigma^2\n\\]\n\n1.5.1 Uso de la muestra\nSupongamos que queremos estudiar una característica de cierta población. Esta característica se representa mediante una variable aleatoria \\(X\\) y el estudio se centra en su valor medio \\(E[X]\\). Para ello, se decide tomar una muestra y se obtiene un estadístico, la media muestral, que se utiliza para estimar el valor de \\(E[X]\\): \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\] Fíjate que este estadístico \\(\\bar{X}\\) es una variable aleatoria. Una vez tomada una muestra particular \\((x_1,\\ldots,x_n)\\) de la variable aleatoria \\(X\\) se obtiene un valor numérico particular para la variable aleatoria \\(\\bar{X}\\): \\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\n\\]\n\n\n\n\n\n\nPara recordar\n\n\n\n\\[\\bar{X}\\neq\\bar{x}\\]\n\n\n\n\n\n\n\n\nEjemplo. Diferencia entre parámetro y estadístico\n\n\n\n\n\nSe quiere estudiar la temperatura de una solución líquida en un laboratorio y estimar el valor medio \\(\\mu\\) (parámetro de la población), que es desconocido. Se supone que la temperatura se puede aproximar mediante una variable aleatoria de la que se desconoce su distribución. Una opción razonable para estimarla sería escoger una muestra de la solución, medir su temperatura, y estimar \\(\\mu\\) mediante el promedio de esa muestra (estadístico muestral): \\[ \\hat{\\mu}=\\bar{X}\\]. Es importante señalar que podríamos emplear otros estadísticos diferentes.\nSupongamos ahora que se toma una muestra de 10 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n [1] 45.76 42.75 45.54 49.61 53.00 44.15 51.48 48.69 51.33 49.21\n\n\nSiendo su valor promedio \\(\\bar{x}=\\) 48.15. Este valor estima el verdadero valor desconocido del parámetro \\(\\mu\\).\nAhora, supongamos que se toma otra muestra de 5 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n[1] 50.18 43.89 46.18 47.74 46.59\n\n\nLa media de esta otra muestra es \\(\\bar{x}=\\) 46.92 otra estimación del desconocido valor del parámetro \\(\\mu\\).\n¿Cuál de las dos estimaciones te parece más fiable? ¿Por qué? ¿Cómo podríamos “asegurar” que nuestro estimador es altamente fiable?\nRepetimos el primer experimento \\(100\\) días consecutivos. Es decir, tomamos \\(10\\) muestras de tamaño 10 y estimamos, para cada muestra el valor del parámetro \\(\\mu\\) mediante la media muestral. El siguiente gráfico muestra los valores de esas \\(100\\) muestras:\n\n\n\n\n\n\n\n\n\n¿Cuál es, a tu juicio, un buen valor del estimador del parámetro? Supongamos que tu profesor ha simulado estos datos. Es decir, tu profesor conoce realmente el verdadero valor del parámetro. En ese caso, te propone el siguiente reto: Dame dos valores (uno bajo y otro alto) entre los cuales crees que está el verdadero valor que solo yo (el profesor) conozco. ¿Qué valores darías? Te propone un reto mayor: ¿Qué valores darías si quisieras estar seguro al \\(100\\%\\) de acertar? Es decir, que la probabilidad de fallo sea \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "href": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "title": "1  Introducción",
    "section": "1.6 Estadística paramétrica y no paramétrica",
    "text": "1.6 Estadística paramétrica y no paramétrica\nTal y como hemos visto hasta ahora, el objetivo general de la inferencia es obtener información acerca de la distribución de una variable aleatoria \\(X\\) mediante la observación de una muestra \\((X_1,\\ldots,X_n)\\). En este curso vamos a tratar con dos tipos de herramientas para realizar inferencia estadística: la estadística paramétrica y la no paramétrica. Veamos sus semejanzas y diferencias:\nEstadística Paramétrica\nLa estadística paramétrica se basa en la suposición de que los datos siguen una distribución conocida, como la distribución normal, binomial, Poisson, etc. Estas suposiciones (o hipótesis) deben ser comprobadas para dar validez a este tipo de pruebas. Los parámetros de estas distribuciones, como la media y la varianza, se utilizan para resumir la información de los datos y realizar inferencias.\n\n\n\n\n\n\nFamilias paramétricas\n\n\n\n\n\nSe tiene una variable aleatoria \\(X\\) cuya distribución se supone perteneciente a una cierta familia paramétrica {\\(f_\\theta\\)} donde \\(\\theta \\in \\Theta\\).\nLa distribución de \\(X\\) es conocida excepto por el valor del parámetro \\(\\theta\\), del cual lo único que se conoce es su rango de posibles valores, \\(\\Theta\\), denominado espacio paramétrico.\nEjemplos de familias paramétricas\n\n\\(X \\sim N(\\mu,\\sigma^2) \\rightarrow \\theta=(\\mu,\\sigma^2)\\)\n\\(X \\sim Bernoulli(p) \\rightarrow \\theta=p\\)\n\\(X \\sim Exp(\\lambda) \\rightarrow \\theta=\\lambda\\)\n\n\n\n\nPor tanto, el objeto de los métodos paramétricos es obtener información sobre el parámetro de interés mediante la obtención de muestras de la variable aleatoria.\nLos métodos paramétricos son más potentes que los no paramétricos (es decir, tienen una mayor probabilidad de detectar un efecto verdadero) si las suposiciones son correctas. Ejemplos de pruebas paramétricas incluyen la prueba t de Student, el análisis de varianza (ANOVA), que estudiaremos en el Capítulo 5 y la regresión lineal que veréis en el segundo cuatrimestre.\nEstadística no paramétrica\nLa estadística no paramétrica no hace suposiciones fuertes sobre la distribución de los datos. Estos métodos son más flexibles y robustos a las violaciones de las suposiciones, pero pueden ser menos potentes si las suposiciones de los métodos paramétricos son verdaderas. Los métodos no paramétricos a menudo se basan en el orden de los datos, en lugar de sus valores exactos. Ejemplos de pruebas no paramétricas incluyen la prueba de Mann-Whitney U, la prueba de Kruskal-Wallis y la prueba de Chi-cuadrado. Tendremos un capítulo (Capítulo 4), dedicado a este tipo de herramientas.\nLa elección entre métodos paramétricos y no paramétricos depende de la naturaleza de tus datos y de las suposiciones que estés dispuesto a hacer. Si tus datos cumplen con las suposiciones de una prueba paramétrica, esa prueba puede ser la opción más potente. Si no, una prueba no paramétrica puede ser más apropiada.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#frecuentistas-vs-bayesianos",
    "href": "intro.html#frecuentistas-vs-bayesianos",
    "title": "1  Introducción",
    "section": "1.7 Frecuentistas vs Bayesianos",
    "text": "1.7 Frecuentistas vs Bayesianos\nEn el campo de la estadística existen dos enfoques diferentes que han de ser comentados, la inferencia clásica (o frecuentista) y la inferencia Bayesiana.\nEnfoque Frecuentista\nLos frecuentistas interpretan la probabilidad como la frecuencia relativa de un evento en un número infinito de repeticiones del experimento. Se obtienen datos a través de una muestra y con técnicas estadísticas se extrae información de los mismos mediante, los llamados, estimadores. En base a esas estimaciones se toman decisiones en el dominio de aplicación.\nLos métodos frecuentistas son ampliamente utilizados y son la base de muchas técnicas estadísticas clásicas. Los parámetros son considerados como valores fijos y desconocidos que se estiman a partir de los datos. Los intervalos de confianza, que veremos más adelante en el Capítulo 3, se interpretan en términos de repetibilidad: si se repite el experimento muchas veces, el intervalo de confianza capturará el verdadero parámetro en un porcentaje dado de las repeticiones. Esta interpretación es un poco extraña para el no iniciado y trataremos de explicarlo en detalle en capítulos posteriores.\nEnfoque Bayesiano\nLa inferencia Bayesiana tiene su fundamento en el teorema de Bayes. El teorema de Bayes, también conocido como regla de Bayes, es un principio fundamental en la teoría de la probabilidad que describe la forma de actualizar las probabilidades de una hipótesis basándose en nueva evidencia o información. Fue formulado por el matemático británico Thomas Bayes en el siglo XVIII. Es posible que hayas estudiado este teorema en asignaturas anteriores del grado. En cualquier caso, es sencillo y dice así:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\ndonde:\n\n\\(P(A|B)\\) es la probabilidad de que ocurra el evento \\(A\\) dado que ha ocurrido el evento \\(B\\). Esta es conocida como la probabilidad a posteriori.\n\\(P(B|A)\\) es la probabilidad de que ocurra el evento \\(B\\) dado que ha ocurrido el evento \\(A\\). Esta es conocida como la probabilidad verosímil o likelihood.\n\\(P(A)\\) es la probabilidad de que ocurra el evento \\(A\\) sin ninguna información adicional sobre \\(B\\). Esta es conocida como la probabilidad a priori o simplemente la probabilidad previa.\n\\(P(B)\\) es la probabilidad de que ocurra el evento \\(B\\) bajo todas las posibles hipótesis. Esta es conocida como la probabilidad marginal de \\(B\\).\n\nVemos que el teorema de Bayes permite actualizar la probabilidad de una hipótesis \\(A\\) a la luz de nueva evidencia \\(B\\). Básicamente, proporciona una forma de ajustar nuestras creencias iniciales (probabilidad a priori) en base a la nueva información disponible (evidencia).\n\n\n\n\n\n\nEjemplo. Teorema de Bayes.\n\n\n\n\n\nImaginemos que estamos tratando de diagnosticar una enfermedad rara que afecta al \\(1\\%\\) de la población (es decir, \\(P(\\text{Enfermedad}) = 0.01\\)). Ademas, se sabe que existe una prueba para esta enfermedad que es \\(99\\%\\) precisa:\n\nSi una persona tiene la enfermedad, la prueba es positiva el \\(99\\%\\) de las veces, es decir \\(P(\\text{Positivo}|\\text{Enfermedad}) = 0.99\\).\nSi una persona no tiene la enfermedad, la prueba es negativa el \\(99\\%\\) de las veces \\(P(\\text{Negativo}|\\text{No Enfermedad}) = 0.99\\), lo que significa que tiene un \\(1\\%\\) de falsos positivos \\(P(\\text{Positivo}|\\text{No Enfermedad}) = 0.01\\).\n\nEn este caso, deseamos saber cuál es la probabilidad de que una persona tenga la enfermedad si la prueba ha sido positiva \\(P(\\text{Enfermedad}|\\text{Positivo})\\).\nAplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad})}{P(\\text{Positivo})}\n\\]\nPrimero, calculamos \\(P(\\text{Positivo})\\), la probabilidad total de que la prueba sea positiva:\n\\[\nP(\\text{Positivo}) = P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad}) +\n\\] \\[P(\\text{Positivo}|\\text{No Enfermedad}) \\cdot P(\\text{No Enfermedad})\n\\]\n\\[\nP(\\text{Positivo}) = (0.99 \\times 0.01) + (0.01 \\times 0.99) = 0.0099 + 0.0099 = 0.0198\n\\]\nAhora, aplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{0.99 \\times 0.01}{0.0198} = \\frac{0.0099}{0.0198} \\approx 0.50\n\\]\nEsto significa que, a pesar de que la prueba es bastante precisa, si una persona da positivo en la prueba, la probabilidad de que realmente tenga la enfermedad es aproximadamente \\(50\\%\\), debido a la baja prevalencia de la enfermedad en la población.\n\n\n\nEl teorema de Bayes es una herramienta poderosa para la toma de decisiones y la inferencia estadística, ya que proporciona un marco formal para actualizar nuestras creencias en base a la evidencia disponible.\nLos bayesianos interpretan la probabilidad como una medida de la creencia o confianza en un evento. Esta creencia puede ser actualizada a medida que se obtiene más información. Los parámetros son considerados como variables aleatorias y se describe su incertidumbre a través de distribuciones de probabilidad. Los intervalos de credibilidad bayesianos proporcionan una medida directa de la incertidumbre del parámetro: hay una probabilidad dada de que el verdadero parámetro esté dentro del intervalo de credibilidad. Esto parece tener más lógica que el enfoque frecuentista, pero es menos habitual. Los métodos bayesianos permiten la incorporación directa de conocimientos previos en el análisis a través de la distribución a priori.\nPor tanto, la principal diferencia entre los enfoques frecuentista y bayesiano radica en cómo interpretan el concepto de probabilidad. El enfoque frecuentista se basa en frecuencias de eventos, mientras que el enfoque bayesiano se basa en la incertidumbre y la actualización de las creencias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#notación",
    "href": "intro.html#notación",
    "title": "1  Introducción",
    "section": "1.8 Notación",
    "text": "1.8 Notación\nA lo largo de este libro vamos a usar la siguiente notación:\n\n\\(X, Y\\): Variables\n\\(i\\): Identificador o índice para cada observación o clase\n\\(x_i\\): Valor que toma la variable \\(X\\) en la observación \\(i\\)\n\\(c_i\\): Marca de clase en datos agrupados\n\\(n\\): Número total de observaciones\n\\(k\\): Número de clases\n\\(n_i\\): Número de observaciones en la clase \\(i\\)\n\\(\\bar{x}\\): Media muestral de la variable \\(X\\)\n\\(s^2\\): Varianza muestral de la variable \\(X\\)\n\\(s\\): Desviación típica muestral de la variable \\(X\\)\n\\(\\mu\\): Media poblacional\n\\(\\sigma^2\\): Varianza poblacional\n\\(\\hat{[\\cdot]}\\): Simboliza un estimador de \\(\\cdot\\). Por ejemplo, \\(s = \\hat{\\sigma}\\) quiere decir que la desviación típica muestral \\(s\\) es un estimador de la desviación típica poblacional \\(\\sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "href": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "title": "1  Introducción",
    "section": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística",
    "text": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística\n\n1.9.1 Métodos numéricos\nTal y como hemos indicado anteriormente, los métodos numéricos de inferencia estadística son técnicas y procedimientos utilizados para analizar datos y hacer inferencias sobre una población a partir de una muestra. Estos métodos se apoyan en herramientas matemáticas y computacionales para estimar parámetros, evaluar hipótesis y tomar decisiones informadas. A continuación, se describen los principales métodos numéricos en la inferencia estadística. No tienes que aprenderlos ahora puesto que vamos a trabajar con estos métodos a lo largo de todo el curso. Los veremos con mayor detalle en los próximos capítulos.\n\n\n\n\n\n\nEstimación puntual\n\n\n\n\n\nLa estimación puntual implica el uso de un solo valor estadístico de la muestra para estimar un parámetro de la población.\n\nMedia Muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)).\nProporción Muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)).\nVarianza Muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\n\n\nEstimación por intervalo\n\n\n\n\n\nLa estimación por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con un cierto nivel de confianza.\n\nIntervalo de confianza para la media:\n\nPara muestras grandes (\\(n \\ge 30\\)): \\(\\bar{x} \\pm Z_{\\alpha/2} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right )\\)\n\nPara muestras pequeñas (\\(n &lt; 30\\)) y cuando la distribución es normal: \\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\left(\\frac{s}{\\sqrt{n}}\\right)\\)\n\nIntervalo de confianza para la proporción: \\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nIntervalo de confianza para la varianza: \\(\\left( \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\\)\n\n\n\n\n\n\n\n\n\n\nContraste de hipótesis\n\n\n\n\n\nEl contraste de hipótesis es un procedimiento para tomar decisiones sobre los parámetros poblacionales basándose en la evidencia proporcionada por los datos muestrales.\n\nFormulación de hipótesis:\n\nHipótesis nula (\\(H_0\\)): Es la afirmación que se desea probar o refutar.\nHipótesis alternativa (\\(H_1\\)): Es la afirmación que se acepta si se rechaza (\\(H_0\\)).\n\nPruebas para la media:\n\nPrueba Z: Utilizada para muestras grandes (\\(n \\ge 30\\)) o cuando se conoce la desviación estándar poblacional (\\(\\sigma\\)).\nPrueba t: Utilizada para muestras pequeñas (\\(n &lt; 30\\)) y cuando no se conoce (\\(\\sigma\\)).\n\nPruebas para la proporción:\n\nPrueba Z para proporciones: Utilizada para evaluar hipótesis sobre una proporción poblacional.\n\nPruebas para la Varianza:\n\nPrueba Chi-cuadrado: Utilizada para evaluar hipótesis sobre la varianza poblacional.\n\n\n\n\n\n\n\n\n\n\n\nMétodos de resampling\n\n\n\n\n\nLos métodos de resampling, como el bootstrap y el jackknife, son técnicas computacionales utilizadas para estimar la precisión de los estadísticos de la muestra.\n\nBootstrap: Consiste en tomar múltiples muestras con reemplazo de los datos originales y calcular el estadístico de interés para cada muestra. Esto permite construir una distribución empírica del estadístico y estimar intervalos de confianza.\nJackknife: Involucra excluir sistemáticamente cada observación de la muestra y recalcular el estadístico de interés. Esto proporciona una manera de estimar el sesgo y la varianza del estimador.\n\n\n\n\n\n\n\n\n\n\nMétodos bayesianos\n\n\n\n\n\nLa inferencia bayesiana utiliza la probabilidad subjetiva para actualizar la creencia sobre los parámetros poblacionales basándose en la evidencia muestral.\n\nTeorema de Bayes: \\(P( \\theta |x) = \\frac{P(x | \\theta)P( \\theta)}{P(x)}\\), donde \\(P(\\theta|x)\\) es la distribución a posteriori, \\(P(x|\\theta)\\) es la verosimilitud, \\(P(\\theta)\\) es la distribución a priori y \\(P(x)\\) es la probabilidad marginal de los datos.\nSimulación Monte Carlo Markov Chain (MCMC): Es un método numérico para aproximar distribuciones posteriores complejas.\n\n\n\n\n\n\n\n\n\n\nPruebas no paramétricas\n\n\n\n\n\nCuando no se cumplen los supuestos de normalidad, se utilizan pruebas no paramétricas que no requieren asumir una distribución específica.\n\nPrueba de Wilcoxon: Para comparar medianas entre dos muestras emparejadas.\nPrueba de Mann-Whitney: Para comparar medianas entre dos muestras independientes.\nPrueba de Kruskal-Wallis: Extensión de la prueba de Mann-Whitney para más de dos grupos.\nPrueba de Chi-cuadrado: Para pruebas de independencia y homogeneidad en tablas de contingencia.\n\n\n\n\n\n\n1.9.2 Métodos gráficos\nA lo largo del curso usaremos numerosos métodos gráficos para explorar los datos dentro del contexto de la inferencia estadística. Algunas de los métodos básicos son:\n\n\n\n\n\n\nHistogramas\n\n\n\n\n\nGráficos de barras que muestran la distribución de frecuencias de datos cuantitativos.\n\n\n\n\n\n\n\n\n\nGráficos de caja (boxplots)\n\n\n\n\n\nMuestran la mediana, los cuartiles y los posibles valores atípicos.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión (scatterplots)\n\n\n\n\n\nUtilizados para observar la relación entre dos variables cuantitativas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#teorema-central-del-límite",
    "href": "intro.html#teorema-central-del-límite",
    "title": "1  Introducción",
    "section": "1.10 Teorema Central del Límite",
    "text": "1.10 Teorema Central del Límite\nEl Teorema Central del Límite (TCL) es uno de los principios más fundamentales en la estadística y la probabilidad. Establece que, bajo ciertas condiciones, la distribución de la suma (o el promedio) de un gran número de variables aleatorias independientes e identicamente distribuidas tiende a seguir una distribución Normal, independientemente de la distribución original de las variables.\nFormalmente, el TCL establece que si, \\(X_1,X_2,\\ldots,X_n\\) son variables aleatorias independientes e idénticamente distribuidas, con media \\(\\mu\\) y varianza \\(\\sigma^2&lt;\\infty\\), entonces, para \\(n\\) suficientemente grande se verifica:\n\\[\\bar{X} \\approx N \\left ( \\mu,\\frac{\\sigma^2}{n}\\right )\\] Este resultado es válido tanto para variables discretas como continuas, sean simétricas o asimétricas, unimodales o multimodales.\n\n\n\n\n\n\nPara recordar\n\n\n\nEl Teorema Central del Límite asegura que con muestas suficientemente grandes se pueden utilizar estimaciones basadas en la distribución Normal independientemente del tipo de distribución que siga la variable que nos interesa.\n\n\n\n\n\n\n\n\nEjemplo. Teorema Central del Límite\n\n\n\n\n\nAcabamos de ver que el TCL establece que, para una gran cantidad de muestras aleatorias tomadas de una población con una distribución cualquiera (con una media y una varianza finitas), la distribución de las medias muestrales tiende a ser Normal, independientemente de la forma de la distribución original.\nAquí tienes un ejemplo en R que ilustra el Teorema Central del Límite. Vamos a tomar muestras de una distribución no normal (por ejemplo, una distribución uniforme). Calcularemos las medias de estas muestras y observaremos cómo las medias muestrales se aproximan a una distribución normal.\n\n# Configuraciones iniciales\nset.seed(123)          # Fijamos la semilla para reproducibilidad\nn_samples &lt;- 1000      # Número de muestras\nsample_size &lt;- 30      # Tamaño de cada muestra\n\n# Generamos las muestras de una distribución uniforme\nsample_means &lt;- numeric(n_samples)\nfor (i in 1:n_samples) {\n  sample &lt;- runif(sample_size, min = 0, max = 10)\n  sample_means[i] &lt;- mean(sample)\n}\n\n# Crear un data frame con las medias muestrales\ndata &lt;- data.frame(sample_means)\n\n# Graficamos el histograma de las medias muestrales utilizando ggplot2\nggplot(data, aes(x = sample_means)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mean(sample_means), sd = sd(sample_means)),\n                color = \"red\", size = 1) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\", y = \"Densidad\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = mean(sample_means), y = max(density(sample_means)$y) / 2,\n           label = \"Distribución Normal\", color = \"red\")\n\n\n\n\n\n\n\n\nPara visualizar el resultado, graficamos un histograma de las medias muestrales. Luego, superponemos una curva de densidad de una distribución normal con la misma media y desviación estándar que las medias muestrales para observar cómo se ajusta a una distribución normal.\nEl histograma de las medias muestrales se aproxima a una distribución normal, a pesar de que las muestras originales provienen de una distribución uniforme.\n\n\n\n\n\n\n\nHao, Jiangang, y Tin Kam Ho. 2019. «Machine learning made easy: a review of scikit-learn package in python programming language». Journal of Educational and Behavioral Statistics 44 (3): 348-61.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, y Jerome H Friedman. 2009. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nKelleher, John D, Brian Mac Namee, y Aoife D’arcy. 2020. Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies. MIT press.\n\n\nOsmani, Addy. 2012. Learning JavaScript Design Patterns: A JavaScript and jQuery Developer’s Guide. \" O’Reilly Media, Inc.\".\n\n\nOualline, Steve. 2003. Practical C++ programming. \" O’Reilly Media, Inc.\".\n\n\nWirth, Rüdiger, y Jochen Hipp. 2000. «CRISP-DM: Towards a standard process model for data mining». En Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining, 1:29-39. Manchester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "2.1 Preguntas\nTu objetivo principal durante el EDA es adquirir una comprensión profunda de los datos que se están analizando. La forma más sencilla de hacerlo es utilizar preguntas como herramientas para guiar la investigación. Cuando planteas una pregunta, ésta centra tu atención en una parte específica del conjunto de datos y te ayuda a decidir qué gráficos, modelos o transformaciones realizar.\nEDA es un proceso creativo y como tal, la clave para llevarlo a cabo consiste en el planteamiento de preguntas de calidad. ¿Qué preguntas son las correctas? La respuesta es que depende del conjunto de datos con el que se trabaje.\nAl inicio del análisis, puede resultar todo un desafío formular preguntas reveladoras, ya que aún no se conoce completamente la información contenida en el conjunto de datos. Si estás involucrado en un proceso de inferencia estadística, en muchas ocasiones, esas preguntas vendrán formuladas por un experto del dominio o por un superior con conocimientos estadísticos. Cada nueva pregunta que se plantee te llevará a explorar un nuevo aspecto de tus datos, aumentando así las posibilidades de hacer descubrimientos importantes.\nAlgunas de las preguntas que, generalmente, deberían de abordarse durante el EDA son:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#preguntas",
    "href": "eda.html#preguntas",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "John Tukey\n\n\n\nMucho mejor una respuesta aproximada a la pregunta correcta, que a menudo es vaga, que una respuesta exacta a la pregunta incorrecta, que siempre se puede precisar.\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nDurante la preparación y limpieza de los datos acumulamos pistas sobre los modelos de aprendizaje más adecuados que podrán ser aplicados en etapas posteriores.\n\n\n\n\n¿Cuál es el tamaño de la base de datos? Es decir:\n\n¿Cuántas observaciones hay?\n¿Cuántas variables/características están medidas?\n¿Disponemos de capacidad de cómputo en nuestra máquina para procesar la base de datos o necesitamos más recursos?\n¿Existen valores faltantes?\n\n¿Qué tipo variables aparecen en la base de datos?\n\n¿Qué variables son discretas?\n¿Cuáles son continuas?\n¿Qué categorías tienen las variables?\n¿Hay variables tipo texto?\n\nVariable objetivo: ¿Existe una variable de “respuesta”?\n\n¿Binaria o multiclase?\n\n¿Es posible identificar variables irrelevantes?. Estudiar variables relevantes requiere, habitualmente, métodos estadísticos.\n¿Es posible identificar la distribución que siguen las variables?\nCalcular estadísticos resumen (media, desviación típica, frecuencia,…) de todas las variables de interés. Estudiaremos las propiedades de estos estimadores en el próximo capítulo.\nDetección y tratamiento de valores atípicos.\n\n¿Son errores de media?\n¿Podemos eliminarlos?\n\n¿Existe correlación entre variables?\n\n\n\n\n\n\n\nPara recordar\n\n\n\nUna correcta preparación y limpieza de datos implica, sin duda, un ahorro de tiempo en etapas posteriores del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#entender-el-negocio",
    "href": "eda.html#entender-el-negocio",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.2 Entender el negocio",
    "text": "2.2 Entender el negocio\nLa comprensión del problema que estamos abordando representa una de las primeras etapas en cualquier proyecto de ciencia de datos. En la mayoría de los casos, esta tarea se realiza en estrecha colaboración con expertos en el dominio correspondiente, quienes a menudo son las personas que han solicitado (y a menudo financian) el análisis de datos. Es importante recordar que cualquier estudio que involucre ciencia de datos requiere un conocimiento profundo del dominio, el cual debe ser compartido con el científico de datos. Por lo tanto, el profesional de la ciencia de datos debe poseer un conocimiento suficiente para enfrentar con confianza los diversos desafíos que puedan surgir. Esta comprensión inicial permite establecer los objetivos del proyecto y procesar los datos de manera correcta para obtener información valiosa. A través de esta información, se busca derivar conocimientos aplicables. Este conocimiento puede ser aprendido y almacenado para su uso futuro, lo que lleva a la sabiduría, según la jerarquía de conocimiento presentada en la Figura Figura 2.1.\n\n\n\n\n\n\nFigura 2.1: Jerarquía de Conocimiento\n\n\n\n\n\n\n\n\n\nClaude Lévi-Strauss\n\n\n\n“El científico no es una persona que da las respuestas correctas, sino una persona que hace las preguntas correctas.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#un-primer-vistazo-a-los-datos",
    "href": "eda.html#un-primer-vistazo-a-los-datos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.3 Un primer vistazo a los datos",
    "text": "2.3 Un primer vistazo a los datos\nEn este capítulo vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo final, que abordaremos en cursos posteriores, es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). En este curso nos conformamos con adquirir conocimiento de los datos, planteando algunas hipótesis de interés.\nLas variables que debemos estudiar son:\nVariables de entrada:\n\nDatos del cliente bancario:\n\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n\nOtros atributos\n\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n\nVariable de salida (objetivo deseado):\n\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente. Los datos suelen ser más complejos que la teoría detrás de los datos.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla! Si no conocemos el significado de una variable, difícilmente podremos interpretar los resultados asociados a ella.\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#tipo-de-variables",
    "href": "eda.html#tipo-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.4 Tipo de variables",
    "text": "2.4 Tipo de variables\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\nCreamos las particiones sobre los datos y trabajamos sobre la partición de entrenamiento. Este paso cobrará significado en asignaturas posteriores, especialmente en las enfocadas en el aprendizaje automático (Machine Learning). Sin entrar en más detalles, podemos decir que la idea fundamental es estudiar las hipótesis en un subconjunto de la muestra disponible y evaluar la veracidad (o no) de dichas hipótesis en muestras diferentes.\n\n# Parciticionamos los datos\nset.seed(2138)\nn=dim(bank)[1]\nindices=seq(1:n)\nindices.train=sample(indices,size=n*.5,replace=FALSE)\nindices.test=sample(indices[-indices.train],size=n*.25,replace=FALSE)\nindices.valid=indices[-c(indices.train,indices.test)]\n\nbank.train=bank[indices.train,]\nbank.test=bank[indices.test,]\nbank.valid=bank[indices.valid,]\n\n\n\n\n\n\n\nAtrévete\n\n\n\n¿Te has hecho (ya) alguna pregunta sobre los datos? Si es así, no esperes más, busca la respuesta!\n\n\nPor ejemplo, ¿qué te parecen estas preguntas que nosotros proponemos?\n¿Qué día del año se producen más depósitos por parte de los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\") %&gt;% \n  count(month, day) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 apr      30    79\n\n\n¿En qué mes del año realizan más depósitos los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\" & job==\"student\") %&gt;% \n  count(month) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 2\n  month     n\n  &lt;chr&gt; &lt;int&gt;\n1 apr      21\n\n\n¿Qué trabajo está asociado con el mayor porcentaje de depósitos?\n\n\nClick para ver el código\nbank.train %&gt;%\n  group_by(job) %&gt;%\n  mutate(d = n()) %&gt;%\n  group_by(job, deposit) %&gt;%\n  summarise(Perc = n()/first(d), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    id_cols = job,\n    names_from = deposit,\n    values_from = Perc\n  ) %&gt;%\n  top_n(1)\n\n\n# A tibble: 1 × 3\n  job        no   yes\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 student 0.218 0.782\n\n\n\n\n\n\n\n\nRepaso\n\n\n\ndplyr es un paquete en R diseñado para facilitar la manipulación y transformación de datos de manera eficiente y estructurada. Fue desarrollado por Hadley Wickham y se ha convertido en una de las herramientas más populares en la ciencia de datos y análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#variable-objetivo",
    "href": "eda.html#variable-objetivo",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.5 Variable objetivo",
    "text": "2.5 Variable objetivo\nEn el ámbito del Aprendizaje Automático, en problemas de clasificación (Aprendizaje Supervisado) existe una variable de interés fundamental, es la variable respuesta o variable objetivo. En el próximo cuatrimestre se trata con detalle este tipo de problemas. En el caso que nos ocupa dicha variable es la característica: “deposit”. Vamos a estudiar la información que nos proporciona dicha variable.\n\nlibrary(ggplot2)\ntable(bank.train$deposit)\n\n\n  no  yes \n2908 2673 \n\nggplot(data=bank.train,aes(x=deposit,fill=deposit)) +\n  geom_bar(aes(y=(..count..)/sum(..count..))) +\n  scale_y_continuous(labels=scales::percent) +\n  theme(legend.position=\"none\") +\n  ylab(\"Frecuencia relativa\") +\n  xlab(\"Variable respuesta: deposit\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#visualizar-distribuciones",
    "href": "eda.html#visualizar-distribuciones",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.6 Visualizar distribuciones",
    "text": "2.6 Visualizar distribuciones\nLa forma de visualizar la distribución de una variable dependerá de si la variable es categórica o continua. Una variable es categórica si sólo puede tomar uno de un pequeño conjunto de valores. En R, las variables categóricas suelen guardarse como factores o vectores de caracteres. Para examinar la distribución de una variable categórica, utiliza un gráfico de barras:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = contact))\n\n\n\n\n\n\n\n\nPuedes obtener los valores exactos en cada categoría como sigue:\n\nbank.train%&gt;% \n  count(contact)\n\n# A tibble: 3 × 2\n  contact       n\n  &lt;chr&gt;     &lt;int&gt;\n1 cellular   4025\n2 telephone   383\n3 unknown    1173\n\n\nUna variable es continua si puede tomar cualquiera de un conjunto infinito de valores ordenados. Para examinar la distribución de una variable continua, utiliza un histograma:\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = age), binwidth = 5)\n\n\n\n\n\n\n\n\nUn histograma divide el eje \\(x\\) en intervalos equidistantes y, a continuación, utiliza la altura de una barra para mostrar el número de observaciones que se encuentran en cada intervalo. En el gráfico anterior, la primera barra muestra unas \\(100\\) observaciones (realmente son \\(119\\)) tienen un valor de edad por debajo de \\(22.5\\) años. Puede establecer la anchura de los intervalos en un histograma con el argumento binwidth, que se mide en las unidades de la variable \\(x\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nSiempre se deben explorar una variedad de anchos de intervalo cuando trabajamos con histogramas, ya que diferentes anchos de intervalo pueden revelar diferentes patrones.\n\n\nPodemos representar funciones de densidad de probabilidad.\n\nggplot(bank.train, aes(x = age)) +\ngeom_density() +\nggtitle('KDE de edad en datos bank')\n\n\n\n\n\n\n\n\nOtro gráfico muy utilizado para variables cuantitativas univariantes es el boxplot, también llamado box-and-whisker plot (diagrama de caja y bigotes). Es especialmente útil para detectar posibles datos atípicos en los valores de una variable, siempre que su distribución sea parecida a una distribución Normal. El gráfico muestra:\n\nUna caja cuyos límites son el primer y el tercel cuartil de la distribución de valores.\nUna línea central, que marca la mediana.\nLos bigotes, que por defecto (en R) se extienden hasta 1.5 veces el valor del rango intercuartílico (IQR) por encima y por debajo de la caja.\nPuntos individuales, que quedan más allá del límite de los bigotes, marcan posibles datos atípicos.\n\nEn distribuciones muy asimétricas o con muchos valores extremos, muy diferentes a una distribución Normal, aparecerán demasiados puntos más allá de los bigotes y no se podrán apreciar fácilmente los atípicos (demasiados puntos considerados como tales). En ese caso, es conveniente intentar una transformación de la variable antes de representar el boxplot.\n\nggplot(bank.train, aes(x=deposit, y=age, color=deposit)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#transformación-de-variables",
    "href": "eda.html#transformación-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.7 Transformación de variables",
    "text": "2.7 Transformación de variables\n\n2.7.1 Transformación de variables cuantitativas\nEn algunos métodos de inferencia estadística y aprendizaje automático será necesario contar con variables que cumplan requisitos de normalidad. Por ejemplo, si tomamos la transformación \\(log\\) sobre la variable edad obtenemos una distribución multimodal que, probablemente, corresponda a la combinación de dos (o más) normales.\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = log(age)), binwidth = .1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nLos modelos de aprendizaje serán tan buenos como lo sean las variables de entrada de dichos algoritmos.\n\n\n\n2.7.1.1 Transformaciones para igualar dispersión\nCon frecuencia, el objetivo de la transformación de variables cuantitativas es obtener una variable cuya distribución de valores sea:\n\nMás simétrica y con menor dispersión que la original.\nMás semejante a una distribución normal (e.g. para algunos modelos lineales).\nRestringida en un intervalo de valores (e.g. \\([0,1]\\) ).\n\nLa forma más sencilla de detectar que alguna de nuestras variables necesita ser transformada es representar un gráfico que muestre la distribución de valores de la variable. Por ejemplo, un histograma o un diagrama de densidad de probabilidad (o ambos).\nEl uso de los logaritmos tiene su propia recomendación en preparación de datos (Fox y Weisberg 2018):\n\n\n\n\n\n\nJohn Fox\n\n\n\n“Si la variable es estrictamente positiva, no tiene un límite superior para sus valores, y su rango abarca dos o más órdenes de magnitud (potencias de \\(10\\)), entonces la transformación logarítmica suele ser útil. A la inversa, cuando la variable tiene un rango de valores pequeño (menor de un orden de magnitud), el logaritmo o cualquier otra transformación simple no ayudará mucho.”\n\n\nLa versión general de esta transformación son las transformaciones de escala-potencia (scaled-power transformations), también denominadas transformaciones de Box-Cox.\n\\[x(\\lambda)= \\begin{cases} \\frac{x^\\lambda-1}{\\lambda},& \\text{cuando } \\lambda \\neq 0,\\\\ log_e(x), & \\text{cuando } \\lambda = 0 \\end{cases}\\]\nLa función car::symbox(...) permite probar varias combinaciones típicas del parámetro \\(\\lambda\\) , para comprobar con cuál de ellas obtenemos una distribución más simétrica de valores.\n\nlibrary(car)\n\nbank.train %&gt;% symbox(~ age, data = .)\n\n\n\n\n\n\n\n\n\n\n2.7.1.2 Transformaciones para igualar dispersión\nTambién es bastante común aplicar transformaciones en datos cuantitativos para igualar las escalas de representación de las variables. En muchos modelos, si una de nuestras variables tiene una escala mucho mayor que las demás, sus valores tienden a predominar en los resultados, enmascarando la influencia del resto de variables en el modelo.\nPor este motivo, en muchos modelos es importante garantizar que todas las variables se representan en escalas comparables, de forma que ninguna predomine sobre el resto. Conviene aclarar un poco algunos términos que se suelen emplear de forma indistinta:\n\nReescalado o cambio de escala: Consiste en sumar o restar una constante a un vector, y luego multiplicar o dividir por una constante. Por ejemplo, para transformar la unidad de medida de una variable (grados Farenheit → grados Celsius).\nNormalización: Consiste en dividir por la norma de un vector, por ejemplo para hacer su distancia euclídea igual a \\(1\\).\nEstandarización: Consiste en restar a un vector una medida de localización o nivel (e.g. media, mediana) y dividir por una medida de escala (dispersión). Por ejemplo, si restamos la media y dividimos por la desviación típica hacemos que la distribución tenga media \\(0\\) y desviación típica \\(1\\).\n\nAlgunas aternativas comunes son:\n\\[\nEstandarización \\rightarrow Y=\\frac{X-\\overline{x}}{s_x}\n\\]\n\\[\nEscalado \\space min-max \\rightarrow Y=\\frac{X-min_x}{max_x-min_x}\n\\]\nEn R, la función scale() se puede utilizar para realizar estas operaciones de estandarización. Automáticamente, puede actuar sobre las columnas de un data.frame, aplicando la misma operación a todas ellas (siempre que todas sean cuantitativas).\n\n\n\n2.7.2 Transformación de variables cualitativas\nA diferencia de las variables cuantitativas, que representan cantidades numéricas, las variables cualitativas, también conocidas como variables categóricas, se utilizan para describir características o cualidades que no tienen un valor numérico intrínseco. Las variables cualitativas son esenciales en la investigación y el análisis de datos, ya que a menudo se utilizan para clasificar, segmentar y comprender información sobre grupos, categorías o características. Algunas técnicas comunes para analizar variables cualitativas incluyen la creación de tablas de frecuencia para contar la ocurrencia de cada categoría y el uso de gráficos como gráficos de barras o diagramas de sectores para visualizar la distribución de categorías. Estos análisis pueden proporcionar información valiosa sobre patrones, tendencias y relaciones en los datos cualitativos, lo que puede ser fundamental para tomar decisiones informadas en una amplia gama de campos, desde marketing hasta investigación social y más.\nLas variables cualitativas se dividen en dos categorías principales:\n\n\n\n\n\n\nVariables Cualitativas Nominales\n\n\n\n\n\nLas variables nominales representan categorías o etiquetas que no tienen un orden inherente. Ejemplos comunes incluyen el género (masculino, femenino, otro), el estado civil (soltero, casado, divorciado) o los colores (rojo, azul, verde). No se pueden realizar operaciones matemáticas en variables nominales, como sumar o restar.\n\n\n\n\n\n\n\n\n\nVariables Cualitativas Ordinales\n\n\n\n\n\nLas variables ordinales representan categorías con un orden natural o jerarquía, pero la distancia entre las categorías no es necesariamente uniforme ni conocida. Ejemplos incluyen la calificación de satisfacción del cliente (muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho) o el nivel de educación (primaria, secundaria, universitaria). Aunque se pueden establecer comparaciones de orden (por ejemplo, “mayor que” o “menor que”), no es apropiado realizar operaciones matemáticas en variables ordinales.\n\n\n\nEn R, las variables categóricas se denominan factores (factors) y sus categorías niveles (levels). Es importante procesarlos adecuadamente para que los modelos aprovechen la información que contienen estas variables. Por otro lado, si se codifica incorrectamente esta información los modelos pueden estar realizando operaciones absurdas aunque nos devuelvan resultados aparentemente válidos.\n\n\n\n\n\n\nR\n\n\n\nPor defecto, R transforma columnas tipo string en factores al leer los datos de un archivo. Además, por defecto, R ordena los niveles de los factores alfabéticamente, según sus etiquetas. Debemos tener cuidado con esto, puesto que en muchos análisis es muy importante saber qué nivel se está tomando como referencia, de entre los valores posibles de un factor, para comparar con los restantes. En ciertos modelos, la elección como referencia de uno de los valores del factor (típicamente el primero que aparece en la lista de niveles) cambia por completo los resultados, así como la interpretación de los mismos.\n\n\nEn variables ordinales se debe respetar estrictamente el orden preestablecido de los niveles. Por ejemplo, una ordenación (“regular” &lt; “bueno” &lt; “malo”) es inaceptable. Para establecer una ordenación explícita entre los niveles hay que especificarla manualmente si no coincide con la alfabética, y además configurar el argumento ordered = TRUE en la función factor():\n\nsatisfaccion &lt;- rep(c(\"malo\", \"bueno\", \"regular\"), c(3,3,3))\nsatisfaccion &lt;- factor(satisfaccion, ordered = TRUE, levels = c(\"malo\", \"regular\", \"bueno\"))\nsatisfaccion\n\n[1] malo    malo    malo    bueno   bueno   bueno   regular regular regular\nLevels: malo &lt; regular &lt; bueno\n\n\nPara comprobar qué nivel se toma como referencia en cada uno de los factores de una base de datos usamos la funión levels():\n\nlevels(bank.train$marital)\n\nNULL\n\n\nY esto, ¿es correcto? Veamos la distribución de las observaciones en las categorías de la variable marital:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = marital))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\n\nHabitualmente será más recomendable elegir como categoría de referencia para variables categóricas aquella categoría con mayor número de observaciones.\n\n\nPor tanto, en este caso particular deberíamos modificar la categoría de referencia como sigue:\n\nreorder_marital = factor(bank.train$marital, levels=(c(' married', ' single', ' divorced')))\nlevels(reorder_marital)\n\n[1] \" married\"  \" single\"   \" divorced\"\n\n\nNótese que la nueva variable aquí creada, reorder_marital, no ha sido incluida (aún) en el tibble bank. Para ello:\n\nbank.train$marital = reorder_marital\n\n\n2.7.2.1 Conversión de variables cuantitativas a variables categóricas\nLa conversión de variables cuantitativas a variables categóricas es un proceso importante en EDA que implica transformar datos numéricos en categorías. Esto se realiza con el propósito de simplificar el análisis, resaltar patrones específicos y facilitar la interpretación de los resultados. A continuación, se destacan algunas situaciones comunes en las que se realiza esta conversión y cómo se lleva a cabo:\n\nAgrupación de datos numéricos: En ocasiones, es útil agrupar datos numéricos en intervalos o categorías para resaltar tendencias generales. Por ejemplo, en un estudio de edades de una población, en lugar de analizar cada edad individual, se pueden crear grupos como “menos de 18 años”, “18-30 años”, “31-45 años” y así sucesivamente.\nCreación de variables binarias: A menudo, se convierten variables numéricas en variables binarias (\\(1\\) o \\(0\\)) para simplificar el análisis. Por ejemplo, en un estudio de satisfacción del cliente, se puede crear una variable binaria donde “\\(1\\)” indica clientes satisfechos y “\\(0\\)” indica clientes insatisfechos.\nCategorización de variables continuas: Las variables continuas, como ingresos o puntuaciones, se pueden convertir en categorías para segmentar la población. Esto puede ser útil en análisis demográficos o de segmentación de mercado.\nSimplificación de modelos: Algunos modelos de ML pueden beneficiarse de la conversión de variables cuantitativas a categóricas para mejorar la interpretación y la eficacia del modelo.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEl proceso de conversión de variables cuantitativas a categóricas generalmente implica definir criterios o reglas claras para agrupar los valores numéricos en categorías significativas. Estos criterios pueden basarse en conocimiento previo del dominio, EDA o consideraciones específicas del problema. En esta etapa te vendrá genial contar con la ayuda de un experto en el dominio de aplicación, y puedes llevar a cabo cambios catastróficos en caso de no contar con esa ayuda.\n\n\nEs importante tener en cuenta que la conversión de variables cuantitativas a categóricas debe realizarse de manera cuidadosa y considerar el impacto en el análisis. La elección de cómo categorizar los datos debe estar respaldada por una comprensión sólida del problema y los objetivos del estudio. Además, se debe documentar claramente el proceso de conversión para que otros puedan replicarlo y comprender las categorías resultantes.\nA modo de ejemplo, vamos a categorizar la varible age en la base de datos bank. Para ello elegimos (elegimos!!!) las siguientes agrupaciones en la variable edad: (0,40],(40,60],(60,100].\n\n bank.train &lt;- within(bank.train, {   \n  age.cat &lt;- NA # need to initialize variable\n  age.cat[age &lt;= 40] &lt;- \"Low\"\n  age.cat[age &gt; 40 & age &lt;= 60] &lt;- \"Middle\"\n  age.cat[age &gt; 60] &lt;- \"High\"\n   } )\n\nbank.train$age.cat &lt;- factor(bank.train$age.cat, levels = c(\"Low\", \"Middle\", \"High\"))\nsummary(bank.train$age.cat)\n\n   Low Middle   High \n  3116   2151    314",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-comunes-y-atípicos",
    "href": "eda.html#valores-comunes-y-atípicos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.8 Valores comunes y atípicos",
    "text": "2.8 Valores comunes y atípicos\nLos gráficos de barras relacionados con variables cualitativas nos han ayudado a identificar los valores más frecuentes o las categorías más repetidas en esas variables. Estos gráficos reflejan la frecuencia de cada categoría, es decir, el número de veces que aparece en el conjunto de datos. A veces ese número se representa en porcentaje respecto al número total de observaciones, proporcionando una visión relativa de la prevalencia en cada categoría. La moda es la categoría que aparece con mayor frecuencia en el conjunto de datos. Es especialmente útil para identificar la categoría más común y es aplicable a variables categóricas.\nEn el caso de las variables cuantitativas, el histograma de frecuencias se convierte en una herramienta gráfica sumamente útil para alcanzar este mismo objetivo.\n\n2.8.1 Estadísticos resumen\nA continuación te explicamos algunas medidas que resumen el comportamiento de una variable aleatoria cuantitativa:\n\n\n\n\n\n\nMedia\n\n\n\n\n\nLa media aritmética es el promedio de todos los valores de la variable. Se calcula sumando todos los valores y dividiendo por el número de observaciones. La media proporciona una indicación de la tendencia central de los datos.\n\n\n\n\n\n\n\n\n\nMediana\n\n\n\n\n\nLa mediana es el valor central en un conjunto de datos ordenados en forma ascendente o descendente. Divide el conjunto de datos en dos mitades iguales. La mediana es menos sensible a valores extremos que la media y es especialmente útil cuando los datos no siguen una distribución (aproximadamente) normal.\n\n\n\n\n\n\n\n\n\nModa\n\n\n\n\n\nLa moda es el valor que ocurre con mayor frecuencia en un conjunto de datos. Puede haber una o más modas en un conjunto de datos, y esta medida es especialmente útil para variables discretas.\n\n\n\n\n\n\n\n\n\nRango\n\n\n\n\n\nEl rango es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Proporciona una indicación de la dispersión o variabilidad de los datos.\n\n\n\n\n\n\n\n\n\nDesviación Estándar\n\n\n\n\n\nLa desviación estándar mide la dispersión de los datos con respecto a la media, y tiene sus mismas unidades de medida. Valores más altos indican mayor variabilidad. Es especialmente útil cuando se asume una distribución normal.\n\n\n\n\n\n\n\n\n\nCuartiles y Percentiles\n\n\n\n\n\nLos cuartiles dividen un conjunto de datos en cuatro partes iguales, mientras que los percentiles dividen los datos en cien partes iguales. Los cuartiles y percentiles son útiles para identificar valores atípicos y comprender la distribución de los datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de Variación\n\n\n\n\n\nEl coeficiente de variación es una medida de la variabilidad relativa de los datos y se calcula como la desviación estándar dividida por la media. Se expresa como un porcentaje y es útil para comparar la variabilidad entre diferentes conjuntos de datos.\n\n\n\nEn R, podemos obtener algunos estadísticos resumen mediante la opción summary.\n\nsummary(bank.train$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   32.00   39.00   41.24   49.00   95.00 \n\n\nCuriosamente, R no tiene una función estándar incorporada para calcular la moda. Así que creamos una función de usuario para calcular la moda de un conjunto de datos en R. Esta función toma el vector como entrada y da el valor de la moda como salida.\n\n# Create the function.\nsummary_moda &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nsummary_moda(bank.train$age)\n\n[1] 31\n\n\n\n\n2.8.2 Valores atípicos\nLos valores atípicos (outliers en inglés) son observaciones inusuales, puntos de datos que no parecen encajar en el patrón o el rango de la variable estudiada. A veces, los valores atípicos son errores de introducción de datos; otras veces, sugieren nuevos datos científicos importantes.\nCuando es posible, es una buena práctica llevar a cabo el análisis con y sin los valores atípicos. Si se determina que su influencia en los resultados es insignificante y no se puede identificar su origen, puede ser razonable reemplazarlos con valores faltantes y continuar con el análisis. Sin embargo, si estos valores atípicos tienen un impacto sustancial en los resultados, no se deben eliminar sin una justificación adecuada. En este caso, será necesario investigar la causa subyacente (por ejemplo, un error en la entrada de datos) y documentar su exclusión en el informe correspondiente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-faltantes",
    "href": "eda.html#valores-faltantes",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.9 Valores faltantes",
    "text": "2.9 Valores faltantes\nLos valores faltantes (missing), también conocidos como valores nulos o valores ausentes, son observaciones o datos que no están disponibles o que no han sido registrados para una o más variables en un conjunto de datos. Estos valores pueden surgir por diversas razones, como errores de entrada de datos, respuestas incompletas en una encuesta, fallos en la medición o simplemente porque cierta información no está disponible en un momento dado.\n\n\n\n\n\n\nPara recordar\n\n\n\nLa presencia de valores faltantes en un conjunto de datos es un problema común en el análisis de datos y puede tener un impacto significativo en la calidad de los resultados. Es importante abordar adecuadamente los valores faltantes, ya que pueden sesgar los análisis y conducir a conclusiones incorrectas si no se manejan correctamente.\n\n\nAlgunas de las estrategias comunes para tratar los valores faltantes incluyen:\n\nEliminación de filas o columnas: Si la cantidad de valores faltantes es pequeña en comparación con el tamaño total del conjunto de datos, una opción es eliminar las filas o columnas que contengan valores faltantes. Sin embargo, esta estrategia puede llevar a la pérdida de información importante.\nImputación de valores: Esta estrategia implica estimar o llenar los valores faltantes con valores calculados a partir de otros datos disponibles. Esto puede hacerse utilizando técnicas como la imputación media (rellenar con la media de la variable), imputación mediana (rellenar con la mediana), imputación de vecinos más cercanos o técnicas más avanzadas como regresión u otras técnicas de modelado.\nMarcadores especiales: En algunos casos, es útil asignar un valor específico (como “N/A” o “-999”) para indicar que un valor está ausente. Esto puede ser útil cuando se desea mantener un registro explícito de los valores faltantes sin eliminarlos o imputarlos. Es importante que, en este caso, el valor asignado no tenga otro significado. Por ejemplo, asignamos “-999” como marcador de valor faltante y sin embargo, es un valor plausible dentro del rango de valores de la variable.\nMétodos basados en modelos: Utilizar modelos estadísticos o de aprendizaje automático para predecir los valores faltantes en función de otras variables disponibles. Esto puede ser especialmente eficaz cuando los datos faltantes siguen un patrón que puede ser capturado por el modelo.\n\nLa elección de la estrategia adecuada para tratar los valores faltantes depende del contexto del análisis, la cantidad de datos faltantes y la naturaleza de los datos. Es fundamental abordar este problema de manera cuidadosa y transparente, documentando cualquier procedimiento de imputación o tratamiento de valores faltantes utilizado en el análisis para garantizar la integridad y la validez de los resultados.\n\n\n\n\n\n\nPeligro\n\n\n\nSustituir valores faltantes por otros obtenidos con técnicas y métodos estadísticos o de aprendizaje automático siempre es un riesgo, pues implica “inventar” datos allá donde no los hay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#correlación-entre-variables",
    "href": "eda.html#correlación-entre-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.10 Correlación entre variables",
    "text": "2.10 Correlación entre variables\nExisten varios métodos y técnicas para estudiar la correlación entre variables, lo que ayuda a comprender las relaciones entre las diferentes características en un conjunto de datos. En próximos cursos estudiarás que es de especial interés estudiar las relaciones entre la variable objetivo y las variables explicativas.\nPuedes desplegar los paneles siguientes para averiguar alguno de los métodos más comunes.\n\n\n\n\n\n\nMatriz de correlación\n\n\n\n\n\nLa matriz de correlación es una tabla que muestra las correlaciones entre todas las combinaciones de variables en un conjunto de datos. Los valores de correlación varían entre \\(-1\\) y \\(1\\), donde \\(-1\\) indica una correlación negativa perfecta, \\(1\\) indica una correlación positiva perfecta y \\(0\\) indica la ausencia de correlación. Este método es especialmente útil para identificar relaciones lineales entre variables numéricas.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión\n\n\n\n\n\nLos gráficos de dispersión muestran la relación entre dos variables numéricas mediante puntos en un plano cartesiano. Estos gráficos permiten visualizar patrones de dispersión y tendencias entre las variables. Si los puntos se agrupan en una forma lineal, indica una posible correlación lineal.\n\n\n\n\n\n\n\n\n\nMapas de calor\n\n\n\n\n\nLos mapas de calor son representaciones visuales de la matriz de correlación en forma de un gráfico de colores. Permiten identificar rápidamente las relaciones fuertes o débiles entre variables y son útiles para resaltar patrones en grandes conjuntos de datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Pearson\n\n\n\n\n\nEste coeficiente mide la correlación lineal entre dos variables numéricas. Varía entre \\(-1\\) y \\(+1\\), donde valores cercanos a \\(-1\\) o \\(+1\\) indican una correlación fuerte, mientras que valores cercanos a \\(0\\) indican una correlación débil o nula.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Spearman\n\n\n\n\n\nEste coeficiente evalúa la correlación monotónica entre dos variables, lo que significa que puede detectar relaciones no lineales. Es útil cuando las variables no siguen una distribución normal.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Kendall\n\n\n\n\n\nSimilar al coeficiente de Spearman, evalúa la correlación entre variables, pero se centra en la concordancia de los rangos de datos, lo que lo hace útil para datos no paramétricos y muestras pequeñas.\n\n\n\n\n\n\n\n\n\nPruebas estadísticas\n\n\n\n\n\nLas pruebas estadísticas, como la prueba t de Student o la ANOVA, pueden utilizarse para evaluar si existe una diferencia significativa en los promedios de una variable entre diferentes categorías de otra variable. Si la diferencia es significativa, puede indicar una correlación entre las variables.\n\n\n\nVamos a estudiar la relación existente entre la variable objetivo deposit y la variable duration de la base de datos bank.\n\nggplot(bank.train, aes(x = log(duration), colour = deposit)) +\n  geom_density(lwd=2, linetype=1)\n\n\n\n\n\n\n\n\nPuede observarse una relación. Valores altos de la variable duración parecen estar relacionados con observaciones con deposit igual a ‘yes’.\n\ndf = bank.train %&gt;% \n      select(duration,deposit)%&gt;%\n      mutate(log.duration=log(duration))\n\n# Resumen para los casos de depósito\nsummary(df %&gt;% filter(deposit==\"yes\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.079   5.497   6.073   6.046   6.593   8.087 \n\n# Resumen para los casos de no depósito\nsummary(df %&gt;% filter(deposit==\"no\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6931  4.5433  5.0999  5.0308  5.6276  7.5022 \n\n\nGráficamente, podemos comparar los boxplots.\n\nggplot(df, aes(deposit, log.duration)) +\n        geom_boxplot()\n\n\n\n\n\n\n\n\nPodemos determinar la importancia de relación. Por ejemplo, podemos realizar un test de la T para igualdad de medias. Estudiaremos estos conceptos en el Capítulo 3.\n\nt.test(log.duration ~ deposit, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  log.duration by deposit\nt = -45.828, df = 5464.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -1.0583835 -0.9715488\nsample estimates:\n mean in group no mean in group yes \n         5.030821          6.045787 \n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nComprenderás este resultado a lo largo del curso. De momento, puedes preguntar al profesor. Dejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\nEs posible estudiar la relación entre dos variables categóricas de manera gráfica.\n\nggplot(data = bank.train, aes(x = housing, fill = deposit)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nParece haber una relación, estando asociados las observaciones de personas con casa propia a un mayor porcentaje de `no’ en la variable respuesta. Podemos obtener la tabla de contingencia:\n\ndata1=table(bank.train$housing, bank.train$deposit)\n\n\ndimnames(data1) &lt;- list(housing = c(\"no\", \"yes\"),\n                        deposit = c(\"no\", \"yes\"))\ndata1\n\n       deposit\nhousing   no  yes\n    no  1246 1688\n    yes 1662  985\n\n\nY el contraste correspondiente para la hipótesis nula de no existencia de relación. Estudiaremos estos conceptos en el Capítulo 3.\n\nchisq.test(bank.train$housing, bank.train$deposit)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bank.train$housing and bank.train$deposit\nX-squared = 229.44, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\n\n\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.\n\n\nTukey, John W et al. 1977. Exploratory data analysis. Vol. 2. Reading, MA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "para.html",
    "href": "para.html",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "3.1 Definición de estadístico\nUn estadístico es una medida calculada a partir de una muestra de datos que se utiliza para describir o resumir características de la muestra. En otras palabras, un estadístico es un valor numérico que resume o describe algún aspecto de los datos recolectados. Los estadísticos se utilizan ampliamente en análisis de datos, inferencia estadística y para hacer estimaciones sobre poblaciones más grandes basadas en la información obtenida de una muestra.\nSi te fijas bien, verás que en el Capítulo 2 ya hemos estado trabajando con estadísticos. Los estadísticos juegan un papel crucial en la inferencia estadística, donde se utilizan para hacer estimaciones o probar hipótesis sobre una población a partir de la información contenida en una muestra.\nEjemplos comunes de estadísticos incluyen:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#definición-de-estadístico",
    "href": "para.html#definición-de-estadístico",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "Media: Promedio aritmético de los valores de una variable en la muestra.\nMediana: Valor que divide la muestra en dos partes iguales respecto a una variable.\nModa: Valor de una variable que aparece con mayor frecuencia en la muestra.\nVarianza: Medida de la dispersión de los datos respecto a la media.\nDesviación estándar: Raíz cuadrada de la varianza, que también mide la dispersión.\nCoeficiente de correlación: Medida de la relación entre dos variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-puntual",
    "href": "para.html#estimación-puntual",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.2 Estimación puntual",
    "text": "3.2 Estimación puntual\nLa estimación puntual es una técnica estadística que consiste en utilizar los datos de una muestra para calcular un valor único, denominado estimador puntual, que se usa como mejor aproximación de un parámetro desconocido de la población. Este parámetro puede ser, por ejemplo, la media, la varianza, la proporción, entre otros. La estimación puntual proporciona una forma simple y directa de hacer inferencias sobre parámetros poblacionales a partir de una muestra, aunque su simplicidad también implica que no proporciona información sobre la precisión o variabilidad de la estimación, aspectos que se abordan mediante la estimación por intervalos y otras técnicas inferenciales.\n\n3.2.1 Conceptos clave en la estimación puntual\nEstimador: Es una fórmula o función que se aplica a los datos de la muestra para obtener la estimación puntual. Por ejemplo, la media muestral (\\(\\bar{x}\\)) es un estimador de la media poblacional (\\(\\mu\\)). Formalmente, dada una variable aleatoria \\(X\\) con función de distribución \\(F_\\theta\\), con parámetro \\(\\theta\\) desconocido, un estadístico o estimador \\(T = T(X_1,\\ldots,X_n)\\) es una función real de la muestra aleatoria simple (m.a.s.) \\((X_1,\\ldots, X_n)\\) que estima el valor del parámetro desconocido. \\[\nT = T(X_1,\\ldots, X_n) = \\hat{\\theta}\n\\] Un estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución que se denomina distribución muestral.\nPor ejemplo, la media \\[T_1(X_1,\\ldots,X_n)=\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\\] y la mediana \\[T_2(X_1,\\ldots,X_n)=\\frac{X_{(n/2)}+X_{(n/2+1)}}{2}\\] son estimadores.\nEstimación: Es el valor numérico específico obtenido al aplicar el estimador a una muestra concreta de datos. Por ejemplo, si \\(\\bar{x} = 5.4\\), esa es la estimación puntual de \\(\\mu\\).\n\n\n3.2.2 Ejemplos de estimadores puntuales\n\nMedia muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)). Aplicamos el estimador a una realización de la muestra (\\(x_1,\\ldots,x_n\\)), obteniendo el estimador muestral: \\[\n\\bar{x} = T_1(x_1,\\ldots,x_n)= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nVarianza muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)). \\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nDesviación típica muestral (\\(s\\)). Es la raíz cuadrada de la varianza muestral. Tiene las mismas unidades de medida que la variable original.\nProporción muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)). \\[\n\\hat{p} = \\frac{x}{n}\n\\] donde \\(x\\) es el número de éxitos en la muestra y \\(n\\) es el tamaño de la muestra.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#propiedades-de-los-estimadores",
    "href": "para.html#propiedades-de-los-estimadores",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.3 Propiedades de los estimadores",
    "text": "3.3 Propiedades de los estimadores\nExisten diferentes métodos para obtener estimadores de un parámetro poblacional. ¿Cómo elegir el estimador más adecuado para un parámetro desconocido? ¿Cuáles son las propiedades de un buen estimador?\nPara que un estimador sea considerado adecuado, generalmente debe cumplir con ciertas propiedades:\n\nInsesgadez: Un estimador es insesgado (o centrado) si, en promedio, coincide con el valor verdadero del parámetro que se estima. Es decir, el valor esperado del estimador es igual al parámetro poblacional.\n\n\\[E(\\hat{\\theta}) = \\theta\\] La comparaciones que implican estimadores sesgados a menudo se basan en el error cuadrático medio definido como: \\[\nECM(\\hat{\\theta})=E[(\\hat{\\theta}- \\theta)^2]=Var(\\hat{\\theta})+(E(\\hat{\\theta})- \\theta)^2=Eficiencia+  Sesgo\n\\] En este grado vas a volver a oir hablar de esta medida en la asignatura de regresión. En ese caso, la medida de error más empleada es: \\[\nECM=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\n\\] donde \\(\\hat{f}(x_i)\\) e sla predicción que hace un modelo de regresión mediante una función \\(\\hat{f}\\) para la i-ésima observación muestral \\(x_i\\).\n\nConsistencia: Un estimador es consistente si, a medida que el tamaño de la muestra aumenta, la estimación se aproxima al valor verdadero del parámetro. Es decir: \\[\nlim_{n  \\rightarrow \\infty}P(|\\hat{\\theta}-\\theta|\\geq\\delta)=0, \\forall\\delta&gt;0\n\\] donde \\(n\\) es el tamaño muestral.\nEficiencia: La varianza de un estimador debe ser lo más pequeña posible. Entre dos estimadores insesgados, el más eficiente es el que tiene menor varianza, es decir, el que proporciona estimaciones más precisas.\nSuficiencia: Un estimador es suficiente si utiliza toda la información contenida en la muestra sobre el parámetro que se está estimando.\n\n\n\n\n\n\n\nEjemplo Práctico. Insesgadez\n\n\n\n\n\nPara entender la propiedad de insesgadez en inferencia estadística, es útil realizar una simulación en R. Como hemos visto, la insesgadez de un estimador significa que, en promedio, el estimador coincide con el parámetro verdadero de la población.\nVamos a realizar una simulación para ilustrar esta propiedad utilizando la media muestral como estimador de la media poblacional. Generaremos muchas muestras aleatorias de una distribución normal y compararemos la media de las medias muestrales con la media verdadera de la población.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(123)  # Para reproducibilidad\nn_muestras &lt;- 1000  # Número de muestras\ntamano_muestra &lt;- 30  # Tamaño de cada muestra\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\n\n# Generar muestras y calcular medias muestrales\nmedias_muestrales &lt;- numeric(n_muestras)\nfor (i in 1:n_muestras) {\n  muestra &lt;- rnorm(tamano_muestra, mean = media_poblacional, sd = desviacion_estandar)\n  medias_muestrales[i] &lt;- mean(muestra)\n}\n\n# Calcular la media de las medias muestrales\nmedia_de_medias_muestrales &lt;- mean(medias_muestrales)\n\n# Imprimir resultados\ncat(\"Media verdadera de la población:\", media_poblacional, \"\\n\")\n\nMedia verdadera de la población: 50 \n\ncat(\"Media de las medias muestrales:\", media_de_medias_muestrales, \"\\n\")\n\nMedia de las medias muestrales: 49.93807 \n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(medias_muestrales)\n\n# Graficar las medias muestrales usando ggplot2\nggplot(datos, aes(x = medias_muestrales)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = media_poblacional), color = \"red\", linetype = \"dashed\", size = 1.2) +\n  geom_vline(aes(xintercept = media_de_medias_muestrales), color = \"blue\", linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\",\n       y = \"Frecuencia\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = media_poblacional, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media Verdadera\", color = \"red\", angle = 90, vjust = -0.5) +\n  annotate(\"text\", x = media_de_medias_muestrales, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media de las Medias Muestrales\", color = \"blue\", angle = 90, vjust = 1.5)\n\n\n\n\n\n\n\n\nHemos creado un bucle para generar n_muestras muestras aleatorias de una distribución normal con la media y desviación estándar especificadas. Para cada muestra, calculamos la media muestral y la almacenamos en el vector medias_muestrales. Calculamos la media de todas las medias muestrales generadas y mostramos la media verdadera de la población y la media de las medias muestrales.\nEl gráfico resultante muestra un histograma de las medias muestrales con líneas verticales indicando la media verdadera de la población y la media de las medias muestrales. Esto ilustra visualmente la propiedad de insesgadez del estimador de la media.\n\n\n\n\n\n\n\n\n\nEjemplo Práctico. Consistencia\n\n\n\n\n\nPara ilustrar la propiedad de consistencia de un estimador, podemos realizar una simulación similar a la anterior, pero en lugar de enfocarnos en la media de las medias muestrales, nos centraremos en cómo el estimador se aproxima a la verdadera media poblacional a medida que aumenta el tamaño de la muestra. En otras palabras, mostraremos cómo el estimador se vuelve más preciso a medida que se incrementa el tamaño de la muestra.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(12443)  # Para reproducibilidad\nn_simulaciones &lt;- 1000  # Número de simulaciones\ntamanos_muestra &lt;- seq(10, 1000, by = 10)  # Tamaños de las muestras\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\nmedias_estimadas &lt;- numeric(length(tamanos_muestra))  # Vector para almacenar medias estimadas\n\n# Realizar simulaciones para diferentes tamaños de muestra\nfor (i in 1:length(tamanos_muestra)) {\n  # Generar muestras y calcular medias muestrales\n  medias_muestrales &lt;- replicate(n_simulaciones, mean(rnorm(tamanos_muestra[i], mean = media_poblacional, sd = desviacion_estandar)))\n  # Calcular la media de las medias muestrales\n  medias_estimadas[i] &lt;- mean(medias_muestrales)\n}\n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(tamanos_muestra, medias_estimadas)\n\n# Graficar las medias estimadas vs. el tamaño de la muestra usando ggplot2\nggplot(datos, aes(x = tamanos_muestra, y = medias_estimadas)) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = media_poblacional, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Convergencia del Estimador a la Media Verdadera\",\n       x = \"Tamaño de la Muestra\",\n       y = \"Media Estimada\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nEl gráfico resultante muestra cómo las medias estimadas convergen hacia la media verdadera de la población a medida que aumenta el tamaño de la muestra. Esto ilustra la propiedad de consistencia del estimador. La línea roja representa la media verdadera de la población, mientras que la línea azul representa las medias estimadas en función del tamaño de la muestra. A medida que el tamaño de la muestra aumenta, las medias estimadas se acercan cada vez más a la media verdadera.\n\n\n\n\n3.3.1 Distribuciones muestrales\nHemos visto que usamos estadísticos para estimar los parámetros desconocidos de la población. Estamos interesados en estadísticos con buenas propiedades. Además, estos estadísticos son variables aleatorias con distribución de probabilidad.\nSabemos, por el TCL visto en la Introducción, que, teniendo el tamaño muestral adecuado, la distribución de los estadísticos será una Normal.\nPor ejemplo, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) conocida, la media muestral: \\[\n\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\n\\] tiene media igual a \\(E[\\bar{X}]=\\mu\\) y varianza igual a \\(V[\\bar{X}]=\\sigma^2/n\\).\nEntonces, por el TCL, con \\(n\\) suficientemente grande: \\[\n\\bar{X} \\sim N \\left ( \\mu,\\frac{\\sigma^2}{n} \\right )\n\\]\nOtro ejemplo sería, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con una distribución Bernoulli de parámetro \\(p\\). Para \\(n\\) suficientemente grande se tiene que: \\[\n\\hat{p} \\sim N \\left (p,\\frac{p(1-p)}{n} \\right ) \\Leftrightarrow \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-los-momentos",
    "href": "para.html#método-de-los-momentos",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.4 Método de los momentos",
    "text": "3.4 Método de los momentos\nEl método de los momentos es una técnica utilizada en estadística para estimar los parámetros desconocidos de una distribución de probabilidad. Fue introducido por el estadístico Karl Pearson en 1984. Este método se basa en igualar los momentos muestrales (calculados a partir de los datos observados) con los momentos teóricos (expresados en términos de los parámetros de la distribución).\n\n3.4.1 Definición de Momentos\nEn estadística, los momentos de una distribución son medidas que describen diversas características de la distribución, como su media, varianza, simetría y curtosis. Los momentos más comunes son:\n\nPrimer Momento (Media): \\(\\mu = E[X]\\)\nSegundo Momento (Varianza):\\(\\mu_2 = E[X^2]\\)\nTercer Momento (Asimetría):\\(\\mu_3 = E[X^3]\\)\nCuarto Momento (Curtosis):\\(\\mu_4 = E[X^4]\\)\n\n\n\nk-esimo Momento:\\(\\mu_k = E[X^k]\\)\n\nLos momentos poblacionales pueden ser vistos como funciones de los parámetros desconocidos \\(\\theta_1,\\ldots,\\theta_k\\). Se asume que se conoce el modelo de probabilidad de la variable objeto de estudio.\n\n\n3.4.2 Pasos del Método de los Momentos\nEl método de los momentos consiste en resolver un conjunto de ecuaciones y tiene los siguientes pasos:\n\nCalcular momentos muestrales: Se calculan los momentos muestrales de los datos observados. El (k)-ésimo momento muestral se define como: \\(m_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\\) donde \\(n\\) es el tamaño de la muestra y \\(X_i\\) son los valores de la muestra.\nIgualar momentos muestrales y teóricos: Se igualan los momentos muestrales con los momentos teóricos de la distribución. Los momentos teóricos se expresan en términos de los parámetros desconocidos que se desean estimar.\nResolver el sistema de ecuaciones: Se resuelve el sistema de ecuaciones resultante para encontrar los estimadores de los parámetros desconocidos. Fíjate que tenemos \\(k\\) ecuaciones y \\(k\\) parámetros (\\(\\theta_1,\\ldots,\\theta_k\\)). De modo que es posible despejar los parámetros de estas ecuaciones, que quedando estos parámetros en función de los momentos. En estas ecuaciones se sustituyen los momentos poblacionales por sus correspondientes momentos poblacionales. Esto da como resultado estimaciones de esos parámetros.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que deseamos estimar los parámetros (\\(\\mu\\)) y (\\(\\sigma^2\\)) de una distribución Normal (\\(N(\\mu, \\sigma^2)\\)).\n\nCalcular los momentos muestrales: \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i \\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^nX_i^2 \\]\nIgualar los momentos muestrales con los momentos teóricos: Para una distribución Normal, el primer momento teórico (media) es\n\n\\[\\mu_1=E(X)=\\mu\\]\ny el segundo momento teórico es:\n\\[\\mu_2=E(X^2)=Var(X)+E(X)^2=\\mu^2 + \\sigma^2\\]\nIgualando estos con los momentos muestrales obtenidos de los datos: \\[m_1 = \\bar{X}=\\mu\\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2=\\mu_2= \\mu^2 + \\sigma^2\\]\n\nResolver el sistema de ecuaciones: De la primera ecuación, tenemos:\n\n\\[\\hat{\\mu} = \\bar{X}\\]\nSustituyendo en la segunda ecuación: \\[\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n X_i^2-\\bar{X}^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\]\nQue son los estimadores de los parámetros.\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSea \\(X_1,\\ldots,X_n\\) una muestra aleatoria simple de una \\(Binom(k,p)\\), con \\(k\\) y \\(p\\) desconocidos. Entonces, los momentos poblacionales son:\n\\[\\mu_1=E(X)=kp\\] \\[\\mu_2=E(X^2)=Var(X)+E(X)^2=kp(1-p)+k^2p^2\\] Los momentos muestrales son:\n\\[\nm_1=\\bar{X}\n\\] \\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2\n\\] Igualando los momentos poblacionales a los muestrales, obtenemos:\n\\[\nm_1=\\bar{X}=kp\n\\]\n\\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2=\\mu_2=kp(1-p)+k^2p^2\n\\] Despejando \\(k\\) y \\(p\\), obtenemos los estimadores:\n\\[\n\\hat{k}^2=\\frac{\\bar{X}^2}{\\bar{X}-(1/n)\\sum_{i=1}^n(X_i- \\bar{X})^2}\n\\] \\[\\hat{p}=\\frac{\\bar{X}}{\\hat{k}}\\]\n\n\n\n\n\n3.4.3 Ventajas y limitaciones\nLos estimadores de los momentos presentan interesantes propiedades estadísticas, aunqeu también tienen sus limitaciones.\nVentajas:\n\nSimplicidad: El método de los momentos es relativamente sencillo de aplicar y no requiere técnicas complejas de optimización.\nIntuición: Ofrece una interpretación intuitiva de los parámetros en términos de momentos.\n\nLimitaciones:\n\nPrecisión: Los estimadores de los momentos no siempre son los estimadores más eficientes (no tienen la mínima varianza posible).\nAplicabilidad: En algunas distribuciones complejas, los momentos pueden no existir o ser difíciles de calcular.\nConsistencia: Los estimadores de momentos no siempre son consistentes, especialmente en muestras pequeñas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-la-máxima-verosimilud",
    "href": "para.html#método-de-la-máxima-verosimilud",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.5 Método de la máxima verosimilud",
    "text": "3.5 Método de la máxima verosimilud\nEl método de la máxima verosimilitud es una técnica estadística ampliamente utilizada para estimar los parámetros desconocidos de una distribución de probabilidad. Este método se basa en encontrar los valores de los parámetros que maximicen la función de verosimilitud, la cual mide la probabilidad de observar los datos dados los parámetros. El método de máxima verosimilitud es el método más popular para obtener un estimador. La idea básica es seleccionar el valor del parámetro que hace que los datos sean más probables.\nDado un modelo estadiıstico (es decir, una familia de distribuciones \\(f(·|\\theta)| \\theta \\in \\Theta\\) donde \\(\\theta\\) es el parámetro del modelo), el método de máxima verosimilitud encuentra el valor del parámetro del modelo \\(\\theta\\) que maximiza la función de verosimilitud:\n\\[\n\\hat{\\theta}(x)= \\max_{\\theta \\in \\Theta}L(\\theta|\\mathbf{x})\n\\]\nPara una muestra aleatoria \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\) de una variable aleatoria \\(X\\), la verosimilitud es proporcional al producto de las probabilidades asociadas a los valores individuales: \\[\n\\prod_jP(X=x_j)\n\\] El término verosimilitud fue acuñado por Sir Roland Fisher.\nCuando \\(X\\) es una variable aleatoria continua, un valor muestral \\(x_j\\) debe considerarse como que está (en general) en el intervalo \\((x_j-\\delta,x_j+\\delta)\\), donde \\(\\delta\\) representa la precisión de la medición. La verosimilutd es entonces proporcional a: \\[\n\\prod_jP(x_j-\\delta&lt;X&lt;x_j+\\delta).\n\\] Si \\(\\delta\\) es suficientemente pequeño, esta expresión es aproximadamente proporcional a: \\[\n\\prod_jf(x_j).\n\\] donde \\(f\\) es la función de densidad de \\(X\\). Por lo tanto, la verosimilitd describe lo plausible que es un valor del parámetro poblacional, dadas unas observaciones concretas de la muestra.\n\n3.5.1 Conceptos básicos\n\nFunción de verosimilitud: La función de verosimilitud, \\(L(\\theta|\\mathbf{x})\\), para un conjunto de datos \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) y un vector de parámetros \\(\\theta\\), es el producto de las funciones de densidad (o de probabilidad) de los datos observados, dadas las posibles realizaciones de \\(\\theta\\): \\[\nL(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)=f(x_1,\\ldots,x_n|\\theta)=f(x_1|\\theta)f(x_2|\\theta)\\ldots f(x_n|\\theta)=\\prod_{i=1}^n f(x_i| \\theta)\n\\] donde \\(f(x_i|\\theta)\\) es la función de densidad (o de probabilidad) de \\(x_i\\) dado \\(\\theta\\).\nLog-Verosimilitud: Debido a que la función de verosimilitud puede implicar productos de muchos términos, es más práctico trabajar con su logaritmo natural, conocido como la log-verosimilitud: \\[\n\\ell(\\theta|\\mathbf{x}) = \\log L(\\theta|\\mathbf{x}) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\]\n\n\n\n3.5.2 Procedimiento del método de Máxima Verosimilitud\n\nEspecificar la función de verosimilitud: Identificar la función de verosimilitud correspondiente a los datos observados y a la distribución supuesta.\nCalcular la Log-Verosimilitud: Tomar el logaritmo natural de la función de verosimilitud para obtener la función de log-verosimilitud.\nDerivar y resolver: Derivar la función de log-verosimilitud con respecto a cada parámetro y resolver las ecuaciones obtenidas igualando a cero (puntos críticos) para encontrar los estimadores de máxima verosimilitud (EMV).\nVerificar máximos: Asegurarse de que las soluciones encontradas corresponden a máximos y no a mínimos o puntos de inflexión, típicamente verificando la segunda derivada.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que tenemos una muestra \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una distribución Normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\), y queremos estimar estos parámetros.\n\nFunción de verosimilitud: La función de densidad para una distribución normal es: \\[\nf(x_i|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\] Por lo tanto, la función de verosimilitud es: \\[\nL(\\mu, \\sigma^2| \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(\\mu, \\sigma^2|\\mathbf{x}) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(\\mu\\) y \\(\\sigma^2\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{i=1}^n \\frac{x_i - \\mu}{\\sigma^2} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\] \\[\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (x_i - \\mu)^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n\\]\nAsí, los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\) son la media muestral y la varianza muestral, respectivamente.\n\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSupongamos que tenemos una muestra de tamaño \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una variable aleatoria Binomial con parámetro \\(p\\) que deseamos estimar. Se emplea dicha variable para describir el número de errores en las \\(n\\) pruebas asociadas. Se realiza un experimento y se obtienen un total de \\(4\\) errores en las \\(10\\) pruebas.\n\nFunción de verosimilitud: La verosimilud viene dada por:\n\n\\[\nL(p|\\mathbf{x}) = {10 \\choose 4}p^4(1-p)^6\n\\]\n\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(p|\\mathbf{x}) = log\\left({10 \\choose 4}\\right) +4log(p)+6log(1-p)\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(p\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial p} =\\frac{4}{p} -\\frac{6}{1-p}= 0 \\implies \\frac{4}{p}=\\frac{6}{1-p}\\implies 4-4p=6p\\implies 4=10p \\implies p=4/10=0.4\n\\]\n\nLa función de verosimilud nos informa, dados los datos, sobre los valores más plausibles (o creíbles) para el parámetroo \\(p\\).\n\n\n\n\n\n3.5.3 Ventajas y limitaciones\nLos estimadores de máxima verosimilutd presentan buenas propiedades estadísticas.\nVentajas:\n\nConsistencia: Los estimadores de máxima verosimilitud son consistentes, es decir, convergen en probabilidad al valor verdadero del parámetro a medida que el tamaño de la muestra aumenta.\nEficiencia: En muchos casos, los estimadores de máxima verosimilitud son eficientes, alcanzando la varianza mínima entre los estimadores insesgados (cumplen la igualdad de Cramér-Rao). El estimador máximo verosimil es asintóticamente eficiente y su distribución converge a la distribución Normal con valor esperado \\(\\theta\\) y la varianza es igual al inverso de la información de Fisher. La informacioon de Fisher es la cantidad de información que una muestra proporciona sobre el valor de un parámetro desconocido.\nFlexibilidad: Se puede aplicar a una amplia gama de distribuciones y modelos complejos.\nInvariantes: Si \\(T\\) es el estimador de máxima verosimilitud para \\(\\theta\\), entonces \\(\\tau(T)\\) es el estimador de máxima verosimilutd para \\(\\tau(\\theta)\\) para cualquier función \\(\\tau\\).\n\nLimitaciones:\n\nComplejidad computacional: Encontrar los estimadores de máxima verosimilitud puede implicar resolver ecuaciones no lineales, lo cual puede ser complejo y requerir técnicas numéricas.\nExistencia y unicidad: Los estimadores de máxima verosimilitud no siempre existen y, si existen, no siempre son únicos. En problemas reales, la derivada de la función de verosimilitud es, a veces, analíticamente intratable. En esos casos, se utilizan métodos iterativos para encontrar soluciones numéricas para las estimaciones de los parámetros.\nSesgo en muestras pequeñas: Los estimadores pueden ser sesgados en muestras pequeñas, aunque el sesgo disminuye a medida que el tamaño de la muestra aumenta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-por-intervalo",
    "href": "para.html#estimación-por-intervalo",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.6 Estimación por intervalo",
    "text": "3.6 Estimación por intervalo\nLa estimación puntual proporciona una aproximación razonable para un parámetro de la población, pero no tiene en cuenta la variabilidad debido al tamaño muestral, la variabilidad en la población, el conocimiento de otros parámetros, etc.\nLa estimación por intervalo es una técnica en estadística que, a diferencia de la estimación puntual que proporciona un único valor, ofrece un rango de valores dentro del cual se espera que se encuentre el verdadero parámetro poblacional desconocido con un cierto nivel de confianza. Este rango se denomina intervalo de confianza.\nLa estimación por intervalos es una herramienta esencial en la Inferencia Estadística, ya que no solo ofrece una estimación del parámetro poblacional, sino que también proporciona un marco para entender la precisión y confiabilidad de esa estimación. Esto la convierte en una técnica poderosa para hacer inferencias más robustas y útiles basadas en datos muestrales.\n\n3.6.1 Conceptos clave en la estimación por intervalo\nIntervalo de Confianza (IC): Es un rango de valores calculado a partir de los datos de la muestra, que se utiliza para estimar el parámetro poblacional desconocido. Se expresa comúnmente como \\((\\text{Límite Inferior}, \\text{Límite Superior})\\).\nDada una muestra aleatoria simple \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\) de una población \\(X\\) con función de distribución \\(F\\) que depende de un parámetro desconocido \\(\\theta\\), diremos que un estimador por intervalos de confianza del parámetro \\(\\theta\\) con un nivel de confianza de \\((1-\\alpha)=100*(1-\\alpha)\\%\\) es un intervalo de la forma \\((T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X}))\\) que satisface: \\[P(\\theta \\in (T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X})))=1-\\alpha\\]\nNivel de Confianza: Es la probabilidad de que el intervalo de confianza contenga el verdadero valor del parámetro poblacional. Se denota como \\(1 - \\alpha\\), donde \\(\\alpha\\) es el nivel de significancia. Un nivel de confianza común es el \\(95\\%\\), lo que significa que estamos un \\(95\\%\\) seguros de que el intervalo contiene el parámetro verdadero. Si repetimos el experimento \\(N\\) veces, en el \\(95\\%\\) de las ocasiones el verdadero valor del parámetro estará incluido en el intervalo proporcionado. Sin embargo es importante señalar que, dado que el experimento solo suele realizarse en una ocasión, no podemos estar seguros de que el verdadero valor del parámetro está incluido en nuestro intervalo. Estará incluido o no estará incluido, pero no podemos saber en qué situación nos encontramos. Estar seguro sería tanto como decir que conocemos el verdadero valor del parámetro. En ese caso, obviamente, no necesitaríamos estimación ninguna.\nError Estándar (SE): Es una medida de la variabilidad de un estimador. Se utiliza para calcular los límites del intervalo de confianza.\n\n\n3.6.2 Cálculo del Intervalo de Confianza\nEl cálculo de un intervalo de confianza generalmente sigue la fórmula:\n\\[\n\\text{Estimación Puntual} \\pm (\\text{Valor Crítico} \\times \\text{Error Estándar})\n\\] Para alcanzar el intervalo de confianza, generalmente se busca una cantidad (aleatoria) \\(C(\\mathbf{X},\\theta)\\) relacionada con el parámetro desconocido \\(\\theta\\) y con la muestras \\(\\mathbf{X}\\), cuya distribución sea conocida y no dependa del valor del parámetro. Esta cantidad recibe el nombre de pivote o cantidad pivotal para \\(\\theta\\).\nDado que conocemos la distribución del pivote, podemos usar los cuartiles \\(1-\\alpha/2\\) y \\(\\alpha/2\\) de dicha distribución, y la desviación estándar dle estimador por intervalos de confianza, para plantear la siguiente ecuación: \\[\nP(1-\\alpha/2 \\text{ cuantil}&lt; C(\\mathbf{X},\\theta)&lt;\\alpha/2 \\text{ cuantil}) = 1- \\alpha\n\\]Para obtener los extremos (inferior y superior) del estimador por intervalos de confianza \\(T_{inf}(\\mathbf{X})\\) y \\(T_{sup}(\\mathbf{X})\\), se resuelve la doble desigualdad en \\(\\theta\\). De este modo el intervalo de confianza al \\(100(1-\\alpha)\\%\\) para \\(\\theta\\) es \\((T_{inf}(\\mathbf{x}),T_{sup}(\\mathbf{x}))\\)\n\n\n3.6.3 Importancia de la estimación por intervalos\nA diferencia de la estimación puntual, el intervalo de confianza proporciona información sobre la precisión de la estimación y la variabilidad inherente en los datos muestrales.\nAdemás, la estimación por intervalo proporciona un rango de valores que es útil para la toma de decisiones en el dominio de aplicación.\nPodemos señalar que la estimación por intervalos es menos susceptible a errores muestrales y proporciona una medida más realista del parámetro poblacional que la obtenida con la estimación puntual.\n\n\n3.6.4 Intervalo de Confianza para la media (cuando la varianza es conocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza conocida (\\(\\sigma^2\\)). Fijate que este último supuesto es muy poco realista (no conocemos la media, pero conocemos la varianza). En este caso, el estadístico \\(\\bar{X}\\) tiene una distribución normal: \\[\n\\bar{X} \\sim N \\left( \\mu,\\sigma_{\\bar{X}}=\\frac{\\sigma}{\\sqrt{n}}\\right )\n\\]La desviación típica de \\(\\bar{X}\\) ( o de cualquier otro estadístico) se conoce como su error estándar.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nZ=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N \\left( 0,1 \\right )\n\\] Ahora, si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuartiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de la distribución \\(N(0,1\\), entonces tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP\\left (z_{1-\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2} \\right)=1-\\alpha\n\\] Hay que notar que para la distribución Normal: \\(z_{1-\\alpha/2}=-z_{\\alpha/2}\\)\nResolvemos la doble desigualdad para \\(\\mu\\): \\[\n-z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}\n\\] \\[\n-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}&lt;\\bar{X}-\\mu&lt;z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}-\\bar{X}&lt;-\\mu&lt;-\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}+\\bar{X}&gt;\\mu&gt;\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] De modo que el estimador por intervalos de confianza es: \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] y por tanto, el intervalo de confianza para la media se calcula como: \\[     \nIC_{1-\\alpha}(\\mu)=\\left (\\bar{x} -z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}},\\bar{x} +z_{\\alpha/2}  \\frac{\\sigma}{\\sqrt{n}} \\right )= \\left( \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] donde \\(\\bar{x}\\) es la media muestral, \\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) para el nivel de confianza deseado, \\(\\sigma\\) es la desviación estándar poblacional, y \\(n\\) es el tamaño de la muestra.\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal\n\n\n\n\n\nSe ha probado que la altura de los alumnas de primer curso de la URJC se puede aproximar mediante una variable aleatoria con distribución normal con desviación típica \\(\\sigma=10\\) cm pero la media \\((\\mu)\\) desconocida. En un estudio con \\(50\\) alumnas se obtiene una media de \\(166\\) cm. Vamos a construir un intervalo de confianza al \\(95\\%\\) para \\(\\mu\\).\nSea \\(X\\) la altura, y sabemos que las variables independientes y identicamente distribuidas: \\[\nX_1,X_2,\\ldots,X_{50}\\sim N(\\mu,\\sigma^2=10^2).\n\\] Dado que: \\[\n\\frac{\\sigma^2}{n}=\\frac{10^2}{50}=2,\n\\] sabemos que: \\[\n\\bar{X}\\sim N(\\mu,2),\n\\] y por tanto: \\[\n\\frac{\\bar{X}-\\mu}{\\sqrt{2}}\\sim N(0,1).\n\\] Además los cuartiles de la distribución normal nos dicen que si \\(Z\\sim N(0,1)\\), entonces: \\[\nP(-1,96&lt;Z&lt;1,96)=0,95.\n\\] Por tanto: \\[\nP\\left(-1,96&lt;\\frac{\\bar{X}-\\mu}{\\sqrt{2}}&lt;1,96\\right)=0,95.\n\\] Despejamos \\(\\mu\\):\n\\[\nP\\left(\\bar{X}-1,96\\sqrt{2}&lt;\\mu&lt;\\bar{X}+1,96\\sqrt{2}\\right)=0,95.\n\\] Por tanto, si \\(\\bar{x}\\) es una realización particular de la variable aleatoria \\(\\bar{X}\\) en la muestra observada, el intervalo de confianza al \\(95\\%\\) será: \\[\nIC_{0.5}(\\mu)=\\bar{x}\\pm1.96 \\sqrt{2} = \\bar{x}\\pm2.77\n\\] En nuestro caso particular como la media era \\(\\bar{x}=166\\) cm, tenemos: \\[\nIC_{0.5}(\\mu)= 166\\pm2.77=(163.23 , 168.77)\n\\]\n\n\n\n\n\n3.6.5 Intervalo de Confianza para la media (cuando la varianza es desconocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza desconocida (\\(\\sigma^2\\)). Este supuesto es más realista que el caso anterior. Lo habitual es no disponer de información sobre la varianza poblacional.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nT=\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\] donde \\(s^2\\) es la es la cuasi-varianza muestral: \\(s^2=\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) y \\(t_n\\) es la distribución \\(t\\) de Student con \\(n\\) grados de libertad.\n\n\n\n\n\n\nRepaso\n\n\n\nEs posible que hayas estudiado la distribución \\(t\\) de Student en la asignatura de Probabilidad del primer curso del grado en Ciencia e Ingeniería de datos. En cualquier caso, repasamos: Si \\(T\\sim t_n\\) entonces: \\[\nE[T]=0\n\\] y \\[\nVar[T]=\\frac{n}{n-2}\n\\]\n\n\nSi \\(t_{n-1;1-\\alpha/2}\\) y \\(t_{n-1;\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\((\\alpha/2)\\) respectivamente de una distribución \\(t\\) de Student con \\(n-1\\) grados de libertad: \\[\nP(t_{n-1;1-\\alpha/2}&lt;T&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP(-t_{n-1;\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Se resuelve la doble desigualdad para \\(\\mu\\) y se obtiene el estimador por intervalos de confianza: \\[\n\\left ( \\bar{X}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{X}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\] Resultando el intervalo de confianza: \\[\nIC_{1-\\alpha}(\\mu) = \\left ( \\bar{\\mathbf{x}}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{\\mathbf{x}}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal con varianza desconocida\n\n\n\n\n\nSe ha medido la temperatura media de una muestra aleatoria de \\(10\\) soluciones salinas, obteniendo los siguiente resultados:\n\n\n [1] 37.2 34.1 35.5 34.5 32.9 37.3 32.0 33.1 42.0 34.8\n\n\nSe nos mide calcular el IC al \\(90\\%\\) para la temperatura media, suponiendo que la temperatura de la solución salina se puede aproximar mediante una variable aleatoria con distribución normal.\nVemos que la población a estudiar es “X=temperatura de una solución salina” donde \\(X \\sim N(\\mu,\\sigma^2)\\) con \\(\\sigma^{2}\\) desconocida.\nTenemos: \\(n=10\\), \\(\\bar{x}=\\) 35.3, \\(s^2=\\) 8.5, \\(s=\\) 2.9. Además: \\(t_{n-1;\\alpha/2}=t_{9;0.05}=1.83\\).\nPor tanto el intervalo de confianza que buscamos es: \\[\nIC_{0.9}(\\mu)=\\left (35.3\\pm 1.83\\frac{2.91}{\\sqrt{10}} \\right )=(35.3\\pm 1.68)=(33.62;36.98)\n\\]\n\n\n\n\n\n3.6.6 Media poblacional para muestras grandes\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución (conocida o no) con parámetros \\(\\mu\\) y \\(\\sigma^2\\). Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\mu\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}\\sim  N(0,1)\n\\] Si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de \\(N(0,1)\\), tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\]\nY así, tenemos la condición: \\[\nP\\left ( -z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}&lt;z_{\\alpha/2} \\right )=1-\\alpha\n\\] Obtenemos el estimador por intervalos de confianza resolviendo la doble desigualdad para \\(\\mu\\): \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{X}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\] El intervalo de confianza es: \\[\nIC_{1-\\alpha}(\\mu)=\\left ( \\bar{\\mathbf{x}}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{\\mathbf{x}}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para media de muestras grandes\n\n\n\n\n\nSupongamos que estamos interesados en estimar la media del tiempo diario que las personas pasan en redes sociales en la URJC. Hemos tomado una muestra aleatoria de \\(200\\) estudiantes y medido el tiempo que pasan en redes sociales. Los resultados muestran una media muestral \\(\\bar{x}\\) de \\(2.5\\) horas al día con una desviación estándar muestral \\(s\\) de \\(0.8\\) horas. Es decir, de \\(2\\) horas y \\(30\\) minutos.\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la media del tiempo que la población pasa en redes sociales.\nCalculamos el error estándar de la media (SE):\n\\[\nSE = \\frac{s}{\\sqrt{n}}= \\frac{0.8}{\\sqrt{200}} \\approx 0.0566\n\\]\nDe este modo el IC al \\(95\\%\\) queda como sigue: \\[\nIC_{0.95}(\\mu)=\\left ( 2.5\\pm1.96 * 0.0566\\right) =(2.389, 2.611)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la media del tiempo diario que las personas pasan en redes sociales en la población está entre \\(2.389\\) y \\(2.611\\) horas. Esto es, entre \\(2\\) horas y \\(23\\) minutos y \\(2\\) horas y \\(37\\) minutos, aproximadamente.\n\n\n\n\n\n3.6.7 Intervalo de Confianza para la proporción\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución de Bernoulli con parámetro \\(p\\). Esto es: \\[\n\\mu=E[X]=p\n\\] y \\[\n\\sigma^2=Var[X]=p(1-p)\n\\] Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\hat{p}=\\bar{X}\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\hat{p}-p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}\\sim  N(0,1)\n\\] EL intervalo de confianza para estimar una proporción poblacional (\\(p\\)) es: \\[\nIC_{1-\\alpha}(p)=\\left ( \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para proporción\n\n\n\n\n\nSupongamos que estamos realizando una encuesta para determinar la proporción de personas que apoyan una nueva política ambiental en una ciudad. Hemos encuestado a \\(1000\\) personas, y \\(560\\) de ellas han respondido que apoyan la nueva política.\nEs decir, la proporción muestral es: \\(\\hat{p} = \\frac{560}{1000} = 0.56\\)\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la proporción de apoyo en toda la población.\nEl error estándar para la proporción es:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}=\\sqrt{\\frac{0.56 \\times (1 - 0.56)}{1000}}  \\approx 0.016\n\\]\nObteniendo: \\[\nIC_{0.95}(p)= 56 \\pm 1.96 * 0.016 =  (0.529, 0.591)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la proporción de personas en la población que apoyan la nueva política ambiental está entre el \\(52.9\\%\\) y el \\(59.1\\%\\).\n\n\n\n\n\n3.6.8 Interpretación de los intervalos de confianza\nSi calculamos un intervalo de confianza del \\(95\\%\\) para la media poblacional y, por ejemplo, obtenemos un intervalo de \\((5, 10)\\), esto no significa que hay un \\(95\\%\\) de probabilidad de que la media poblacional esté en ese intervalo en un caso particular, sino que, si repetimos este procedimiento muchas veces, el \\(95\\%\\) de los intervalos construidos contendrán la verdadera media poblacional. Podríamos decir que estamos un \\(95\\%\\) seguros de que la media poblacional se encuentra entre \\(5\\) y \\(10\\), pero ¡ojo!, la media poblacional (cuyo valor desconocemos) estará o no estará en ese intervalo.\n\n\n\n\n\n\nSimulación Intervalo de Confianza\n\n\n\nVamos a realizar un ejercicio de simulación para interpretar correctamente el concepto frecuentista de intervalo de confianza. Para ello, generamos \\(100\\)muestras de tamaño \\(n=50\\) de una distribución \\(X\\sim N(\\mu=10,sigma^2=1)\\). Para cada una de estas muestras, se construye un intervalo de confianza para la media con \\(\\alpha=0.05\\). Y representamos todos esos intervalos de confianza en un único gráfico. En verde se pintan los intervalos de confianza que incluyen el verdadero valor del parámetro \\(10\\). En rojo los que no.\n\n\nClick para ver el código\nset.seed(3983)\nlibrary(dplyr)\nlibrary(ggplot2)\nic=matrix(0,100,5)\nic[,1]=seq(1:100)\nic=as.data.frame(ic)\ncolnames(ic)=c(\"id\",\"estimador\",\"inferior\",\"superior\",\"resultado\")\nic$resultado=\"in\"\nfor (i in 1:100){\n  muestra=rnorm(50,10,1)\n  ic[i,2]=mean(muestra)\n  ic[i,3]=mean(muestra)-1.96*1/sqrt(50)   \n  ic[i,4]=mean(muestra)+1.96*1/sqrt(50) \n}\nic$resultado=!(ic[,3]&gt;10 | ic[,4]&lt;10)\nic %&gt;%\n  ggplot(aes(estimador, id, color = resultado)) +\n  geom_point() +\n  geom_segment(aes(x =inferior, y = id, xend = superior, yend = id, color = resultado))+\n  geom_vline(xintercept = 10, linetype = \"dashed\") +\n  ggtitle(\"Varios Intervalos de Confianza\") +\n  scale_color_manual(values = c(\"#FF3333\", \"#009900\")) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(), \n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n¿Cuántos intervalos de confianza, de entre los \\(100\\), contienen al verdadero valor del parámetro? Razona ese resultado.\n¿Cuándo se toma una única muestra, cómo podrías estar seguro de estar en uno de los intervalos de confianza que recoge el verdadero valor del parámetro?\n¿Cómo crees que afecta a la longitud del intervalo de confianza los siguientes aspectos:?\n\nTamaño muestral\nNivel de confianza\n\n\nDiscute estas cuestiones con tu profesor.\n\n\n\n\n3.6.9 Determinación del tamaño muestral\nDeterminar el tamaño adecuado de una muestra es crucial en la inferencia estadística, ya que un tamaño muestral adecuado garantiza que los intervalos de confianza sean precisos y que las conclusiones obtenidas sean representativas de la población. Las técnicas para determinar el tamaño muestral están relacionadas directamente con los intervalos de confianza y se basan en varios factores, entre los que se incluyen el nivel de confianza deseado, la precisión (o margen de error) deseada y la variabilidad esperada en la población.\nLos factores clave para determinar el tamaño muestral son:\n\nNivel de confianza (\\(1-\\alpha\\)):\n\nEl nivel de confianza indica el grado de certeza de que el intervalo de confianza contiene el parámetro poblacional. Tal y como hemos indicado anteriormente, niveles de confianza comunes son \\(90\\%\\), \\(95\\%\\) y \\(99\\%\\). Un nivel de confianza más alto requiere una muestra más grande para asegurar la misma precisión.\n\nMargen de error (E):\n\nEl margen de error es la máxima diferencia tolerable entre la estimación muestral y el valor real del parámetro poblacional. Un margen de error más pequeño requiere una muestra más grande para asegurar una estimación precisa.\n\nVariabilidad poblacional (\\(\\sigma\\)):\n\nLa variabilidad en la población, medida por la desviación estándar, afecta directamente al tamaño muestral. Una mayor variabilidad requiere una muestra más grande para obtener una estimación precisa.\n\n\nA continuación mostramos algunos ejemplos del cálculo del tamaño muestral para diferentes situaciones.\n\n3.6.9.1 Tamaño muestral para estimar una media poblacional de una Normal\nEl tamaño muestral \\(n\\) necesario para estimar una media poblacional con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\left( \\frac{z_{\\alpha/2} \\cdot \\sigma}{E} \\right)^2\n\\]\ndonde:\n\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\\(\\sigma\\) es la desviación estándar de la población (si es desconocida, se puede usar la desviación estándar de la muestra \\(s\\)).\n\nEfectivamente, teníamos que el intervalo de confianza para la media se obtenía mediante la fórmula: \\[\n\\left ( \\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right )\n\\] Y buscamos el \\(n\\) tal que la desviación respecto a la media sea menor que \\(E\\), es decir: \\[\nz_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}  &lt; E\n\\] Esto es: \\[\nn &gt;  \\left ( \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}} \\right )^2\n\\]\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una Media\n\n\n\n\n\nSupongamos que deseamos estimar la media de una población con un nivel de confianza del \\(95\\%\\), un margen de error de \\(5\\) unidades y se estima que la desviación estándar de la población es \\(15\\) unidades. El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\left( \\frac{1.96 \\cdot 15}{5} \\right)^2 = \\left( \\frac{29.4}{5} \\right)^2 \\approx 34.57\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(35\\) individuos.\n\n\n\n\n\n3.6.9.2 Tamaño muestral para estimar una proporción poblacional\nEl tamaño muestral \\(n\\) necesario para estimar una proporción poblacional \\(p\\) con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\frac{z_{\\alpha/2}^2 \\cdot p \\cdot (1 - p)}{E^2}\n\\]\ndonde:\n\n\\(p\\) es la proporción esperada (si no se conoce, se usa \\(p = 0.5\\) para maximizar el tamaño muestral).\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una proporción\n\n\n\n\n\nSupongamos que deseamos estimar la proporción de personas que aprueban una nueva ley con un nivel de confianza del \\(95\\%\\), un margen de error del \\(3\\%\\) \\((0.03)\\) y se estima que la proporción esperada es \\(p = 0.5\\). El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\frac{1.96^2 \\cdot 0.5 \\cdot (1 - 0.5)}{0.03^2} = \\frac{3.8416 \\cdot 0.25}{0.0009} = \\frac{0.9604}{0.0009} \\approx 1067.11\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(1068\\) individuos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#contraste-de-hipótesis",
    "href": "para.html#contraste-de-hipótesis",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.7 Contraste de hipótesis",
    "text": "3.7 Contraste de hipótesis\nLos contrastes de hipótesis son una herramienta fundamental en la inferencia estadística utilizada para tomar decisiones basadas en datos muestrales. Permiten evaluar si los datos disponibles proporcionan suficiente evidencia en contra de una hipótesis previamente establecida sobre una población.\nEl contraste de hipótesis es un proceso estructurado para evaluar afirmaciones sobre parámetros poblacionales utilizando datos muestrales. Mediante la formulación de hipótesis, selección de niveles de significancia, elección de estadísticas de prueba y evaluación del valor p, podemos tomar decisiones informadas y cuantitativamente justificadas. Este enfoque es fundamental en muchas áreas de investigación y análisis de datos, proporcionando un marco riguroso para la Inferencia Estadística.\n\n3.7.1 Conceptos básicos\nHipótesis Nula \\((H_0)\\):\n\nLa hipótesis nula es una afirmación sobre un parámetro poblacional que se asume verdadera hasta que se presente suficiente evidencia en contra. Se asume inicialmente que la hipótesis nula es correcta (semejante a suponer inocencia en un juicio a menos que se pruebe la culpabilidad). Habitualmente corresponde al estatus quo. Esto es, generalmente, la hipótesis nula representa un estado de “no efecto” o “no diferencia”.\n\nEjemplo: \\((H_0: \\mu = 50)\\) (la media poblacional es \\(50\\)). En este ejemplo, la idea fundamental del contraste sería toma una muestra aleatoria simple de la población, estudiar su media, y ver si hay evidencia suficiente como para rechazar la hipótesis nula establecida. La probabilidad de que la media sea exactamente igual a \\(50\\) en la muestra es muy baja. Es decir, probablemente \\(\\bar{\\mathbf{x}} \\neq 50\\). Sin embargo, lo importante para rechazar la hipótesis nula es si la diferencia encontrada entre la media muestral y \\(50\\) es tan grande como para rechazar que podría ser \\(50\\). Esto va a depender fuertemente del tamaño muestral y de la variabilidad de las observaciones en la muestra.\n\n\nHipótesis Alternativa \\((H_1)\\):\n\nLa hipótesis alternativa es una afirmación que contrasta con la hipótesis nula y representa el efecto o diferencia que se desea detectar.\n\nEjemplo: \\((H_1: \\mu \\neq 50)\\) (la media poblacional no es 50).\nEjemplo: \\((H_1: \\mu \\geq 50)\\) (la media poblacional es mayor o igual que 50).\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Media poblacional\n\n\n\n\n\nSupongamos que una empresa de educación en línea afirma que sus estudiantes pasan en promedio al menos \\(4\\) horas diarias estudiando en su plataforma. Queremos comprobar si esta afirmación es cierta basándonos en una muestra de estudiantes.\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la media del tiempo de estudio diario es de \\(4\\) horas. \\[\nH_0: \\mu \\geq 4 \\text{ horas}\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si el tiempo de estudio diario es menor de \\(4\\) horas. Fíjate que la empresa podría estar “inflando” sus resultados y lo “intersante” en este caso es “demostrar” que realmente los alumnos pasan menos tiempo en la plataforma. \\[\nH_1: \\mu &lt; 4 \\text{ horas}\n\\]\n\n\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Proporción poblacional\n\n\n\n\n\nSupongamos que el rectorado de la URJC afirma que menos del \\(20\\%\\) de los estudiantes de sus grados, fuman. Queremos verificar si la proporción de fumadores es mayor al \\(20\\%\\).\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la proporción de fumadores es menor o igual al \\(20\\%\\). \\[\nH_0: p \\leq 0.20\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si la proporción de fumadores es mayor al \\(20\\%\\). \\[\nH_1: p &gt; 0.20\n\\]\n\n\n\n\n\n\n3.7.2 Pasos en un Contraste de Hipótesis\n\nFormular las hipótesis:\n\nDefinir \\(H_0\\) y \\(H_1\\) claramente.\n\nSeleccionar el nivel de significatividad estadística \\((\\alpha)\\):\n\nEl nivel de significatividad estadística es la probabilidad de rechazar \\(H_0\\) cuando es verdadera. Comúnmente, se utilizan \\(\\alpha = 0.05\\), \\(\\alpha = 0.01\\), o \\(\\alpha = 0.10\\).\n\nElegir el estadístico de prueba:\n\nSeleccionar un estadístico que siga una distribución conocida bajo \\(H_0\\) (por ejemplo, la distribución Normal, t de Student o Chi-cuadrado).\n\nCalcular el \\(p-valor\\):\n\nEl \\(p-valor\\) es la probabilidad de observar un valor tan extremo o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera. Después volveremos sobre este valor.\n\nTomar una decisión:\n\nLa regla de decisión de un contraste de hipótesis se basa en la “distancia” entre los datos muestrales y los valores esperados si \\(H_0\\) es cierta. Esta distancia se calcula a partir de un estadístico del contraste y se considera “grande” o no, en base a la distribución del mismo y a la probabilidad de observar realizaciones “más extremas” de dicho estadístico. Para tomar la decisión, comparamos el valor p con \\(\\alpha\\):\nSi \\(p-valor \\leq \\alpha\\), se rechaza \\(H_0\\). Hay suficiente evidencia en la muestra como para rechazar la hipótesis nula. El valor del parámetro establecido en \\(H_0\\) es poco creíble dada la muestra observada.\nSi \\(p-valor &gt; \\alpha\\), no se rechaza \\(H_0\\).\n\n\n\n\n\n\n\n\n¡Importante!\n\n\n\nNo rechazar la hipótesis nula no significa que la hipótesis nula sea cierta. La interpretación es que no existe, en la muestra que hemos observado, suficiente evidencia en contra de la hipótesis nula como para recharzarla.\n\n\nTenemos por tanto que el \\(p-valor\\) es una medida que nos dice cuán probable sería obtener nuestros datos observados si la hipótesis nula fuera verdadera. En otras palabras, mide la evidencia en contra \\(H_0\\). Si el \\(p-valor\\) es pequeño (generalmente menor que \\(0.05\\)), tenemos razones para rechazar \\(H_0\\). Si es grande, no tenemos suficiente evidencia para rechazarla.\n\n\n\n\n\n\nEjemplo Práctico. Contraste de Hipótesis\n\n\n\n\n\nSupongamos que una empresa afirma que el tiempo promedio de espera en su servicio al cliente es de \\(10\\) minutos. Queremos probar esta afirmación con una muestra de \\(30\\) clientes que tienen un tiempo promedio de espera de \\(12\\) minutos y una desviación estándar de \\(3\\) minutos.\nFormulamos las hipótesis: - \\(H_0: \\mu = 10\\) - \\(H_1: \\mu \\neq 10\\)\nSeleccionamos el nivel de significancia estadística deseado: \\(\\alpha = 0.05\\).\nElegimos el estadístico de prueba: usamos una prueba \\(t\\) (dado que la muestra es pequeña y no conocemos la desviación estándar poblacional) y calculamos su valor:\n\\[\n   t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} = \\frac{12 - 10}{3 / \\sqrt{30}} \\approx \\frac{2}{0.5477} \\approx 3.65\n\\]\nA continuación, obtenemos el \\(p-valor\\) correspondiente al estadístico para la distribución \\(t\\) de Studento con \\(29\\) grados de libertad. Para \\(t = 3.65\\), el \\(p-valor\\) es menor que \\(0.001\\).\nDado que \\(p &lt; 0.05\\), rechazamos la hipótesis nula \\(H_0\\). Podemos afirmar que los resultados muestrales son “demasiado extraños” para aceptar la hipótesis nula.\n\n\n\n\n\n3.7.3 Errores tipo I y tipo II. Potencia\nComo hemos visto, una vez especificadas las hipótesis nula y alternativa y recogida la información muestral, se toma una decisión sobre la hipótesis nula (rechazar o no rechazar \\(H_0\\)). Sin embargo, existe la posibilidad de llegar a una conclusión equivocada, porque solo se dispone de una muestra aleatoria y no se puede tener la certeza de que \\(H_0\\) sea correcta o no.\nEn la Inferencia Estadística, cuando realizamos un contraste de hipótesis, hay dos tipos de errores que pueden ocurrir: el error de tipo I o \\(\\alpha\\) y el error de tipo II o \\(\\beta\\). Entender estos errores es fundamental para interpretar correctamente los resultados de cualquier prueba estadística. El balance entre \\(\\alpha\\) y \\(\\beta\\), así como el tamaño de la muestra, juegan un papel importante en la fiabilidad de los resultados obtenidos.\n\n3.7.3.1 Error de Tipo I \\((\\alpha)\\)\nEl error de tipo I ocurre cuando rechazamos la hipótesis nula \\(H_0\\) siendo esta verdadera. En otras palabras, concluimos que hay un efecto o una diferencia cuando, en realidad, no la hay. El nivel de significancia \\(\\alpha\\) es la probabilidad de cometer un error de tipo I.\n\\[\n\\alpha=P(\\text{rechazar } H_0 \\text{ | } H_0 \\text{ es correcta})\n\\]\nRecuerda que establecemos de antemano esta valor, comúnmente \\(0.05\\), \\(0.01\\) o \\(0.10\\). Si el \\(p-valor\\) de nuestra prueba es menor o igual a \\(\\alpha\\), rechazamos \\(H_0\\).\nAsí, por ejemplo, Si \\(\\alpha = 0.05\\), esto significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de rechazar \\(H_0\\) cuando es verdadera.\n\n\n\n\n\n\nAtención\n\n\n\n¿Qué relación existe entre el error de tipo I y el \\(\\%\\) de un IC?\n\n\n\n\n3.7.3.2 Error de Tipo II \\((\\beta)\\)\nEl error de tipo II ocurre cuando no rechazamos la hipótesis nula \\(H_0\\) siendo esta falsa. En otras palabras, concluimos que no hay un efecto o una diferencia cuando, en realidad, sí la hay.\n\\[\n\\beta=P(\\text{No rechazar } H_0 \\text{ | } H_0 \\text{ es incorrecta})\n\\]\nPotencia del test: La potencia de una prueba estadística es la probabilidad de rechazar \\(H_0\\) cuando \\(H_0\\) es falsa. Se calcula como \\(1 - \\beta\\). Una alta potencia es deseable ya que indica una mayor probabilidad de detectar un efecto o diferencia cuando realmente existe.\n\\[\nPotencia=1-\\beta=P(\\text{Rechazar } H_0 \\text{ | } H_1 \\text{ es correcta})\n\\]\nAsí, por ejemplo, si \\(\\beta = 0.20\\), esto significa que hay un \\(20\\%\\) de probabilidad de no rechazar \\(H_0\\) cuando es falsa. La potencia de la prueba sería \\(0.80\\) (o del \\(80\\%\\)). La probabilidad de detectar un efecto cuando relamente existe es del \\(80\\%\\)\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento.\n\nHipótesis Nula \\(H_0\\): El medicamento no tiene efecto \\((\\mu = 0)\\).\nHipótesis Alternativa \\(H_1\\): El medicamento tiene un efecto \\((\\mu \\neq 0)\\).\n\n\nError de Tipo I: Si el medicamento no tiene ningún efecto pero el estudio concluye que sí lo tiene, hemos cometido un error de tipo I. Esto podría llevar a la aprobación y uso de un medicamento ineficaz.\nError de Tipo II: Si el medicamento tiene un efecto, pero el estudio concluye que no lo tiene, hemos cometido un error de tipo II. Esto podría llevar a la no aprobación de un medicamento potencialmente beneficioso.\n\n\n\n\n\n\n3.7.3.3 Relación entre errores de tipo I y II\n\nInversamente proporcionales: Reducir \\(\\alpha\\) (haciendo la prueba más conservadora y menos propensa a rechazar \\(H_0\\) generalmente aumenta \\(\\beta\\) (haciendo la prueba más propensa a no detectar un efecto cuando realmente existe), y viceversa. Fíjate que los errores de Tipo I y de Tipo II no se pueden comenter simultáneamente:\n\nEl error de Tipo I solo puede darse si \\(H_0\\) es correcta.\nEl error de Tipo II solo puede darse si \\(H_0\\) es incorrecta.\n\nTamaño de la muestra: Aumentar el tamaño de la muestra puede reducir ambos tipos de errores, incrementando la precisión de la prueba.\n\nLa siguiente tabla refleja la relación entre los dos tipos de errores en relación con la decisión del contraste y la verdadera situación en la población:\n\n\n\n\n\n\n\n\n\nVerdadera situación\n\n\n\n\n\nDecisión\n\\(H_0\\) correcta\n\\(H_0\\) incorrecta\n\n\nNo rechazar \\(H_0\\)\nSin error (\\(1-\\alpha\\))\nError de Tipo II (\\(\\beta\\))\n\n\nRechazar \\(H_0\\)\nError de Tipo I (\\(\\alpha\\))\nSin error (\\(1-\\beta\\)=potencia)\n\n\n\nEs importante notar que, si todo lo demás no cambia, entonces la potencia del contraste disminuye cuando:\n\nLa diferencia entre el valor supuesto para el parámetro y el valor real disminuye.\nLa variabilidad de la población aumenta.\nEl tamaño muestra disminuye.\n\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nLos errores de tipo I y tipo II son inversamente proporcionales. Al disminuir la probabilidad de cometer un error de tipo I (haciendo la prueba más conservadora), aumentamos la probabilidad de cometer un error de tipo II (haciendo la prueba menos sensible), y viceversa.\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento para reducir la presión arterial. Queremos probar la siguiente hipótesis:\n\nHipótesis Nula \\((H_0)\\): El nuevo medicamento no reduce la presión arterial (\\(\\mu = 0\\)).\nHipótesis Alternativa \\((H_1)\\): El nuevo medicamento reduce la presión arterial \\((\\mu \\neq 0)\\).\n\nInicialmente fijamos el nivel de significancia (\\(\\alpha\\)) en \\(0.05\\). Consideramos un tamaño de muestra inicial de \\(100\\) pacientes.\nEn este caso el error de Tipo I, significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de concluir que el medicamento es efectivo cuando en realidad no lo es.\nLa probabilidad de error de Tipo II (\\(\\beta\\)) y, por tanto, la pontencia del contraste depende de varios factores, incluidos el tamaño del efecto, el tamaño de la muestra y el nivel de significancia.\nCaso 1: (\\(\\alpha = 0.05\\))\nEn este caso somos bastante conservadores con el riesgo de falso positivo. Supongamos que el poder estadístico de la prueba con \\((\\alpha = 0.05)\\) y una muestra de \\(100\\) es \\(0.80\\), lo que significa que (\\(\\beta = 0.20\\)).\nCaso 2: (\\(\\alpha = 0.01\\))\nAhora somos más conservadores, reduciendo la probabilidad de cometer un error de tipo I. Al ser más conservadores y reducir (\\(\\alpha\\)), la prueba se vuelve menos sensible a detectar el efecto real. Esto aumenta la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) aumenta a \\(0.30\\). Se reduce la potencia del contraste.\nCaso 3: (\\(\\alpha = 0.10\\))\nAhora somos menos conservadores, aumentando la probabilidad de cometer un error de tipo I. Al ser menos conservadores y aumentar (\\(\\alpha\\)), la prueba se vuelve más sensible a detectar el efecto real. Esto disminuye la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) disminuye a 0.10 y, por tanto, aumenta la potencia del contraste.\nEn resumen:\n\n\n\n\\(\\alpha\\)\n\\(\\beta\\)\nPotencia (\\(1-\\beta\\))\n\n\n\n\n0.05\n0.20\n0.80\n\n\n0.01\n0.30\n0.70\n\n\n0.10\n0.10\n0.90\n\n\n\ny como conclusión\n\nCaso 1 (\\(\\alpha = 0.05\\)): Balance estándar entre el riesgo de falso positivo y falso negativo.\nCaso 2 (\\(\\alpha = 0.01\\)): Reducimos el riesgo de falso positivo (\\(\\alpha\\)), pero aumentamos el riesgo de falso negativo (()).\nCaso 3 (\\(\\alpha = 0.10\\)): Aumentamos el riesgo de falso positivo (\\(\\alpha\\)), pero reducimos el riesgo de falso negativo (\\(\\beta\\)).\n\n\n\n\n\n\n\n3.7.4 Contraste para la media de una población normal con varianza conocida\nEl contraste para la media de una población normal con varianza conocida es un procedimiento estadístico utilizado para determinar si la media de una población difiere de un valor específico (hipótesis nula). Sin embargo, como hemos indicado anteriormente, es poco realista pensar que conocemos la varianza de una variable aleatoria en la población.\nEl parámetro de estudio es la media de la variable aleatoria:\n\\[ X \\sim N(\\mu,\\sigma^2) \\]\nEn primer lugar fijamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = \\mu_0\\) (La media de la población es igual a \\(\\mu_0\\))\n\nTenemos varias opciones para la hipótesis alternativa:\n\n\\(H_1\\): \\(\\mu \\neq \\mu_0\\) (Contraste bilateral)\n\\(H_1\\): \\(\\mu &gt; \\mu_0\\) (Contraste unilateral derecho)\n\\(H_1\\): \\(\\mu &lt; \\mu_0\\) (Contraste unilateral izquierdo)\n\nDebemos fijar el nivel de el nivel de significación (\\(\\alpha\\)). Recordemos, \\(\\alpha\\) es la probabilidad de rechazar la hipótesis nula cuando esta es verdadera.\nEl estadístico de prueba se calcula utilizando la distribución Normal estándar (\\(Z\\)), dado que la varianza (\\(\\sigma^2\\)) es conocida. La fórmula para el estadístico de prueba es:\n\\[ Z = \\frac{\\bar{\\mathbf{X}} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ z = \\frac{\\bar{\\mathbf{x}} - \\mu_0}{\\sigma/\\sqrt{n}} \\] donde \\(\\bar{\\mathbf{x}}\\) es la media muestral.\nDespués calculamos el \\(p-valor\\) como sigue:\n\nContraste bilateral: \\(p-valor=P(|Z|\\geq z)\\)\nContraste unilateral derecho: \\(p-valor=P(Z\\geq z)\\)\nContraste unilateral izquierdo: \\(p-valor=P(Z\\leq z)\\)\n\nSi esta probabilidad es menor o igual que el valor de referencia \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\nOtra forma práctica de plantear el contraste de hipótesis es determinando el rechazo de \\(H_0\\). Para ello, debemos comparar el valor del estadístico \\(Z\\) con los valores críticos de la distribución Normal estándar.\n\nPara un contraste bilateral (dos colas):\n\nRechaza \\(H_0\\) si \\(|z| &gt; z_{\\alpha/2}\\).\n\nPara un contraste unilateral derecho (una cola):\n\nRechaza \\(H_0\\) si \\(z &gt; z_{\\alpha}\\).\n\nPara un contraste unilateral izquierdo (una cola):\n\nRechaza \\(H_0\\) si \\(z &lt; -z_{\\alpha}\\).\n\n\nAquí, \\(z_{\\alpha}\\) y \\(z_{\\alpha/2}\\) son los valores críticos de la distribución Normal estándar correspondientes al nivel de significación \\(\\alpha\\).\nEs decir, decidimos si rechazamos o no la hipótesis nula del siguiente modo:\n\nSi el valor del estadístico de prueba está en la región crítica, rechaza (H_0).\nSi el valor del estadístico de prueba no está en la región crítica, no rechaces (H_0).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza conocida\n\n\n\n\n\nSupón que queremos probar si la edad media de los profesores de la URJC es igual a \\(50\\) años con una desviación estándar conocida de \\(10\\) años, y tienes una muestra de \\(36\\) observaciones con una media muestral de \\(52\\).\nFormulamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = 50\\)\n\\(H_1\\): \\(\\mu \\neq 50\\)\n\nFijamos el nivel de significación: \\(\\alpha = 0.05\\).\nCalculamos el estadístico de prueba: \\[ z = \\frac{52 - 50}{10/\\sqrt{36}}  \\approx 1.20 \\]\nPara \\(\\alpha = 0.05\\) en un contraste bilateral, los valores críticos son \\(z_{0.05/2}=\\pm 1.96\\). Como \\(|1.20| &lt; 1.96\\), no tenemos evidencia en la muestra como para rechazar \\(H_0\\).\nSi hubiéramos calculado el \\(p-valor\\): \\[p-valor=P(|Z| \\geq 1.20)=2*P(Z \\geq 1.20)=2*0.115\\approx0.23\\] Como el \\(p-valor\\) es mayor que el nivel de significatividad estadística, no podemos rechazar la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n3.7.5 Contraste para la media de una población normal con varianza desconocida\nEl contraste de hipótesis para la media de una población normal con varianza desconocida es similar al caso con varianza conocida, pero utilizamos la distribución \\(t\\) de Student en lugar de la distribución Normal estándar.\nEn este caso, el cuando la varianza poblacional es desconocida, se utiliza la desviación estándar muestral (\\(s\\)) y el estadístico de prueba se basa en la distribución \\(t\\) de Student con (\\(n - 1\\)) grados de libertad. La fórmula es:\n\\[ T=\\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}  \\]\nEl p-valor es la probabilidad de obtener un valor del estadístico de prueba al menos tan extremo como el observado, bajo la suposición de que \\(H_0\\) es verdadera. Dependiendo del tipo de contraste, el p-valor se calcula de diferentes formas:\n\nPara un contraste bilateral:\n\np-valor = \\(2 \\cdot P(T_{n-1} &gt; |t|)\\)\n\nPara un contraste unilateral derecho:\n\np-valor = \\(P(T_{n-1} &gt; t)\\)\n\nPara un contraste unilateral izquierdo:\n\np-valor = \\(P(T_{n-1} &lt; t)\\)\n\n\nAquí, \\(T_{n-1}\\) es una variable aleatoria con una distribución \\(t\\) de Student con \\(n - 1\\) grados de libertad.\nLa decisión asociada al contraste es:\n\nSi el \\(p-valor \\leq \\alpha\\), rechazar \\(H_0\\).\nSi el \\(p-valor &gt; \\alpha\\), no rechazar \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza desconocida\n\n\n\n\n\nSupón que una empresa quiere verificar si el tiempo promedio de entrega de sus pedidos es mayor de \\(30\\) minutos. Toma una muestra aleatoria de \\(16\\) entregas y encuentra que el tiempo promedio de entrega es de \\(32\\) minutos con una desviación estándar muestral de \\(4\\) minutos. Realiza un contraste de hipótesis con un nivel de significación del \\(0.05\\) para ver si el tiempo promedio de entrega es mayor de \\(30\\) minutos.\nEn primer lugar establecemos las hipótesis: - \\(H_0\\): $= 30 (El tiempo promedio de entrega es de 30 minutos) - \\(H_1\\): $&gt; 30 (El tiempo promedio de entrega es mayor de 30 minutos)\nCalculamos el estadístico de Prueba \\[ t = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} = \\frac{32 - 30}{4/\\sqrt{16}} = \\frac{2}{1} = 2.00 \\]\nEstamos ante un contraste unilateral derecho con \\(n - 1 = 16 - 1 = 15\\) grados de libertad. El \\(p-valor\\) es: \\[P(T_{15} &gt; 2.00) \\approx 0.031\\]\nComo el \\(p-valor\\) es menor que el grado de significatividad estadística \\(0.05\\), entonces podemos rechazar la hipótesis nula en favor de la alternativa. Hay suficiente evidencia para rechazar la hipótesis nula y concluir que el tiempo promedio de entrega es mayor de \\(30\\) minutos con un nivel de significación del \\(5\\%\\).\n\n\n\n\n\n3.7.6 Contraste de hipótesis para la igualdad de medias de dos muestras independientes\nCuando se desea comparar las medias de dos muestras independientes asumiendo que los datos siguen una distribución normal, se puede usar el contraste de hipótesis paramétrico conocido como la prueba \\(t\\) de Student para muestras independientes. Este método es robusto y se basa en suposiciones claras acerca de la normalidad de las distribuciones subyacentes.\nSuposiciones:\n\nLas dos muestras son independientes.\nLos datos de cada muestra se distribuyen normalmente. Si las muestras son suficientemente grandes, se puede invocar el Teorema Central del Límite, que establece que la distribución de la media muestral se aproxima a una distribución normal independientemente de la forma de la distribución original.\nLas varianzas poblacionales son desconocidas, pero se pueden asumir iguales para una versión específica del test t (si esta suposición es razonable).\n\nSupongamos m.a.s. independientes con medias, desviaciones típicas y tamaño muestral igual a: \\(\\bar{\\mathbf{x}}_1\\),\\(\\bar{\\mathbf{x}}_2\\),\\(s_1^2\\),\\(s_2^2\\),\\(n_1\\),y \\(n_2\\), respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestras, en este caso: \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2 - \\mu_1 - \\mu_2}{SE}\n\\] donde: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}\n\\] siendo \\(S_p\\) la desviación típica combinada: \\[\nS^2_p= \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2-2\\), buscamos el valor crítico \\(t\\) para la distribución \\(t\\) de Student para una prueba de dos colas.\nComparamos el valor del estadístico \\(t\\) calculado con el valor crítico:\n\nSi \\(|t| &gt; T_{df,1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|t| \\leq T_{df,1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de medias\n\n\n\n\n\nSupongamos que un investigador quiere comparar la efectividad de dos métodos de enseñanza de matemáticas. Se seleccionan dos grupos de estudiantes al azar, uno para cada método. Después de un semestre, se mide el puntaje de un examen final de matemáticas.\nDatos:\n\nGrupo A (Método 1):\n\nTamaño de la muestra (\\(n_1\\)) = 12\nPuntajes: 85, 78, 92, 88, 75, 84, 90, 91, 83, 79, 87, 86\n\nGrupo B (Método 2):\n\nTamaño de la muestra (\\(n_2\\)) = 10\nPuntajes: 82, 77, 85, 80, 79, 81, 83, 78, 82, 76\n\n\nLas hipótesis del contraste son:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia \\(\\alpha=0.05\\).\nPara el Grupo A, se tiene:\n\nTamaño de la muestra (\\(n_1= 12\\))\nMedia (\\(\\bar{\\mathbf{x}}_1 = 84.83\\))\nDesviación estándar (\\(s_1=5.33\\))\n\nPara el Grupo B:\n\nTamaño de la muestra (\\(n_2= 10\\))\nMedia (\\(\\bar{\\mathbf{x}}_2 = 80.3\\))\nDesviación estándar (\\(s_2=2.83\\))\n\nUsaremos la prueba t para muestras independientes. Calculamos la varianza combinada: \\[\nS^2_p=  \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}=\\frac{(12-1)5.33^2 + (10-1)2.83^2}{12+10-2} \\approx 19.288\n\\] El erro estándar combinados: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}= \\sqrt{19.288 \\left( \\frac{1}{12} + \\frac{1}{10} \\right)}\\approx 1.88\n\\] Para calcular el estadístico \\(t\\): \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2}{SE}=\\frac{84.83-80.3}{1.88}\\approx 2.41\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2- 2 = 20\\), el valor crítico t de t de Student para una prueba de dos colas es aproximadamente \\(\\pm 2.086\\).\nEn nuestro caso, \\(|t| = 2.41\\), que es mayor que \\(2.086\\) y por tanto, eechazamos la hipótesis nula (\\(H_0\\)). Esto indica que hay evidencia suficiente para afirmar que existe una diferencia significativa en los puntajes de los exámenes de matemáticas entre los dos grupos de estudiantes.\n\n\n\nEn el caso en el que no se pueda asumir que las varianzas son iguales, el test cambia ligeramente. Es decir, cuando se quieren comparar las medias de dos muestras independientes con varianzas distintas, el estadístico del contraste bajo la hipótesis nula es\n\\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2 - \\mu_1 - \\mu_2}{\\sqrt{\\frac{s_{1}^{2}}{n_1}+\\frac{s^{2}_2}{n_2}}} \\sim t_{q, \\alpha/2}\n\\] siendo \\[q = \\frac{\\left(\\frac{s_{1}^{2}}{n_1} + \\frac{s_{2}^{2}}{n_2}\\right)^{2}}{\\frac{\\frac{s_{1}^{2}}{n_1}}{n_1 -1} + \\frac{\\frac{s_{2}^{2}}{n_2}}{n_2 -1}} \\] que se calculan mediante la aproximación de Welch-Satterthwaite.\n\n\n3.7.7 Contraste de hipótesis para la diferencia de proporciones\nEl contraste de hipótesis para la diferencia de proporciones se utiliza para determinar si hay una diferencia significativa entre las proporciones de éxito en dos grupos independientes. Supongamos dos variables aleatorias \\(X\\) e \\(Y\\) que siguen una distribución binomial de parámetros \\(p_1\\) y \\(p_2\\) respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las proporciones de las dos poblaciones son iguales (\\(p_1=p_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las proporciones de las dos poblaciones son diferentes (\\(p_1 \\neq p_2\\)).\n\nConsideremos dos m.a.s. de tamaño \\(n_1\\) y \\(n_2\\), siendo \\(\\mathbf{x}\\) y \\(\\mathbf{y}\\) el número de observaciones que cumplen un criterio, de modo que: \\[\n\\hat{p}_1=\\frac{\\mathbf{x}}{n_1}, \\hat{p}_2=\\frac{\\mathbf{y}}{n_2}\n\\] son los estimadores de máxima verosimilitud de \\(p_1\\) y \\(p_2\\) respectivamente.\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestral, en este caso: \\[\nZ = \\frac{\\hat{p}_1-\\hat{p_2}}{SE}\n\\]\ndónde \\[\nSE=\\sqrt{\\hat{p}(1-\\hat{p})}\\left ( \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\right )\n\\] Donde \\(\\hat{p}=\\frac{\\mathbf{x}+\\mathbf{y}}{n_1+n_2}\\).\nPara valores grandes de \\(n_1\\) y \\(n_2\\), la distribución de \\(Z\\) es Normal de media \\(0\\) y desviación típica \\(1\\).\nPara un nivel de significancia \\(\\alpha= 0.05\\), buscamos el valor crítico \\(Z\\) para la distribución Normal.\nComparamos el valor del estadístico \\(Z\\) calculado con el valor crítico:\n\nSi \\(|Z| &gt; Z_{1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|Z| \\leq Z_{1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de proporciones\n\n\n\n\n\nSupongamos que una empresa de marketing quiere evaluar la efectividad de dos campañas publicitarias diferentes (Campaña A y Campaña B) para atraer clientes. La empresa desea saber si hay una diferencia significativa en la proporción de clientes que responden positivamente a cada campaña.\n\nCampaña A:\n\nNúmero de personas que recibieron la campaña: \\(500\\)\nNúmero de personas que respondieron positivamente: \\(75\\)\n\nCampaña B:\n\nNúmero de personas que recibieron la campaña: \\(600\\)\nNúmero de personas que respondieron positivamente: \\(120\\)\n\n\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): No hay diferencia en la proporción de éxito entre las dos campañas \\((p_1 = p_2)\\).\nHipótesis Alternativa (\\(H_1\\)): Hay una diferencia en la proporción de éxito entre las dos campañas \\((p_1 \\neq p_2)\\).\n\nCalculamos las proporciones muestrales:\n\\[\\hat{p}_1 = \\frac{75}{500} = 0.15, \\hat{p}_2 = \\frac{120}{600} = 0.20\\] calculamos la proporción combinada: \\[  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{75 + 120}{500 + 600} = \\frac{195}{1100} = 0.1773\n\\]\nCalcular el Error Estándar de la diferencia de proporciones \\[  SE = \\sqrt{\\hat{p} (1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} = \\sqrt{0.1733 \\times 0.8227 \\left( \\frac{1}{500} + \\frac{1}{600} \\right)} \\approx 0.0231\n\\]\nA continuación calculamos el estadístico de prueba: \\[z = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE} = \\frac{0.15 - 0.20}{0.0231} \\approx -2.16\\] El \\(p-valor\\) asociado para una prueba bilateral es aproximadamente \\(P(|Z|\\geq 2.16)\\approx 0.031\\).\nDado que el \\(p-valor\\) es menor que el nivel de significancia típico (\\(\\alpha = 0.05\\)), rechazamos la hipótesis nula. Por tanto, hay evidencia suficiente para afirmar que existe una diferencia significativa en las proporciones de éxito entre la Campaña A y la Campaña B.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html",
    "href": "nopara.html",
    "title": "4  Contraste no paramétrico",
    "section": "",
    "text": "4.1 Introducción a los contrastes no paramétricos\nEn el campo de la inferencia estadística, los contrastes no paramétricos se utilizan para comparar grupos o evaluar hipótesis cuando no se cumplen los supuestos necesarios para aplicar métodos paramétricos tradicionales como los vistos en el Capítulo 3. Los métodos no paramétricos no dependen de la suposición de que los datos sigan una distribución específica, como la Normal, lo que los hace especialmente útiles en situaciones donde los datos presentan sesgos, distribuciones desconocidas o tamaños de muestra pequeños.\nLos contrastes no paramétricos ofrecen una alternativa robusta y flexible para analizar datos en diversas circunstancias. Entre sus ventajas se incluyen la capacidad de manejar datos ordinales que no se ajustan a una escala continua y la resistencia a la influencia de valores atípicos. Esto permite obtener resultados fiables y válidos sin necesidad de transformaciones complicadas de los datos.\nAlgunos de los contrastes no paramétricos más conocidos incluyen la prueba de Mann-Whitney, utilizada para comparar medianas entre dos grupos independientes, y la prueba de Wilcoxon para muestras relacionadas, que evalúa diferencias en medianas para datos apareados. Otros ejemplos son la prueba de Kruskal-Wallis, que extiende el análisis a más de dos grupos independientes, y la prueba de Friedman, que se aplica a diseños con medidas repetidas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "href": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "title": "4  Contraste no paramétrico",
    "section": "4.2 Prueba Chi-cuadrado de Independencia",
    "text": "4.2 Prueba Chi-cuadrado de Independencia\nLa prueba Chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas \\(X\\) con categorías \\(X_1,X_2,\\ldots,X_r\\) e \\(Y\\) con categorías \\(Y_1,Y2,\\ldots,Y_c\\). Esta prueba compara las frecuencias observadas en una tabla de contingencia con las frecuencias esperadas bajo la hipótesis de independencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_1\\)\n\\(\\ldots\\)\n\\(Y_j\\)\n\\(\\ldots\\)\n\\(Y_c\\)\n\n\n\n\n\n\\(X_1\\)\n\\(n_{1,1}\\)\n\\(\\ldots\\)\n\\(n_{1,j}\\)\n\\(\\ldots\\)\n\\(n_{1,c}\\)\n\\(n_{1.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_i\\)\n\\(n_{i,1}\\)\n\\(\\ldots\\)\n\\(n_{i,j}\\)\n\\(\\ldots\\)\n\\(n_{i,c}\\)\n\\(n_{i.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_r\\)\n\\(n_{r,1}\\)\n\\(\\ldots\\)\n\\(n_{r,j}\\)\n\\(\\ldots\\)\n\\(n_{r,c}\\)\n\\(n_{r.}\\)\n\n\n\n\\(n_{.1}\\)\n\n\\(n_{.j}\\)\n\n\\(n_{.c}\\)\n\\(n_{..}\\)\n\n\n\nLa hipótesis nula \\(H_0\\) de esta prueba es que no hay asociación entre las variables, esto es, que las variables implicadas son independientes:\n\nHipótesis nula, \\(H_0\\): No hay asociación entre las variables (son independientes).\nHipótesis alternativa, \\(H_1\\): Hay una asociación entre las variables (son dependientes).\n\nLas frecuencias observadas Las frecuencias esperadas se calculan como sigue: \\[\n   E_{ij} = \\frac{(n_{i.} \\times n_{.j})}{N}\n   \\] donde \\(E_{ij}\\) es la frecuencia esperada en la celda \\((i,j)\\), \\(n_{i.}\\) es el total de la fila (\\(i\\)), (\\(n_{.j}\\)) es el total de la columna (\\(j\\)), y \\(n_{..}\\) es el total general.\nAhora, comparamos las frecuencias esperadas con las frecuencias observadas, definiendo con ellos el estadístico chi-cuadrado: \\[\n\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\ndonde (\\(O_{ij}\\)) es la frecuencia observada en la celda \\((i,j)\\).\nBajo la hipótesis nula, el estadístico de la prueba sigue una distribución Chi-cuadrado con \\((r-1)(c-1)\\) grados de libertad, donde (\\(r\\)) es el número de filas y (\\(c\\)) es el número de columnas. Podemos calcular el \\(p-valor\\) como: \\[\np-valor=P(\\chi^2_{(r-1)(c-1)}\\geq\\chi^2)\n\\]\nComo ocurría en los contrastes de hipótesis paramétricos, comparamos el \\(p-valor\\) con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo práctico. Prueba Chi-cuadrado de independencia.\n\n\n\n\n\nSupongamos que un investigador desea determinar si hay una asociación entre el tipo de dispositivo usado (Laptop, Tablet, Smartphone) y la satisfacción del usuario (Satisfecho, No satisfecho).\nDeterminamos las hipótesis del problema:\n\nHipótesis nula (\\(H_0\\)): No hay asociación entre el tipo de dispositivo y la satisfacción del usuario (son independientes).\nHipótesis alternativa (\\(H_1\\)): Hay una asociación entre el tipo de dispositivo y la satisfacción del usuario (son dependientes).\n\nRecolectamos datos de una muestra de \\(150\\) usuarios y construimos la siguiente tabla de contingencia:\n\n\n\n\nSatisfecho\nNo satisfecho\nTotal\n\n\n\n\nLaptop\n30\n10\n40\n\n\nTablet\n20\n20\n40\n\n\nSmartphone\n50\n20\n70\n\n\nTotal\n100\n50\n150\n\n\n\nVamos a utilizar R para realizar la prueba Chi-cuadrado de independencia.\n\n# Crear la tabla de contingencia\ntabla &lt;- matrix(c(30, 10, 20, 20, 50, 20), nrow = 3, byrow = TRUE)\nrownames(tabla) &lt;- c(\"Laptop\", \"Tablet\", \"Smartphone\")\ncolnames(tabla) &lt;- c(\"Satisfecho\", \"No Satisfecho\")\ntabla &lt;- as.table(tabla)\n\n# Mostrar la tabla de contingencia\nprint(tabla)\n\n           Satisfecho No Satisfecho\nLaptop             30            10\nTablet             20            20\nSmartphone         50            20\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(tabla)\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabla\nX-squared = 6.9643, df = 2, p-value = 0.03074\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado (\\(chi^2\\)): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba.\nValor p (\\(p-value\\)): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el valor p es 0.031, que es menor que \\(0.05\\). Por lo tanto, rechazamos la hipótesis nula y concluimos que hay una asociación significativa entre el tipo de dispositivo y la satisfacción del usuario.\n\n\n\n\n\n\n\n\n\nEjemplo práctico. Prueba Chi-cuadrado en aprendizaje automático.\n\n\n\n\n\nVamos a realizar un ejemplo de la prueba Chi-cuadrado de independencia utilizando la matriz de confusión de un modelo de aprendizaje automático. Tal y como hemos visto, la prueba Chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas.\nSupongamos que tenemos un modelo de clasificación binaria que predice si un correo electrónico es spam o no. Construiremos estos modelos en la asignatura de Aprendizaje Automático I del Grado en Ciencia e Ingeniería de Datos. Tras entrenar y evaluar el modelo, obtenemos la siguiente matriz de confusión:\n\n\n\n\nPredicción: No Spam\nPredicción: Spam\n\n\n\n\nActual: No Spam\n50\n10\n\n\nActual: Spam\n5\n35\n\n\n\nEsta tabla de doble entrada cruza la predicción del modelo de aprendiaje automático con el verdadero valor en la muestra sobre la que dicho modelo ha sido entrenado. El objetivo de todo modelo de aprendizaje automático es conseguir una matriz de confusión diagonal. En ese caso, no hay errores en la predicción. Todo son éxitos y el modelo es perfecto.\nVamos a realizar la prueba Chi-cuadrado de independencia para ver si hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. Si el modelo es útil, dicha asociación ha de existir. Si el modelo no es útil, entonces la prueba debería de decirnos que no podemos rechazar la hipótesis nula de independencia entre las dos variables (Predicción y Actual).\n\n# Cargar el paquete necesario\nlibrary(MASS)\n\n# Crear la matriz de confusión\nmatriz_confusion &lt;- matrix(c(50, 10, 5, 35), nrow = 2, byrow = TRUE)\ncolnames(matriz_confusion) &lt;- c(\"Predicción: No Spam\", \"Predicción: Spam\")\nrownames(matriz_confusion) &lt;- c(\"Actual: No Spam\", \"Actual: Spam\")\n\n# Mostrar la matriz de confusión\nprint(matriz_confusion)\n\n                Predicción: No Spam Predicción: Spam\nActual: No Spam                  50               10\nActual: Spam                      5               35\n\n# Realizar la prueba Chi-cuadrado de independencia\nprueba_chi_cuadrado &lt;- chisq.test(matriz_confusion)\n\n# Mostrar los resultados de la prueba\nprint(prueba_chi_cuadrado)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matriz_confusion\nX-squared = 45.833, df = 1, p-value = 1.288e-11\n\n\nEl resultado de chisq.test() proporciona el valor de Chi-cuadrado, los grados de libertad y el valor p. En este caso, dado que el \\(p-valor\\) de la prueba es menor que el nivel de significancia (\\(0.05\\)), podemos rechazar la hipótesis nula de independencia y concluir que hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. En otras palabras, el modelo de aprendizaje automático es útil para predecir si un correo electrónico es (o no) spam.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "href": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "title": "4  Contraste no paramétrico",
    "section": "4.3 Prueba de Chi-cuadrado de bondad de ajuste",
    "text": "4.3 Prueba de Chi-cuadrado de bondad de ajuste\nEsta prueba se utiliza para determinar si una distribución de frecuencias observadas sigue una distribución teórica esperada.\n\nHipótesis nula (\\(H_0\\)): Las frecuencias observadas siguen la distribución esperada.\nHipótesis alternativa (\\(H_1\\)): Las frecuencias observadas no siguen la distribución esperada.\n\nEl estadístico del contraste, como en el caso anterior se calcula como sigue: \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\] donde (\\(O_i\\)) son las frecuencias observadas y (\\(E_i\\)) son las frecuencias esperadas según la distribución teórica.\nUtilizando la distribución chi-cuadrado con (\\(k-1\\)) grados de libertad, donde (\\(k\\)) es el número de categorías, podemos calcular el \\(p-valor\\). Comparamos el \\(p-valor\\) con el nivel de significancia. Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Chi-cuadrado de bondad de ajuste.\n\n\n\n\n\nSupongamos que un investigador quiere determinar si los resultados de un dado son uniformemente distribuidos. El dado se lanza \\(60\\) veces y los resultados son los siguientes:\n\n1: 8 veces\n2: 10 veces\n3: 9 veces\n4: 11 veces\n5: 12 veces\n6: 10 veces\n\nQueremos comprobar si estos resultados siguen una distribución uniforme, es decir, cada número tiene la misma probabilidad de 1/6.\nFormulamos las hipótesis:\n\nHipótesis nula (H0): Los resultados del dado siguen una distribución uniforme.\nHipótesis alternativa (H1): Los resultados del dado no siguen una distribución uniforme.\n\nA continuación se recogen los datos:\n\n# Resultados observados\nobserved &lt;- c(8, 10, 9, 11, 12, 10)\n\n# Frecuencias esperadas si el dado es justo (distribución uniforme)\nexpected &lt;- rep(60 / 6, 6)\n\nY se realiza la prueba\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(observed, p = rep(1/6, 6))\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1, df = 5, p-value = 0.9626\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado  (\\(\\chi^2\\)): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba, que es \\(n - 1\\) donde \\(n\\) es el número de categorías.\nValor p (p-value): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el \\(p-valor\\) es 0.963, que es mucho mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que los resultados del dado no siguen una distribución uniforme.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#pruebas-de-de-homogeneidad",
    "href": "nopara.html#pruebas-de-de-homogeneidad",
    "title": "4  Contraste no paramétrico",
    "section": "4.4 Pruebas de de homogeneidad",
    "text": "4.4 Pruebas de de homogeneidad\nLa prueba no paramétrica de homogeneidad se utiliza para determinar si dos o más muestras independientes provienen de la misma distribución o de distribuciones similares. Estas pruebas son útiles cuando no se cumplen los supuestos necesarios para las pruebas paramétricas, como la normalidad de los datos. Ejemplo de pruebas no paramétricas de Homogeneidad son:\n\nPrueba de Kolmogorov-Smirnov para dos muestras: Compara dos muestras para verificar si provienen de la misma distribución.\nPrueba de Mann-Whitney U (o Wilcoxon Rank-Sum Test): Compara dos muestras independientes para determinar si tienen la misma distribución.\nPrueba de Kruskal-Wallis: Extiende la prueba de Mann-Whitney U a más de dos muestras independientes.\n\n\n4.4.1 Prueba de Kolmogorov-Smirnov para dos muestras\nLa Prueba de Kolmogorov-Smirnov (K-S) para dos muestras es una prueba no paramétrica utilizada para determinar si dos muestras independientes provienen de la misma distribución. A diferencia de otras pruebas que se centran en comparar medias o varianzas, la prueba K-S compara las distribuciones acumuladas de dos muestras.\nLas hipótesis de la prueba son:\n\nHipótesis nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis alternativa (\\(H_1\\)): Las dos muestras provienen de distribuciones diferentes.\n\nPara cada muestra, se construyen las funciones de distribución empírica (EDF). La función de distribución empírica \\(F_n(x)\\) es una función escalonada que aumenta en \\(\\frac{1}{n}\\) en cada punto de datos, donde \\(n\\) es el tamaño de la muestra. Para más detalles desplegar aquí:\n\n\n\n\n\n\nFunción de Distribución Empírica\n\n\n\n\n\nLa función de distribución empírica (EDF, por sus siglas en inglés) es una función de distribución de probabilidad utilizada para estimar la distribución subyacente de un conjunto de datos observados. Es una herramienta no paramétrica que proporciona una estimación de la función de distribución acumulada de una muestra de datos.\nDada una muestra de datos \\((X_1, X_2, \\ldots, X_n)\\), la función de distribución empírica \\(F_n(x)\\) se define como: \\[\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(X_i \\leq x)\n\\] donde \\(I(X_i \\leq x)\\) es una función indicadora que toma el valor \\(1\\) si \\(X_i \\leq x\\) y \\(0\\) en caso contrario.\nEn otras palabras, \\(F_n(x)\\) es la proporción de valores en la muestra que son menores o iguales a \\(x\\).\nPropiedades de la Función de Distribución Empírica\n\nEscalonada: La EDF es una función escalonada que incrementa en pasos de \\(1/n\\) en cada punto de datos.\nNo decreciente: La EDF nunca disminuye a medida que \\(x\\) aumenta.\nLímites: La EDF varía entre \\(0\\) y \\(1\\). Específicamente, \\(F_n(x) = 0\\) para \\(x\\) menor que el valor mínimo de la muestra y \\(F_n(x) = 1\\) para \\(x\\) mayor que el valor máximo de la muestra.\n\n\n\n\nCalculamos el Estadístico \\(D\\) de la prueba K-S es la máxima diferencia absoluta entre las dos funciones de distribución empírica: \\[\n     D = \\sup_x |F_{n_1}(x) - F_{n_2}(x)|\n\\]\nDonde, \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) son las funciones de distribución empírica de las dos muestras.\nEl \\(p-valor\\) se calcula para determinar la significancia de la diferencia observada. Se utiliza la distribución asintótica del estadístico D para calcular el valor p. El valor p se determina utilizando la distribución del estadístico \\(D\\) bajo la hipótesis nula de que ambas muestras provienen de la misma distribución. El cálculo exacto del \\(p-valor\\) para la prueba de K-S no es trivial y generalmente se realiza mediante métodos numéricos o tablas pre-calculadas. Sin embargo, se puede aproximar utilizando la distribución asintótica del estadístico \\(D\\).\nPara muestras grandes, el valor p se puede aproximar usando la fórmula: \\[\np \\approx Q_{KS}(\\sqrt{n} D)\n\\] donde:\n\n\\(n = \\frac{n_1 \\cdot n_2}{n_1 + n_2}\\) es el número efectivo de muestras.\n\\(D\\) es el valor del estadístico K-S.\n\\(Q_{KS}\\) es una función que representa la cola superior de la distribución de Kolmogorov-Smirnov.\n\nLa función \\(Q_{KS}\\) para grandes valores de \\(n\\) se puede aproximar usando la siguiente fórmula: \\[\nQ_{KS}(\\lambda) = 2 \\sum_{k=1}^{\\infty} (-1)^{k-1} e^{-2k^2 \\lambda^2}\n\\]\ndonde \\(\\lambda = \\sqrt{n} D\\).\nPara valores prácticos, esta sumatoria converge rápidamente y a menudo solo se necesita calcular unos pocos términos.\nComo en otros contrastes, si el \\(p-valor\\) calculado es menor que un grado de significancia estadística previamente fijado \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n\nEjemplo práctico. Prueba K-S\n\n\n\n\n\nSupongamos que queremos comparar dos muestras para determinar si provienen de la misma distribución.\n\nMuestra 1: 1, 2, 3, 4, 5\nMuestra 2: 2, 3, 4, 5, 6\n\nFormulamos las hipótesis\n\nH0: Las dos muestras provienen de la misma distribución.\nH1: Las dos muestras provienen de distribuciones diferentes.\n\nCalculamos las funciones de distribución empírica \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) para las dos muestras.\n\n\n\n\\(x\\)\n\\(F_{n_1}(x)\\)\n\\(F_{n_2}(x)\\)\n\n\n\n\n1\n0.2\n0.0\n\n\n2\n0.4\n0.2\n\n\n3\n0.6\n0.4\n\n\n4\n0.8\n0.6\n\n\n5\n1.0\n0.8\n\n\n6\n1.0\n1.0\n\n\n\nCalculamos la máxima diferencia absoluta entre \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\): \\[\nD = \\max(|0.2 - 0.0|, |0.4 - 0.2|, |0.6 - 0.4|, |0.8 - 0.6|, |1.0 - 0.8|, |1.0 - 1.0|) = 0.2\n\\]\nEl valor p se determina utilizando la distribución asintótica del estadístico \\(D\\). Esto generalmente se hace usando tablas de referencia o software estadístico.\nImplementación en R\nPodemos realizar esta prueba en R utilizando la función ks.test:\n\n# Datos de las dos muestras\nmuestra1 &lt;- c(1, 2, 3, 4, 5)\nmuestra2 &lt;- c(2, 3, 4, 5, 6)\n\n# Realizar la prueba de Kolmogorov-Smirnov\nks_test &lt;- ks.test(muestra1, muestra2)\n\n# Mostrar los resultados\nprint(ks_test)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  muestra1 and muestra2\nD = 0.2, p-value = 1\nalternative hypothesis: two-sided\n\n\nEl resultado de ks.test incluye varios componentes clave:\n\nEstadístico D: Este es el valor calculado del estadístico K-S.\nValor p (p-value): Este es el valor p asociado con el estadístico calculado.\nDescripción de las muestras: Indica las muestras comparadas.\n\nEn este ejemplo, el valor p es 1, que es mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que las dos muestras provienen de distribuciones diferentes.\n\n\n\n\n\n4.4.2 Prueba de Mann-Whitney U\nLa prueba de Mann-Whitney U, también conocida como prueba de Wilcoxon para muestras independientes, es una prueba no paramétrica que se utiliza para comparar dos muestras independientes para determinar si provienen de la misma distribución. Es una alternativa a la prueba t de Student cuando no se cumplen los supuestos de normalidad. En lugar de trabajar con los valores originales, la prueba utiliza los rangos de los datos.\nLas hipótesis que vamos a contrastar son:\n\nHipótesis Nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Las dos muestras no provienen de la misma distribución.\n\nEn primer lugar se combinan los datos de ambas muestras y se ordenan los valores de menor a mayor. A continuación se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nSe calcula el estadístico del contraste tal y como sigue: \\[  U_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1\n\\] \\[\nU_2 =U_1 = n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2\n\\] donde:\n\n\\(n_1\\) y \\(n_2\\) son los tamaños de las dos muestras.\n\\(R_1\\) y \\(R_2\\) son las sumas de los rangos de las muestras \\(1\\) y \\(2\\), respectivamente.\n\nEl estadístico \\(U\\) final es: \\[\nU=min(U_1,U_2)\n\\]\nEl \\(p-valor\\) se determina comparando el estadístico \\(U\\) con una distribución de referencia para U (tabla de Mann-Whitney) o mediante aproximación normal para grandes tamaños de muestra. En ese caso: \\[\nZ=\\frac{U-E(U)}{\\sqrt{Var(U)}} \\approx N(0,1)\n\\] siendo \\[\nE(U)=n_1n_2+\\frac{n_1(n_1+1)}{2}-E(R_1)\n\\] \\[\nE(U)= n_1n_2+\\frac{n_1(n_1+1)}{2}-\\frac{n_1(n_1+n_2+1)}{2}=\\frac{n_1n_2}{2}\n\\] y \\[\nVar(U)=Var(R_1)=\\frac{n_1n_2(n_1+n_2+1)}{12}\n\\] Y podemos calcular el \\(p-valor\\) como hemos hecho en métodos anteriores: \\[\np-valor=P(Z&gt;z)\n\\] Siendo \\(z\\) el valor del estadístico calculado en las muestras.\nComparamos el \\(p-valor\\) p con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo práctico. Prueba Mann-Whitney\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de entrega (en días) de dos proveedores distintos:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\n\nEn primer lugar combinar y ordenamos las muestra\nValores combinados y ordenados: \\(1, 2, 3, 4, 4, 5, 6, 7, 8, 9\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3\n4: rango 4.5 (promedio de rangos 4 y 5)\n4: rango 4.5\n5: rango 6\n6: rango 7\n7: rango 8\n8: rango 9\n9: rango 10\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3, 6, 7, 9\\) (sumados dan \\(R_1 = 27\\))\nProveedor B: \\(1, 4.5, 4.5, 8, 10\\) (sumados dan \\(R_2 = 28\\))\n\nPasamos a calcular el estadístico \\(U\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\nCálculo de \\(U_1\\) y \\(U_2\\):\n\\[\nU_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1  =5\\cdot 5+ \\frac{5 \\cdot 6}{2} -27 = 23\n\\]\n\\[\nU_2 =n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2  =5\\cdot 5+  \\frac{5 \\cdot 6}{2}-28 = 12\n\\]\nEl estadístico U es el menor de \\(U_1\\) y \\(U_2\\): \\(U = 12\\).\nPara determinar el valor p, utilizamos una tabla de referencia para U o una aproximación normal si las muestras son grandes. En este caso, consultamos una tabla para \\(n_1 = 5\\) y \\(n_2 = 5\\). Si no se dispone de la tabla, se puede utilizar software estadístico para calcular el \\(p-valor\\) como sigue:\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\n\n# Realizar la prueba de Mann-Whitney U\ntest &lt;- wilcox.test(proveedorA, proveedorB, alternative = \"two.sided\")\n\nWarning in wilcox.test.default(proveedorA, proveedorB, alternative =\n\"two.sided\"): cannot compute exact p-value with ties\n\n# Mostrar los resultados\nprint(test)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  proveedorA and proveedorB\nW = 12, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nLos resultados muestra:\n\nEstadístico W: El valor calculado del estadístico U.\nValor p (p-value): El valor p asociado con el estadístico.\nHipótesis alternativa: La prueba es de dos lados, lo que significa que estamos probando si las distribuciones son diferentes en cualquier dirección.\n\nEn este ejemplo, el \\(p-valor\\) es 1, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las dos muestras provienen de diferentes distribuciones.\n\n\n\n\n\n4.4.3 Prueba de Kruskal-Wallis\nLa prueba de Kruskal-Wallis es una prueba no paramétrica utilizada para comparar tres o más muestras independientes para determinar si provienen de la misma distribución. Es una extensión de la prueba de Mann-Whitney U a más de dos grupos y una alternativa robusta a la ANOVA que veremos en el Capítulo 5 cuando no se cumplen los supuestos de normalidad y homogeneidad de varianzas.\nDado que es una prueba no paramétrica, no requiere que los datos provengan de una distribución normal. Al igual que la prueba de Mann-Whitney, la prueba de Kruskal-Wallis trabaja con los rangos de los datos en lugar de los valores originales.\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): Todas las muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Al menos una de las muestras proviene de una distribución diferente.\n\nA continuación, se combinan los datos de todas las muestras y se ordenan los valores de menor a mayor. Se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nCalculamos el estadístico H utilizando la siguiente fórmula: \\[\n     H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\] donde:\n\n\\(N\\) es el tamaño total de la muestra (suma de los tamaños de las \\(k\\) muestras).\n\\(R_i\\) es la suma de los rangos de la \\(i\\)-ésima muestra.\n\\(n_i\\) es el tamaño de la \\(i\\)-ésima muestra.\n\\(k\\) es el número de muestras.\n\nPara tamaños de muestra relativamente grandes el estadístico \\(H\\) sigue una distriniución \\(\\chi^2\\) con \\(k-1\\) grados de libertdad. El \\(p-valor\\) se determina, por tanto como \\[\np-valor=P(H&gt;h)\n\\] donde h es el valor calculado para un caso particular.\n\n\n\n\n\n\nEjemplo práctico. Prueba Kruskal-Wallis\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de espera (en días) de tres proveedores diferentes:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\nProveedor C: \\(3, 4, 6, 8, 10\\)\n\nValores combinados y ordenados: \\(1, 2, 3, 3, 4, 4, 4, 5, 6, 6, 7, 8, 8, 9, 10\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3.5 (promedio de rangos 3 y 4)\n3: rango 3.5\n4: rango 6 (promedio de rangos 5, 6 y 7)\n4: rango 6\n4: rango 6\n5: rango 8\n6: rango 9.5 (promedio de rangos 9 y 10)\n6: rango 9.5\n7: rango 11\n8: rango 12.5 (promedio de rangos 12 y 13)\n8: rango 12.5\n9: rango 14\n10: rango 15\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3.5, 8, 9.5, 12.5\\) (sumados dan \\(R_1 = 35.5\\))\nProveedor B: \\(1, 6, 6, 11, 14\\) (sumados dan \\(R_2 = 38\\))\nProveedor C: \\(3.5, 6, 9.5, 12.5, 15\\) (sumados dan \\(R_3 = 46.5\\))\n\nCalculamos el Estadístico \\(H\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\\(n_3 = 5\\)\n\nTamaño total de la muestra: \\(N = 15\\)\n\\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\]\n\\[\nH = \\frac{12}{15 \\cdot 16} \\left( \\frac{35.5^2}{5} + \\frac{38^2}{5} + \\frac{46.5^2}{5} \\right) - 3 \\cdot 16 = 0.665\n\\]\nPara determinar el \\(p-valor\\), utilizamos una tabla de distribución chi-cuadrado con \\(k-1 = 3-1 = 2\\) grados de libertad.\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\nproveedorC &lt;- c(3, 4, 6, 8, 10)\n\n# Realizar la prueba de Kruskal-Wallis\ntest &lt;- kruskal.test(list(proveedorA, proveedorB, proveedorC))\n\n# Mostrar los resultados\nprint(test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(proveedorA, proveedorB, proveedorC)\nKruskal-Wallis chi-squared = 0.67342, df = 2, p-value = 0.7141\n\n\nde donde obtenemos\n\nKruskal-Wallis chi-squared: El valor calculado del estadístico \\(H\\).\ndf: Los grados de libertad.\nValor p (p-value): El \\(p-valor\\) asociado con el estadístico \\(H\\).\n\nEn este ejemplo, el \\(p-valor\\) es 0.714, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las tres muestras provienen de diferentes distribuciones; podrían provenir de la misma distribución.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "nopara.html#prueba-sobre-muestras-pareadas",
    "href": "nopara.html#prueba-sobre-muestras-pareadas",
    "title": "4  Contraste no paramétrico",
    "section": "4.5 Prueba sobre muestras pareadas",
    "text": "4.5 Prueba sobre muestras pareadas\n\n4.5.1 Prueba del signo\nLa prueba del signo es una prueba no paramétrica utilizada para comparar dos muestras relacionadas o emparejadas. Se emplea cuando se tienen dos conjuntos de datos dependientes y se desea determinar si hay una diferencia significativa en sus medianas. Es particularmente útil cuando los supuestos de normalidad necesarios para pruebas paramétricas como la prueba t para muestras pareadas no se cumplen.\nSupongamos que tenemos dos muestras relacionadas \\(X\\) e \\(Y\\) de tamaño \\(n\\):\nFormular las hipótesis:\n\nHipótesis Nula (\\(H_0\\)): Las medianas de las dos muestras son iguales.\nHipótesis Alternativa (\\(H_1\\)): Las medianas de las dos muestras son diferentes.\n\nCalcular las diferencias: Para cada par \\((X_i, Y_i)\\), calcular la diferencia \\(D_i = X_i - Y_i\\).\nContar los signos: - Contar cuántas diferencias son positivas (\\(D_i &gt; 0\\)). - Contar cuántas diferencias son negativas (\\(D_i &lt; 0\\)). - Ignorar las diferencias que son cero (\\(D_i = 0\\)).\nEstadístico de prueba:\n\nDenotar el número de diferencias positivas por \\(S_+\\).\nDenotar el número de diferencias negativas por \\(S_-\\).\n\nEl estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\):\\[\n   s=min(S_+,S_-)\n   \\]\nDeterminar el valor crítico: Consultar una tabla de la distribución binomial o la tabla de la prueba del signo para obtener el valor crítico correspondiente al nivel de significancia \\(\\alpha\\) y el tamaño de la muestra efectiva \\(n\\) (número de pares no nulos). Podemos calcular el valor crítico como sigue: \\[\np-valor=P(S\\leq s)\n\\] donde \\(S\\) es binomial \\(n\\), \\(p=0.5\\).\nDecisión:\n\nRechazar \\(H_0\\) si el estadístico de la prueba es menor o igual al valor crítico.\nNo rechazar \\(H_0\\) si el estadístico de la prueba es mayor que el valor crítico.\n\n\n\n\n\n\n\nEjemplo práctico. Prueba del signo\n\n\n\n\n\nSupongamos que un investigador quiere comparar los tiempos de reacción antes y después de un tratamiento en 10 sujetos. Los datos son los siguientes:\n\n\n\nSujeto\nAntes (X)\nDespués (Y)\n\n\n\n\n1\n15\n10\n\n\n2\n20\n18\n\n\n3\n25\n20\n\n\n4\n30\n30\n\n\n5\n18\n15\n\n\n6\n22\n19\n\n\n7\n26\n21\n\n\n8\n28\n26\n\n\n9\n24\n22\n\n\n10\n20\n20\n\n\n\nFormular las hipótesis:\n\n\\(H_0\\): No hay diferencia en los tiempos de reacción antes y después del tratamiento.\n\\(H_1\\): Hay una diferencia en los tiempos de reacción antes y después del tratamiento.\n\nCalcular las diferencias:\n\n\n\nSujeto\nDiferencia (D = X - Y)\nSigno\n\n\n\n\n1\n5\n+\n\n\n2\n2\n+\n\n\n3\n5\n+\n\n\n4\n0\n0\n\n\n5\n3\n+\n\n\n6\n3\n+\n\n\n7\n5\n+\n\n\n8\n2\n+\n\n\n9\n2\n+\n\n\n10\n0\n0\n\n\n\nContar los signos:\n\n\\(S_+ = 8\\)\n\\(S_- = 0\\)\nPares nulos (\\(D = 0\\)): 2\n\nEstadístico de prueba: El estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\), que es 0 en este caso.\nDeterminar el valor crítico: Para un nivel de significancia \\(\\alpha = 0.05\\) y \\(n = 8\\) (solo los pares no nulos se consideran), consultamos la tabla de la prueba del signo y encontramos que el valor crítico es 1.\nDecisión: Como el estadístico de la prueba (0) es menor que el valor crítico (1), rechazamos la hipótesis nula \\(H_0\\). Hay suficiente evidencia para concluir que hay una diferencia significativa en los tiempos de reacción antes y después del tratamiento.\n\n\n\n\n\n4.5.2 Prueba de rangos de signos de Wilcoxon\nPara comparar muestras pareadas la opción no paramétrica más común es la prueba de Wilcoxon de rangos de signos (Wilcoxon signed-rank test). La prueba de Wilcoxon de rangos de signos es una alternativa no paramétrica a la prueba t de muestras pareadas. Se utiliza cuando los datos no cumplen con los supuestos de normalidad necesarios para la prueba t. En lugar de comparar medias, esta prueba compara las medianas de las diferencias entre las dos muestras pareadas. Es una prueba ideal para muestras pequeñas y para datos ordinales o de razón/intervalo cuando la normalidad no puede ser asumida.\nProcedimiento:\n\nCalcular las diferencias entre cada par de observaciones.\nOrdenar las diferencias absolutas y asignarles rangos, ignorando las diferencias que sean cero.\nAsignar signos a los rangos de acuerdo con el signo de las diferencias originales.\nCalcular la suma de los rangos positivos y la suma de los rangos negativos.\nDeterminar el estadístico de prueba: El estadístico de Wilcoxon es el menor de las dos sumas de rangos.\nComparar el estadístico de prueba con los valores críticos de la tabla de Wilcoxon para determinar la significancia estadística.\n\nLa prueba de Wilcoxon de signos y rangos es robusta y fácil de implementar, lo que la convierte en una herramienta valiosa para el análisis de muestras pareadas en situaciones donde los supuestos de normalidad no se cumplen.\n\n\n\n\n\n\nEjemplo práctico. Prueba sobre datos emparejados\n\n\n\n\n\nSupongamos que tenemos dos conjuntos de datos emparejados que representan las puntuaciones antes y después de una intervención:\n\n# Datos de ejemplo\nantes &lt;- c(10, 20, 30, 40, 50)\ndespues &lt;- c(12, 18, 33, 35, 55)\n\n# Realizar la prueba de Wilcoxon de signos y rangos\nresultado &lt;- wilcox.test(antes, despues, paired = TRUE)\n\n# Mostrar el resultado\nprint(resultado)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  antes and despues\nV = 6, p-value = 0.7855\nalternative hypothesis: true location shift is not equal to 0\n\n\nEl estadístico V es 6 y el \\(p-valor\\) es 0.785. En este caso, dado que el valor p es mayor que un nivel de significancia común (como \\(0.05\\)), no se rechaza la hipótesis nula de que no hay una diferencia significativa entre las puntuaciones antes y después de la intervención.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contraste no paramétrico</span>"
    ]
  },
  {
    "objectID": "conclusiones.html",
    "href": "conclusiones.html",
    "title": "4  Conclusiones",
    "section": "",
    "text": "4.1 Resumen de los aprendizajes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#resumen-de-los-aprendizajes",
    "href": "conclusiones.html#resumen-de-los-aprendizajes",
    "title": "6  Conclusiones",
    "section": "",
    "text": "Comprensión de poblaciones y muestras: Esperamos que hayas aprendido a distinguir entre población y muestra, comprendiendo la relevancia de los modelos probabilísticos y la importancia de asegurar un muestreo adecuado para obtener resultados representativos.\nEstimación de parámetros: Juntos hemos abordado diversas técnicas de estimación puntual y por intervalos, permitiendo a los estudiantes realizar inferencias sobre parámetros de la población con un nivel de confianza adecuado.\nContrastes de Hipótesis: A lo largo del curso debes haber desarrollado la habilidad de plantear y resolver contrastes de hipótesis, una herramienta fundamental para la toma de decisiones informadas basadas en datos.\nContrastes no paramétricos: Para los casos donde las suposiciones paramétricas no se cumplen, se han introducido y aplicado métodos no paramétricos, ampliando el repertorio de herramientas disponibles para el trabajo futuro del científico e ingeniero de datos.\nAnálisis de la Varianza (ANOVA): Se ha presentado, por primera vez en el Grado, el ANOVA como una técnica esencial para comparar múltiples grupos y entender las variaciones entre ellos, destacando su aplicación en diferentes contextos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#reflexiones-finales",
    "href": "conclusiones.html#reflexiones-finales",
    "title": "4  Conclusiones",
    "section": "4.2 Reflexiones finales",
    "text": "4.2 Reflexiones finales",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#mirando-hacia-adelante",
    "href": "conclusiones.html#mirando-hacia-adelante",
    "title": "4  Conclusiones",
    "section": "4.3 Mirando hacia adelante",
    "text": "4.3 Mirando hacia adelante\nCon el conocimiento y las habilidades adquiridas, los estudiantes están mejor equipados para explorar más a fondo el vasto campo de la ciencia de datos. La estadística y la inferencia estadística seguirán evolucionando con el avance de la tecnología y la disponibilidad de datos. Por lo tanto, es crucial que los futuros profesionales mantengan una mentalidad de aprendizaje continuo y estén abiertos a adoptar nuevas metodologías y herramientas.\nEn conclusión, esperamos que este libro haya proporcionado una comprensión profunda y práctica de los Modelos de Regresión, y que inspire a los estudiantes a aplicar estos conocimientos con confianza y creatividad en sus proyectos futuros. La capacidad de analizar datos de manera crítica y tomar decisiones basadas en evidencias es una habilidad poderosa y transformadora, que sin duda abrirá numerosas oportunidades en el ámbito profesional.\nOs recordamos que en próximos cursos os encontraréis con las asignaturas de Aprendizaje Automático I y Aprendizaje Automático II, donde aplicaréis muchas de las técnicas y herramientas que hemos aprendido juntos.\n¡Buena suerte!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Draper, NR. 1998. Applied Regression Analysis. McGraw-Hill.\nInc.\n\n\nHosmer Jr, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013.\nApplied Logistic Regression. John Wiley & Sons.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. McGraw-hill.\n\n\nLambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nNelder, John Ashworth, and Robert WM Wedderburn. 1972.\n“Generalized Linear Models.” Journal of the Royal\nStatistical Society Series A: Statistics in Society 135 (3):\n370–84.",
    "crumbs": [
      "Bibliografía"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos de Regresión",
    "section": "",
    "text": "Prefacio\nLa ciencia y la ingeniería de datos han emergido como campos fundamentales en la era de la información, donde la capacidad de analizar y predecir comportamientos a partir de datos se ha convertido en una habilidad esencial. En este contexto, los modelos de regresión juegan un papel crucial al permitirnos describir y cuantificar las relaciones entre variables, así como predecir resultados futuros basados en datos históricos. Este libro está diseñado para proporcionar una comprensión profunda y práctica de las diversas técnicas de regresión utilizadas en la ciencia de datos. Basado en el contenido de la asignatura impartida en el Grado en Ciencia e Ingeniería de Datos, este texto abarca desde los modelos de regresión lineal más simples hasta los modelos no lineales y generalizados más complejos.\nA lo largo de los capítulos, los alumnos encontrarán una combinación de teoría rigurosa y aplicaciones prácticas. Se abordarán temas como el modelo de regresión lineal simple y múltiple, métodos de selección de variables, problemas de regularización, transformación de variables, ingeniería de características, y modelos de regresión generalizada, entre otros. Además, se incluirán ejemplos y ejercicios prácticos utilizando el software R, que permitirán a los lectores aplicar los conceptos aprendidos a conjuntos de datos reales.\nEl objetivo de este libro es equipar a los estudiantes con las herramientas necesarias para modelar la dependencia entre variables en conjuntos de datos complejos, evaluar la calidad de los modelos, interpretarlos y proponer mejoras. Al finalizar la lectura, los alumnos habrán desarrollado una comprensión sólida de los modelos de regresión y estarán preparados para enfrentar desafíos en el ámbito de la ciencia e ingeniería de datos con confianza y creatividad.\nAgradecemos a los profesores y colegas que han contribuido al desarrollo de esta asignatura y a la elaboración de este libro. Su dedicación y conocimiento han sido fundamentales para la creación de este recurso educativo.\nEsperamos que este libro sea una guía útil y enriquecedora en tu viaje por el fascinante mundo de los modelos de regresión.\n¡Comenzamos!\n\n\n\n\n\n\nResultados de aprendizaje.\n\n\n\n\n\n\nQue los estudiantes hayan demostrado poseer y comprender conocimientos en un área de estudio que parte de la base de la educación secundaria general, y se suele encontrar a un nivel que, si bien se apoya en libros de texto avanzados, incluye también algunos aspectos que implican conocimientos procedentes de la vanguardia de su campo de estudio.\nQue los estudiantes hayan desarrollado aquellas habilidades de aprendizaje necesarias para emprender estudios posteriores con un alto grado de autonomía.\nComprender y utilizar materias, modelos y herramientas en el ámbito de la ciencia e ingeniería de datos que permitan comprender los problemas científico-técnicos fundamentales en este campo, así como evaluar las ventajas e inconvenientes de diferentes alternativas metodológicas.\nResolver problemas con iniciativa y creatividad.\nTomar decisiones de forma autónoma, elaborando de forma adecuada y original, argumentos razonados, pudiendo obtener así hipótesis razonables y contrastables y derivando conclusiones apropiadas.\nModelar la dependencia entre una variable respuesta y varias variables explicativas, en conjuntos de datos complejos, mediante técnicas de estadística avanzada (incluyendo métodos de regresión, inferencia bayesiana, etc.) y métodos de aprendizaje automático, evaluando e interpretando los resultados obtenidos.\nEmplear distintas técnicas de modelado de datos, análisis de datos y aprendizaje automático: elegir y emplear técnicas adecuadas, evaluando la calidad de los modelos, validándolos, interpretándolos y proponiendo mejoras.\n\n\n\n\n\n\n\n\n\n\nGrado en Ciencia e Ingeniería de Datos\n\n\n\nEste libro presenta el material de la asignatura de Modelos de Regresión del grado en Ciencia e Ingeniería de Datos de la Universidad Rey Juan Carlos. Su contenido está fuertemente relacionado con las asignaturas de Inferencia Estadística, Aprendizaje Automático I y Aprendizaje Automático II.\n\n\n\n\n\n\n\n\nConocimientos previos\n\n\n\nEs conveniente haber superado con éxito las asignaturas de Cálculo, Herramientas Matemáticas para la Ciencia de Datos I y Probabilidad y Simulación e Inferencia Estadística, del grado en Ciencia e Ingeniería de Datos.\n\n\n\n\n\n\n\n\nSobre los autores\n\n\n\nVíctor Aceña Gil es graduado en Matemáticas por la UNED, máster en Tratamiento Estadístico y Computacional de la Información por la UCM y la UPM, doctor en Tecnologías de la Información y las Comunicaciones por la URJC y profesor del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI.\nCarmen Lancho Martín es graduada en Matemáticas y Estadística por la Universidad Complutense de Madrid (UCM), máster en Tratamiento Estadístico y Computacional de la Información por la UCM y la Universidad Politécnica de Madrid (UPM), doctora en Tecnologías de la Información y las Comunicaciones por la Universidad Rey Juan Carlos (URJC) y profesora del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI.\nIsaac Martín de Diego es diplomado en Estadística por la Universidad de Valladolid (UVA), licenciado en Ciencias y Técnicas Estadísticas por la Universidad Carlos III de Madrid (UC3M), doctor en Ingeniería Matemática por la UC3M, catedrático de Ciencias de la Computación e Inteligencia Artificial del departamento de Informática y Estadística de la URJC. Es fundador y coordinador del DSLAB y del DSLAB-TI.\n\n\nEsta obra está bajo una licencia de Creative Commons Atribuciónn-CompartirIgual 4.0 Internacional.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "tema1.html",
    "href": "tema1.html",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "",
    "text": "1.1 Modelización estadística\nLa modelización estadística es un proceso estructurado que permite analizar y describir relaciones entre variables mediante modelos matemáticos. Este enfoque es fundamental en la estadística aplicada y proporciona herramientas para interpretar datos, realizar predicciones y tomar decisiones fundamentadas. A continuación, se describen los pasos clave de este proceso. Este enfoque sistemático asegura que el modelo ajustado sea robusto, interpretable y útil para realizar predicciones confiables. La revisión continua y el ajuste basado en evidencia permiten capturar la complejidad de los datos de manera efectiva.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#preguntas",
    "href": "tema1.html#preguntas",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.1 Preguntas",
    "text": "1.1 Preguntas\nTu objetivo principal durante el EDA es adquirir una comprensión profunda de los datos que se están analizando. La forma más sencilla de hacerlo es utilizar preguntas como herramientas para guiar la investigación. Cuando planteas una pregunta, ésta centra tu atención en una parte específica del conjunto de datos y te ayuda a decidir qué gráficos, modelos o transformaciones realizar.\nEDA es un proceso creativo y como tal, la clave para llevarlo a cabo consiste en el planteamiento de preguntas de calidad. ¿Qué preguntas son las correctas? La respuesta es que depende del conjunto de datos con el que se trabaje.\n\n\n\n\n\n\nJohn Tukey\n\n\n\nMucho mejor una respuesta aproximada a la pregunta correcta, que a menudo es vaga, que una respuesta exacta a la pregunta incorrecta, que siempre se puede precisar.\n\n\nAl inicio del análisis, puede resultar todo un desafío formular preguntas reveladoras, ya que aún no se conoce completamente la información contenida en el conjunto de datos. Si estás involucrado en un proceso de inferencia estadística, en muchas ocasiones, esas preguntas vendrán formuladas por un experto del dominio o por un superior con conocimientos estadísticos. Cada nueva pregunta que se plantee te llevará a explorar un nuevo aspecto de tus datos, aumentando así las posibilidades de hacer descubrimientos importantes.\n\n\n\n\n\n\nPara recordar\n\n\n\nDurante la preparación y limpieza de los datos acumulamos pistas sobre los modelos de aprendizaje más adecuados que podrán ser aplicados en etapas posteriores.\n\n\nAlgunas de las preguntas que, generalmente, deberían de abordarse durante el EDA son:\n\n¿Cuál es el tamaño de la base de datos? Es decir:\n\n¿Cuántas observaciones hay?\n¿Cuántas variables/características están medidas?\n¿Disponemos de capacidad de cómputo en nuestra máquina para procesar la base de datos o necesitamos más recursos?\n¿Existen valores faltantes?\n\n¿Qué tipo variables aparecen en la base de datos?\n\n¿Qué variables son discretas?\n¿Cuáles son continuas?\n¿Qué categorías tienen las variables?\n¿Hay variables tipo texto?\n\nVariable objetivo: ¿Existe una variable de “respuesta”?\n\n¿Binaria o multiclase?\n\n¿Es posible identificar variables irrelevantes?. Estudiar variables relevantes requiere, habitualmente, métodos estadísticos.\n¿Es posible identificar la distribución que siguen las variables?\nCalcular estadísticos resumen (media, desviación típica, frecuencia,…) de todas las variables de interés. Estudiaremos las propiedades de estos estimadores en el próximo capítulo.\nDetección y tratamiento de valores atípicos.\n\n¿Son errores de media?\n¿Podemos eliminarlos?\n\n¿Existe correlación entre variables?\n\n\n\n\n\n\n\nPara recordar\n\n\n\nUna correcta preparación y limpieza de datos implica, sin duda, un ahorro de tiempo en etapas posteriores del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#entender-el-negocio",
    "href": "tema1.html#entender-el-negocio",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.2 Entender el negocio",
    "text": "1.2 Entender el negocio\nLa comprensión del problema que estamos abordando representa una de las primeras etapas en cualquier proyecto de ciencia de datos. En la mayoría de los casos, esta tarea se realiza en estrecha colaboración con expertos en el dominio correspondiente, quienes a menudo son las personas que han solicitado (y a menudo financian) el análisis de datos. Es importante recordar que cualquier estudio que involucre ciencia de datos requiere un conocimiento profundo del dominio, el cual debe ser compartido con el científico de datos. Por lo tanto, el profesional de la ciencia de datos debe poseer un conocimiento suficiente para enfrentar con confianza los diversos desafíos que puedan surgir. Esta comprensión inicial permite establecer los objetivos del proyecto y procesar los datos de manera correcta para obtener información valiosa. A través de esta información, se busca derivar conocimientos aplicables. Este conocimiento puede ser aprendido y almacenado para su uso futuro, lo que lleva a la sabiduría, según la jerarquía de conocimiento presentada en la Figura Figura 1.1.\n\n\n\n\n\n\nFigura 1.1: Jerarquía de Conocimiento\n\n\n\n\n\n\n\n\n\nClaude Lévi-Strauss\n\n\n\n“El científico no es una persona que da las respuestas correctas, sino una persona que hace las preguntas correctas.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#un-primer-vistazo-a-los-datos",
    "href": "tema1.html#un-primer-vistazo-a-los-datos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.3 Un primer vistazo a los datos",
    "text": "1.3 Un primer vistazo a los datos\nEn este capítulo vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo final, que abordaremos en cursos posteriores, es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). En este curso nos conformamos con adquirir conocimiento de los datos, planteando algunas hipótesis de interés.\nLas variables que debemos estudiar son:\nVariables de entrada:\n\nDatos del cliente bancario:\n\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n\nOtros atributos\n\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n\nVariable de salida (objetivo deseado):\n\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente. Los datos suelen ser más complejos que la teoría detrás de los datos.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla! Si no conocemos el significado de una variable, difícilmente podremos interpretar los resultados asociados a ella.\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#tipo-de-variables",
    "href": "tema1.html#tipo-de-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Tipo de variables",
    "text": "1.4 Tipo de variables\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\nCreamos las particiones sobre los datos y trabajamos sobre la partición de entrenamiento. Este paso cobrará significado en asignaturas posteriores, especialmente en las enfocadas en el aprendizaje automático (Machine Learning). Sin entrar en más detalles, podemos decir que la idea fundamental es estudiar las hipótesis en un subconjunto de la muestra disponible y evaluar la veracidad (o no) de dichas hipótesis en muestras diferentes.\n\n# Parciticionamos los datos\nset.seed(2138)\nn=dim(bank)[1]\nindices=seq(1:n)\nindices.train=sample(indices,size=n*.5,replace=FALSE)\nindices.test=sample(indices[-indices.train],size=n*.25,replace=FALSE)\nindices.valid=indices[-c(indices.train,indices.test)]\n\nbank.train=bank[indices.train,]\nbank.test=bank[indices.test,]\nbank.valid=bank[indices.valid,]\n\n\n\n\n\n\n\nAtrévete\n\n\n\n¿Te has hecho (ya) alguna pregunta sobre los datos? Si es así, no esperes más, busca la respuesta!\n\n\nPor ejemplo, ¿qué te parecen estas preguntas que nosotros proponemos?\n¿Qué día del año se producen más depósitos por parte de los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\") %&gt;% \n  count(month, day) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 apr      30    79\n\n\n¿En qué mes del año realizan más depósitos los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\" & job==\"student\") %&gt;% \n  count(month) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 2\n  month     n\n  &lt;chr&gt; &lt;int&gt;\n1 apr      21\n\n\n¿Qué trabajo está asociado con el mayor porcentaje de depósitos?\n\n\nClick para ver el código\nbank.train %&gt;%\n  group_by(job) %&gt;%\n  mutate(d = n()) %&gt;%\n  group_by(job, deposit) %&gt;%\n  summarise(Perc = n()/first(d), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    id_cols = job,\n    names_from = deposit,\n    values_from = Perc\n  ) %&gt;%\n  top_n(1)\n\n\n# A tibble: 1 × 3\n  job        no   yes\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 student 0.218 0.782\n\n\n\n\n\n\n\n\nRepaso\n\n\n\ndplyr es un paquete en R diseñado para facilitar la manipulación y transformación de datos de manera eficiente y estructurada. Fue desarrollado por Hadley Wickham y se ha convertido en una de las herramientas más populares en la ciencia de datos y análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#variable-objetivo",
    "href": "tema1.html#variable-objetivo",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.5 Variable objetivo",
    "text": "1.5 Variable objetivo\nEn el ámbito del Aprendizaje Automático, en problemas de clasificación (Aprendizaje Supervisado) existe una variable de interés fundamental, es la variable respuesta o variable objetivo. En el próximo cuatrimestre se trata con detalle este tipo de problemas. En el caso que nos ocupa dicha variable es la característica: “deposit”. Vamos a estudiar la información que nos proporciona dicha variable.\n\nlibrary(ggplot2)\ntable(bank.train$deposit)\n\n\n  no  yes \n2908 2673 \n\nggplot(data=bank.train,aes(x=deposit,fill=deposit)) +\n  geom_bar(aes(y=(..count..)/sum(..count..))) +\n  scale_y_continuous(labels=scales::percent) +\n  theme(legend.position=\"none\") +\n  ylab(\"Frecuencia relativa\") +\n  xlab(\"Variable respuesta: deposit\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#visualizar-distribuciones",
    "href": "tema1.html#visualizar-distribuciones",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.6 Visualizar distribuciones",
    "text": "1.6 Visualizar distribuciones\nLa forma de visualizar la distribución de una variable dependerá de si la variable es categórica o continua. Una variable es categórica si sólo puede tomar uno de un pequeño conjunto de valores. En R, las variables categóricas suelen guardarse como factores o vectores de caracteres. Para examinar la distribución de una variable categórica, utiliza un gráfico de barras:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = contact))\n\n\n\n\n\n\n\n\nPuedes obtener los valores exactos en cada categoría como sigue:\n\nbank.train%&gt;% \n  count(contact)\n\n# A tibble: 3 × 2\n  contact       n\n  &lt;chr&gt;     &lt;int&gt;\n1 cellular   4025\n2 telephone   383\n3 unknown    1173\n\n\nUna variable es continua si puede tomar cualquiera de un conjunto infinito de valores ordenados. Para examinar la distribución de una variable continua, utiliza un histograma:\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = age), binwidth = 5)\n\n\n\n\n\n\n\n\nUn histograma divide el eje \\(x\\) en intervalos equidistantes y, a continuación, utiliza la altura de una barra para mostrar el número de observaciones que se encuentran en cada intervalo. En el gráfico anterior, la primera barra muestra unas \\(100\\) observaciones (realmente son \\(119\\)) tienen un valor de edad por debajo de \\(22.5\\) años. Puede establecer la anchura de los intervalos en un histograma con el argumento binwidth, que se mide en las unidades de la variable \\(x\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nSiempre se deben explorar una variedad de anchos de intervalo cuando trabajamos con histogramas, ya que diferentes anchos de intervalo pueden revelar diferentes patrones.\n\n\nPodemos representar funciones de densidad de probabilidad.\n\nggplot(bank.train, aes(x = age)) +\ngeom_density() +\nggtitle('KDE de edad en datos bank')\n\n\n\n\n\n\n\n\nOtro gráfico muy utilizado para variables cuantitativas univariantes es el boxplot, también llamado box-and-whisker plot (diagrama de caja y bigotes). Es especialmente útil para detectar posibles datos atípicos en los valores de una variable, siempre que su distribución sea parecida a una distribución Normal. El gráfico muestra:\n\nUna caja cuyos límites son el primer y el tercel cuartil de la distribución de valores.\nUna línea central, que marca la mediana.\nLos bigotes, que por defecto (en R) se extienden hasta 1.5 veces el valor del rango intercuartílico (IQR) por encima y por debajo de la caja.\nPuntos individuales, que quedan más allá del límite de los bigotes, marcan posibles datos atípicos.\n\nEn distribuciones muy asimétricas o con muchos valores extremos, muy diferentes a una distribución Normal, aparecerán demasiados puntos más allá de los bigotes y no se podrán apreciar fácilmente los atípicos (demasiados puntos considerados como tales). En ese caso, es conveniente intentar una transformación de la variable antes de representar el boxplot.\n\nggplot(bank.train, aes(x=deposit, y=age, color=deposit)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#transformación-de-variables",
    "href": "tema1.html#transformación-de-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.7 Transformación de variables",
    "text": "1.7 Transformación de variables\n\n1.7.1 Transformación de variables cuantitativas\nEn algunos métodos de inferencia estadística y aprendizaje automático será necesario contar con variables que cumplan requisitos de normalidad. Por ejemplo, si tomamos la transformación \\(log\\) sobre la variable edad obtenemos una distribución multimodal que, probablemente, corresponda a la combinación de dos (o más) normales.\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = log(age)), binwidth = .1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nLos modelos de aprendizaje serán tan buenos como lo sean las variables de entrada de dichos algoritmos.\n\n\n\n1.7.1.1 Transformaciones para igualar dispersión\nCon frecuencia, el objetivo de la transformación de variables cuantitativas es obtener una variable cuya distribución de valores sea:\n\nMás simétrica y con menor dispersión que la original.\nMás semejante a una distribución normal (e.g. para algunos modelos lineales).\nRestringida en un intervalo de valores (e.g. \\([0,1]\\) ).\n\nLa forma más sencilla de detectar que alguna de nuestras variables necesita ser transformada es representar un gráfico que muestre la distribución de valores de la variable. Por ejemplo, un histograma o un diagrama de densidad de probabilidad (o ambos).\nEl uso de los logaritmos tiene su propia recomendación en preparación de datos (Fox y Weisberg 2018):\n\n\n\n\n\n\nJohn Fox\n\n\n\n“Si la variable es estrictamente positiva, no tiene un límite superior para sus valores, y su rango abarca dos o más órdenes de magnitud (potencias de \\(10\\)), entonces la transformación logarítmica suele ser útil. A la inversa, cuando la variable tiene un rango de valores pequeño (menor de un orden de magnitud), el logaritmo o cualquier otra transformación simple no ayudará mucho.”\n\n\nLa versión general de esta transformación son las transformaciones de escala-potencia (scaled-power transformations), también denominadas transformaciones de Box-Cox.\n\\[x(\\lambda)= \\begin{cases} \\frac{x^\\lambda-1}{\\lambda},& \\text{cuando } \\lambda \\neq 0,\\\\ log_e(x), & \\text{cuando } \\lambda = 0 \\end{cases}\\]\nLa función car::symbox(...) permite probar varias combinaciones típicas del parámetro \\(\\lambda\\) , para comprobar con cuál de ellas obtenemos una distribución más simétrica de valores.\n\nlibrary(car)\n\nbank.train %&gt;% symbox(~ age, data = .)\n\n\n\n\n\n\n\n\n\n\n1.7.1.2 Transformaciones para igualar dispersión\nTambién es bastante común aplicar transformaciones en datos cuantitativos para igualar las escalas de representación de las variables. En muchos modelos, si una de nuestras variables tiene una escala mucho mayor que las demás, sus valores tienden a predominar en los resultados, enmascarando la influencia del resto de variables en el modelo.\nPor este motivo, en muchos modelos es importante garantizar que todas las variables se representan en escalas comparables, de forma que ninguna predomine sobre el resto. Conviene aclarar un poco algunos términos que se suelen emplear de forma indistinta:\n\nReescalado o cambio de escala: Consiste en sumar o restar una constante a un vector, y luego multiplicar o dividir por una constante. Por ejemplo, para transformar la unidad de medida de una variable (grados Farenheit → grados Celsius).\nNormalización: Consiste en dividir por la norma de un vector, por ejemplo para hacer su distancia euclídea igual a \\(1\\).\nEstandarización: Consiste en restar a un vector una medida de localización o nivel (e.g. media, mediana) y dividir por una medida de escala (dispersión). Por ejemplo, si restamos la media y dividimos por la desviación típica hacemos que la distribución tenga media \\(0\\) y desviación típica \\(1\\).\n\nAlgunas aternativas comunes son:\n\\[\nEstandarización \\rightarrow Y=\\frac{X-\\overline{x}}{s_x}\n\\]\n\\[\nEscalado \\space min-max \\rightarrow Y=\\frac{X-min_x}{max_x-min_x}\n\\]\nEn R, la función scale() se puede utilizar para realizar estas operaciones de estandarización. Automáticamente, puede actuar sobre las columnas de un data.frame, aplicando la misma operación a todas ellas (siempre que todas sean cuantitativas).\n\n\n\n1.7.2 Transformación de variables cualitativas\nA diferencia de las variables cuantitativas, que representan cantidades numéricas, las variables cualitativas, también conocidas como variables categóricas, se utilizan para describir características o cualidades que no tienen un valor numérico intrínseco. Las variables cualitativas son esenciales en la investigación y el análisis de datos, ya que a menudo se utilizan para clasificar, segmentar y comprender información sobre grupos, categorías o características. Algunas técnicas comunes para analizar variables cualitativas incluyen la creación de tablas de frecuencia para contar la ocurrencia de cada categoría y el uso de gráficos como gráficos de barras o diagramas de sectores para visualizar la distribución de categorías. Estos análisis pueden proporcionar información valiosa sobre patrones, tendencias y relaciones en los datos cualitativos, lo que puede ser fundamental para tomar decisiones informadas en una amplia gama de campos, desde marketing hasta investigación social y más.\nLas variables cualitativas se dividen en dos categorías principales:\n\n\n\n\n\n\nVariables Cualitativas Nominales\n\n\n\n\n\nLas variables nominales representan categorías o etiquetas que no tienen un orden inherente. Ejemplos comunes incluyen el género (masculino, femenino, otro), el estado civil (soltero, casado, divorciado) o los colores (rojo, azul, verde). No se pueden realizar operaciones matemáticas en variables nominales, como sumar o restar.\n\n\n\n\n\n\n\n\n\nVariables Cualitativas Ordinales\n\n\n\n\n\nLas variables ordinales representan categorías con un orden natural o jerarquía, pero la distancia entre las categorías no es necesariamente uniforme ni conocida. Ejemplos incluyen la calificación de satisfacción del cliente (muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho) o el nivel de educación (primaria, secundaria, universitaria). Aunque se pueden establecer comparaciones de orden (por ejemplo, “mayor que” o “menor que”), no es apropiado realizar operaciones matemáticas en variables ordinales.\n\n\n\nEn R, las variables categóricas se denominan factores (factors) y sus categorías niveles (levels). Es importante procesarlos adecuadamente para que los modelos aprovechen la información que contienen estas variables. Por otro lado, si se codifica incorrectamente esta información los modelos pueden estar realizando operaciones absurdas aunque nos devuelvan resultados aparentemente válidos.\n\n\n\n\n\n\nR\n\n\n\nPor defecto, R transforma columnas tipo string en factores al leer los datos de un archivo. Además, por defecto, R ordena los niveles de los factores alfabéticamente, según sus etiquetas. Debemos tener cuidado con esto, puesto que en muchos análisis es muy importante saber qué nivel se está tomando como referencia, de entre los valores posibles de un factor, para comparar con los restantes. En ciertos modelos, la elección como referencia de uno de los valores del factor (típicamente el primero que aparece en la lista de niveles) cambia por completo los resultados, así como la interpretación de los mismos.\n\n\nEn variables ordinales se debe respetar estrictamente el orden preestablecido de los niveles. Por ejemplo, una ordenación (“regular” &lt; “bueno” &lt; “malo”) es inaceptable. Para establecer una ordenación explícita entre los niveles hay que especificarla manualmente si no coincide con la alfabética, y además configurar el argumento ordered = TRUE en la función factor():\n\nsatisfaccion &lt;- rep(c(\"malo\", \"bueno\", \"regular\"), c(3,3,3))\nsatisfaccion &lt;- factor(satisfaccion, ordered = TRUE, levels = c(\"malo\", \"regular\", \"bueno\"))\nsatisfaccion\n\n[1] malo    malo    malo    bueno   bueno   bueno   regular regular regular\nLevels: malo &lt; regular &lt; bueno\n\n\nPara comprobar qué nivel se toma como referencia en cada uno de los factores de una base de datos usamos la funión levels():\n\nlevels(bank.train$marital)\n\nNULL\n\n\nY esto, ¿es correcto? Veamos la distribución de las observaciones en las categorías de la variable marital:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = marital))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\n\nHabitualmente será más recomendable elegir como categoría de referencia para variables categóricas aquella categoría con mayor número de observaciones.\n\n\nPor tanto, en este caso particular deberíamos modificar la categoría de referencia como sigue:\n\nreorder_marital = factor(bank.train$marital, levels=(c(' married', ' single', ' divorced')))\nlevels(reorder_marital)\n\n[1] \" married\"  \" single\"   \" divorced\"\n\n\nNótese que la nueva variable aquí creada, reorder_marital, no ha sido incluida (aún) en el tibble bank. Para ello:\n\nbank.train$marital = reorder_marital\n\n\n1.7.2.1 Conversión de variables cuantitativas a variables categóricas\nLa conversión de variables cuantitativas a variables categóricas es un proceso importante en EDA que implica transformar datos numéricos en categorías. Esto se realiza con el propósito de simplificar el análisis, resaltar patrones específicos y facilitar la interpretación de los resultados. A continuación, se destacan algunas situaciones comunes en las que se realiza esta conversión y cómo se lleva a cabo:\n\nAgrupación de datos numéricos: En ocasiones, es útil agrupar datos numéricos en intervalos o categorías para resaltar tendencias generales. Por ejemplo, en un estudio de edades de una población, en lugar de analizar cada edad individual, se pueden crear grupos como “menos de 18 años”, “18-30 años”, “31-45 años” y así sucesivamente.\nCreación de variables binarias: A menudo, se convierten variables numéricas en variables binarias (\\(1\\) o \\(0\\)) para simplificar el análisis. Por ejemplo, en un estudio de satisfacción del cliente, se puede crear una variable binaria donde “\\(1\\)” indica clientes satisfechos y “\\(0\\)” indica clientes insatisfechos.\nCategorización de variables continuas: Las variables continuas, como ingresos o puntuaciones, se pueden convertir en categorías para segmentar la población. Esto puede ser útil en análisis demográficos o de segmentación de mercado.\nSimplificación de modelos: Algunos modelos de ML pueden beneficiarse de la conversión de variables cuantitativas a categóricas para mejorar la interpretación y la eficacia del modelo.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEl proceso de conversión de variables cuantitativas a categóricas generalmente implica definir criterios o reglas claras para agrupar los valores numéricos en categorías significativas. Estos criterios pueden basarse en conocimiento previo del dominio, EDA o consideraciones específicas del problema. En esta etapa te vendrá genial contar con la ayuda de un experto en el dominio de aplicación, y puedes llevar a cabo cambios catastróficos en caso de no contar con esa ayuda.\n\n\nEs importante tener en cuenta que la conversión de variables cuantitativas a categóricas debe realizarse de manera cuidadosa y considerar el impacto en el análisis. La elección de cómo categorizar los datos debe estar respaldada por una comprensión sólida del problema y los objetivos del estudio. Además, se debe documentar claramente el proceso de conversión para que otros puedan replicarlo y comprender las categorías resultantes.\nA modo de ejemplo, vamos a categorizar la varible age en la base de datos bank. Para ello elegimos (elegimos!!!) las siguientes agrupaciones en la variable edad: (0,40],(40,60],(60,100].\n\n bank.train &lt;- within(bank.train, {   \n  age.cat &lt;- NA # need to initialize variable\n  age.cat[age &lt;= 40] &lt;- \"Low\"\n  age.cat[age &gt; 40 & age &lt;= 60] &lt;- \"Middle\"\n  age.cat[age &gt; 60] &lt;- \"High\"\n   } )\n\nbank.train$age.cat &lt;- factor(bank.train$age.cat, levels = c(\"Low\", \"Middle\", \"High\"))\nsummary(bank.train$age.cat)\n\n   Low Middle   High \n  3116   2151    314",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#valores-comunes-y-atípicos",
    "href": "tema1.html#valores-comunes-y-atípicos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.8 Valores comunes y atípicos",
    "text": "1.8 Valores comunes y atípicos\nLos gráficos de barras relacionados con variables cualitativas nos han ayudado a identificar los valores más frecuentes o las categorías más repetidas en esas variables. Estos gráficos reflejan la frecuencia de cada categoría, es decir, el número de veces que aparece en el conjunto de datos. A veces ese número se representa en porcentaje respecto al número total de observaciones, proporcionando una visión relativa de la prevalencia en cada categoría. La moda es la categoría que aparece con mayor frecuencia en el conjunto de datos. Es especialmente útil para identificar la categoría más común y es aplicable a variables categóricas.\nEn el caso de las variables cuantitativas, el histograma de frecuencias se convierte en una herramienta gráfica sumamente útil para alcanzar este mismo objetivo.\n\n1.8.1 Estadísticos resumen\nA continuación te explicamos algunas medidas que resumen el comportamiento de una variable aleatoria cuantitativa:\n\n\n\n\n\n\nMedia\n\n\n\n\n\nLa media aritmética es el promedio de todos los valores de la variable. Se calcula sumando todos los valores y dividiendo por el número de observaciones. La media proporciona una indicación de la tendencia central de los datos.\n\n\n\n\n\n\n\n\n\nMediana\n\n\n\n\n\nLa mediana es el valor central en un conjunto de datos ordenados en forma ascendente o descendente. Divide el conjunto de datos en dos mitades iguales. La mediana es menos sensible a valores extremos que la media y es especialmente útil cuando los datos no siguen una distribución (aproximadamente) normal.\n\n\n\n\n\n\n\n\n\nModa\n\n\n\n\n\nLa moda es el valor que ocurre con mayor frecuencia en un conjunto de datos. Puede haber una o más modas en un conjunto de datos, y esta medida es especialmente útil para variables discretas.\n\n\n\n\n\n\n\n\n\nRango\n\n\n\n\n\nEl rango es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Proporciona una indicación de la dispersión o variabilidad de los datos.\n\n\n\n\n\n\n\n\n\nDesviación Estándar\n\n\n\n\n\nLa desviación estándar mide la dispersión de los datos con respecto a la media, y tiene sus mismas unidades de medida. Valores más altos indican mayor variabilidad. Es especialmente útil cuando se asume una distribución normal.\n\n\n\n\n\n\n\n\n\nCuartiles y Percentiles\n\n\n\n\n\nLos cuartiles dividen un conjunto de datos en cuatro partes iguales, mientras que los percentiles dividen los datos en cien partes iguales. Los cuartiles y percentiles son útiles para identificar valores atípicos y comprender la distribución de los datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de Variación\n\n\n\n\n\nEl coeficiente de variación es una medida de la variabilidad relativa de los datos y se calcula como la desviación estándar dividida por la media. Se expresa como un porcentaje y es útil para comparar la variabilidad entre diferentes conjuntos de datos.\n\n\n\nEn R, podemos obtener algunos estadísticos resumen mediante la opción summary.\n\nsummary(bank.train$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   32.00   39.00   41.24   49.00   95.00 \n\n\nCuriosamente, R no tiene una función estándar incorporada para calcular la moda. Así que creamos una función de usuario para calcular la moda de un conjunto de datos en R. Esta función toma el vector como entrada y da el valor de la moda como salida.\n\n# Create the function.\nsummary_moda &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nsummary_moda(bank.train$age)\n\n[1] 31\n\n\n\n\n1.8.2 Valores atípicos\nLos valores atípicos (outliers en inglés) son observaciones inusuales, puntos de datos que no parecen encajar en el patrón o el rango de la variable estudiada. A veces, los valores atípicos son errores de introducción de datos; otras veces, sugieren nuevos datos científicos importantes.\nCuando es posible, es una buena práctica llevar a cabo el análisis con y sin los valores atípicos. Si se determina que su influencia en los resultados es insignificante y no se puede identificar su origen, puede ser razonable reemplazarlos con valores faltantes y continuar con el análisis. Sin embargo, si estos valores atípicos tienen un impacto sustancial en los resultados, no se deben eliminar sin una justificación adecuada. En este caso, será necesario investigar la causa subyacente (por ejemplo, un error en la entrada de datos) y documentar su exclusión en el informe correspondiente.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#valores-faltantes",
    "href": "tema1.html#valores-faltantes",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.9 Valores faltantes",
    "text": "1.9 Valores faltantes\nLos valores faltantes (missing), también conocidos como valores nulos o valores ausentes, son observaciones o datos que no están disponibles o que no han sido registrados para una o más variables en un conjunto de datos. Estos valores pueden surgir por diversas razones, como errores de entrada de datos, respuestas incompletas en una encuesta, fallos en la medición o simplemente porque cierta información no está disponible en un momento dado.\n\n\n\n\n\n\nPara recordar\n\n\n\nLa presencia de valores faltantes en un conjunto de datos es un problema común en el análisis de datos y puede tener un impacto significativo en la calidad de los resultados. Es importante abordar adecuadamente los valores faltantes, ya que pueden sesgar los análisis y conducir a conclusiones incorrectas si no se manejan correctamente.\n\n\nAlgunas de las estrategias comunes para tratar los valores faltantes incluyen:\n\nEliminación de filas o columnas: Si la cantidad de valores faltantes es pequeña en comparación con el tamaño total del conjunto de datos, una opción es eliminar las filas o columnas que contengan valores faltantes. Sin embargo, esta estrategia puede llevar a la pérdida de información importante.\nImputación de valores: Esta estrategia implica estimar o llenar los valores faltantes con valores calculados a partir de otros datos disponibles. Esto puede hacerse utilizando técnicas como la imputación media (rellenar con la media de la variable), imputación mediana (rellenar con la mediana), imputación de vecinos más cercanos o técnicas más avanzadas como regresión u otras técnicas de modelado.\nMarcadores especiales: En algunos casos, es útil asignar un valor específico (como “N/A” o “-999”) para indicar que un valor está ausente. Esto puede ser útil cuando se desea mantener un registro explícito de los valores faltantes sin eliminarlos o imputarlos. Es importante que, en este caso, el valor asignado no tenga otro significado. Por ejemplo, asignamos “-999” como marcador de valor faltante y sin embargo, es un valor plausible dentro del rango de valores de la variable.\nMétodos basados en modelos: Utilizar modelos estadísticos o de aprendizaje automático para predecir los valores faltantes en función de otras variables disponibles. Esto puede ser especialmente eficaz cuando los datos faltantes siguen un patrón que puede ser capturado por el modelo.\n\nLa elección de la estrategia adecuada para tratar los valores faltantes depende del contexto del análisis, la cantidad de datos faltantes y la naturaleza de los datos. Es fundamental abordar este problema de manera cuidadosa y transparente, documentando cualquier procedimiento de imputación o tratamiento de valores faltantes utilizado en el análisis para garantizar la integridad y la validez de los resultados.\n\n\n\n\n\n\nPeligro\n\n\n\nSustituir valores faltantes por otros obtenidos con técnicas y métodos estadísticos o de aprendizaje automático siempre es un riesgo, pues implica “inventar” datos allá donde no los hay.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#correlación-entre-variables",
    "href": "tema1.html#correlación-entre-variables",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.10 Correlación entre variables",
    "text": "1.10 Correlación entre variables\nExisten varios métodos y técnicas para estudiar la correlación entre variables, lo que ayuda a comprender las relaciones entre las diferentes características en un conjunto de datos. En próximos cursos estudiarás que es de especial interés estudiar las relaciones entre la variable objetivo y las variables explicativas.\nPuedes desplegar los paneles siguientes para averiguar alguno de los métodos más comunes.\n\n\n\n\n\n\nMatriz de correlación\n\n\n\n\n\nLa matriz de correlación es una tabla que muestra las correlaciones entre todas las combinaciones de variables en un conjunto de datos. Los valores de correlación varían entre \\(-1\\) y \\(1\\), donde \\(-1\\) indica una correlación negativa perfecta, \\(1\\) indica una correlación positiva perfecta y \\(0\\) indica la ausencia de correlación. Este método es especialmente útil para identificar relaciones lineales entre variables numéricas.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión\n\n\n\n\n\nLos gráficos de dispersión muestran la relación entre dos variables numéricas mediante puntos en un plano cartesiano. Estos gráficos permiten visualizar patrones de dispersión y tendencias entre las variables. Si los puntos se agrupan en una forma lineal, indica una posible correlación lineal.\n\n\n\n\n\n\n\n\n\nMapas de calor\n\n\n\n\n\nLos mapas de calor son representaciones visuales de la matriz de correlación en forma de un gráfico de colores. Permiten identificar rápidamente las relaciones fuertes o débiles entre variables y son útiles para resaltar patrones en grandes conjuntos de datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Pearson\n\n\n\n\n\nEste coeficiente mide la correlación lineal entre dos variables numéricas. Varía entre \\(-1\\) y \\(+1\\), donde valores cercanos a \\(-1\\) o \\(+1\\) indican una correlación fuerte, mientras que valores cercanos a \\(0\\) indican una correlación débil o nula.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Spearman\n\n\n\n\n\nEste coeficiente evalúa la correlación monotónica entre dos variables, lo que significa que puede detectar relaciones no lineales. Es útil cuando las variables no siguen una distribución normal.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Kendall\n\n\n\n\n\nSimilar al coeficiente de Spearman, evalúa la correlación entre variables, pero se centra en la concordancia de los rangos de datos, lo que lo hace útil para datos no paramétricos y muestras pequeñas.\n\n\n\n\n\n\n\n\n\nPruebas estadísticas\n\n\n\n\n\nLas pruebas estadísticas, como la prueba t de Student o la ANOVA, pueden utilizarse para evaluar si existe una diferencia significativa en los promedios de una variable entre diferentes categorías de otra variable. Si la diferencia es significativa, puede indicar una correlación entre las variables.\n\n\n\nVamos a estudiar la relación existente entre la variable objetivo deposit y la variable duration de la base de datos bank.\n\nggplot(bank.train, aes(x = log(duration), colour = deposit)) +\n  geom_density(lwd=2, linetype=1)\n\n\n\n\n\n\n\n\nPuede observarse una relación. Valores altos de la variable duración parecen estar relacionados con observaciones con deposit igual a ‘yes’.\n\ndf = bank.train %&gt;% \n      select(duration,deposit)%&gt;%\n      mutate(log.duration=log(duration))\n\n# Resumen para los casos de depósito\nsummary(df %&gt;% filter(deposit==\"yes\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.079   5.497   6.073   6.046   6.593   8.087 \n\n# Resumen para los casos de no depósito\nsummary(df %&gt;% filter(deposit==\"no\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6931  4.5433  5.0999  5.0308  5.6276  7.5022 \n\n\nGráficamente, podemos comparar los boxplots.\n\nggplot(df, aes(deposit, log.duration)) +\n        geom_boxplot()\n\n\n\n\n\n\n\n\nPodemos determinar la importancia de relación. Por ejemplo, podemos realizar un test de la T para igualdad de medias. Estudiaremos estos conceptos en el ?sec-para.\n\nt.test(log.duration ~ deposit, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  log.duration by deposit\nt = -45.828, df = 5464.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -1.0583835 -0.9715488\nsample estimates:\n mean in group no mean in group yes \n         5.030821          6.045787 \n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nComprenderás este resultado a lo largo del curso. De momento, puedes preguntar al profesor. Dejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\nEs posible estudiar la relación entre dos variables categóricas de manera gráfica.\n\nggplot(data = bank.train, aes(x = housing, fill = deposit)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nParece haber una relación, estando asociados las observaciones de personas con casa propia a un mayor porcentaje de `no’ en la variable respuesta. Podemos obtener la tabla de contingencia:\n\ndata1=table(bank.train$housing, bank.train$deposit)\n\n\ndimnames(data1) &lt;- list(housing = c(\"no\", \"yes\"),\n                        deposit = c(\"no\", \"yes\"))\ndata1\n\n       deposit\nhousing   no  yes\n    no  1246 1688\n    yes 1662  985\n\n\nY el contraste correspondiente para la hipótesis nula de no existencia de relación. Estudiaremos estos conceptos en el ?sec-para.\n\nchisq.test(bank.train$housing, bank.train$deposit)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bank.train$housing and bank.train$deposit\nX-squared = 229.44, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\n\n\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#modelización-estadística",
    "href": "tema1.html#modelización-estadística",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "",
    "text": "1.1.1 Contextualización del problema\nEl primer paso en la modelización estadística es definir el problema que se busca analizar. Esto incluye identificar el contexto, establecer objetivos claros y determinar las variables involucradas.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\nProblema: Investigar si existe una relación positiva entre el tiempo dedicado al estudio semanal y el promedio de calificaciones de los estudiantes universitarios.\n\nVariables:\n\nVariable explicativa (independiente): Tiempo de estudio semanal (en horas).\n\nVariable respuesta (dependiente): Promedio de calificaciones al final del semestre (en una escala de 0 a 10).\n\n\nObjetivo: Determinar si los estudiantes que dedican más horas al estudio semanalmente obtienen mejores calificaciones en promedio.\n\nEmplearemos datos simulados para este ejemplo:\n\n# Establecer la semilla para reproducibilidad\nset.seed(123)\n\n# Número de estudiantes\nn &lt;- 100\n\n# Generar tiempo de estudio (en horas) como una variable independiente\ntiempo_estudio &lt;- round(runif(n, min = 5, max = 40), 1)\n\n# Relación lineal entre tiempo de estudio y calificaciones (con ruido)\nbeta_0 &lt;- 5  # Intercepto\nbeta_1 &lt;- 0.1  # Pendiente (efecto del tiempo de estudio)\nsigma &lt;- 0.5  # Varianza del ruido\n\n# Esta será la relación que deseamos \"descubrir\".\ncalificaciones &lt;- round(beta_0 + beta_1 * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)\n\n# Crear un data frame con los datos generados\ndatos &lt;- data.frame(Tiempo_Estudio = tiempo_estudio, Calificaciones = calificaciones)\n\n# Visualizar los primeros registros\nhead(datos)\n\n  Tiempo_Estudio Calificaciones\n1           15.1           6.64\n2           32.6           8.25\n3           19.3           6.91\n4           35.9           9.27\n5           37.9           8.68\n6            6.6           6.42\n\n\n\n\n\n\n\n1.1.2 Inspección gráfica e identificación de tendencias\nAntes de ajustar un modelo, es esencial realizar una inspección visual de los datos. Los gráficos de dispersión son una herramienta útil para observar tendencias, relaciones o patrones entre las variables.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn el ejemplo, representamos el tiempo de estudio frente a las calificaciones. De este modo podemos analizar si los puntos siguen un patrón lineal o si muestran comportamientos más complejos.\n\n# Graficar los datos de estudio generados anteriormente\nplot(datos$Tiempo_Estudio, datos$Calificaciones,\n     main = \"Relación entre Tiempo de Estudio y Calificaciones\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Calificaciones (promedio)\",\n     pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.3 Propuesta y ajuste del modelo\nCon base en las observaciones previas, se propone un modelo estadístico que relacione las variables. En el caso de relaciones lineales, la ecuación típica es:\n\\[\n\\text{Respuesta} = \\beta_0 + \\beta_1 (\\text{Variable explicativa}) + \\varepsilon,\n\\]\ndonde \\(\\beta_0\\) es la constante (o intercepto), \\(\\beta_1\\) es la pendiente, y \\(\\varepsilon\\) es el término de error aleatorio.\nFíjate que la expresión anterior concuerda con el siguiente principio fundamental del análisis de datos:\n\\[DATOS = MODELO + ERROR\\]\n\nLos datos representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.\nEl modelo es una representación simplificada de la realidad que proponemos para describirla e interpretarla más fácilmente.\nEl error refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\nPropuesta del modelo:\n\\[\n\\text{Calificaciones} = \\beta_0 + \\beta_1 (\\text{Tiempo de Estudio}).\n\\]\nAjuste: Calcular los valores de $ _0 $ y $ _1 $ mediante el método de mínimos cuadrados. Este método busca minimizar la suma de los errores cuadrados entre los valores observados y los predichos por el modelo.\n\n\n# Ajustar un modelo de regresión lineal\nmodelo &lt;- lm(Calificaciones ~ Tiempo_Estudio, data = datos)\nsummary(modelo)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\nTiempo_Estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Graficar los datos de estudio y su recta de regresión lineal\nplot(datos$Tiempo_Estudio, datos$Calificaciones,\n     main = \"Relación entre Tiempo de Estudio y Calificaciones\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Calificaciones (promedio)\",\n     pch = 19, col = \"blue\")\nabline(lm(Calificaciones ~ Tiempo_Estudio, data = datos), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nA la vista de los estimadores de los parámetros del modelo, tenemos la siguiente interpretación:\n\nSe produce un incremento de la calificiación de \\(0.1\\) (aproximadamente) por cada incremento de \\(1\\) hora en el tiempo de estudio. O bien, por cada \\(10\\) horas de estudio, se incrementa la nota en \\(1\\) punto.\nEn ocasiones es complicado interpretar el valor de \\(\\beta_0\\). En este caso, corresponde a la calificación de los estudiantes que no dedican ningún tiempo de estudio. En este caso sería \\(5\\) (aproximadamente).\n\n\n\n\n\n\n1.1.4 Revisión y diagnóstico del modelo\nUna vez ajustado el modelo, es crucial evaluar su calidad. Esto implica analizar si los supuestos del modelo se cumplen, como la linealidad, la homocedasticidad y la independencia de los errores.\nDeberemos realizar las siguientes tareas:\n\nComparar los valores predichos por el modelo con los datos observados para verificar su ajuste.\nExaminar los residuos (errores) para identificar posibles problemas, como tendencias no capturadas o varianzas no constantes.\n\n\n\n1.1.5 Reajuste del modelo\nSi el diagnóstico revela deficiencias en el modelo ajustado, se plantean modificaciones. Estas pueden incluir:\n\nTransformaciones de variables para mejorar la linealidad.\nIntroducción de términos adicionales, como variables cuadráticas o interacciones.\nCambios a modelos no lineales si el comportamiento de los datos lo requiere.\n\n\n\n\n\n\n\nEstudios de Galton sobre estatura\n\n\n\nLos estudios de Sir Francis Galton sobre la estatura son un ejemplo clásico en estadística y forman parte de la historia de la regresión lineal. Galton, un polímata británico del siglo XIX, investigó la herencia biológica y publicó en 1889 su libro Natural Inheritance, donde analizó datos sobre la relación entre las estaturas de padres e hijos. Estos estudios no solo sentaron las bases para la regresión lineal, sino que también ayudaron a formalizar conceptos clave en estadística, haciendo de Galton una figura central en su desarrollo.\nContexto y propósito\nGalton estaba interesado en cómo las características físicas, como la estatura, se transmiten de padres a hijos. Su objetivo era cuantificar esta relación y establecer patrones de herencia. En particular, buscó responder si los hijos de padres altos tienden a ser más altos y si los de padres bajos tienden a ser más bajos.\nDatos recopilados\n\nGalton recopiló datos sobre las estaturas de 928 hijos y sus respectivos padres.\nLas medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).\n\nEn sus análisis, utilizó el promedio de las estaturas de ambos padres, conocido como estatura media parental, para compararlo con la estatura de los hijos.\n\nPrincipales hallazgos\n\nRelación lineal entre padres e hijos:\nGalton observó que existe una relación positiva entre la estatura de los padres y la de los hijos. Los padres altos tienden a tener hijos altos, y los padres bajos tienden a tener hijos bajos. Esta relación puede modelarse con una línea recta, lo que inspiró la formulación de la regresión lineal.\nRegresión a la media:\n\nAunque los hijos de padres altos son, en promedio, más altos que el promedio general de la población, también tienden a ser menos altos que sus padres.\n\nDe manera similar, los hijos de padres bajos son más bajos que el promedio general, pero suelen ser menos bajos que sus padres.\n\nEste fenómeno, que Galton llamó “regresión a la media”, ocurre porque las características extremas tienden a suavizarse en la siguiente generación debido a la influencia de múltiples factores genéticos y ambientales.\n\nEcuación de la recta de regresión:\nGalton ajustó una recta para describir la relación entre la estatura media parental (\\(X\\)) y la estatura de los hijos (\\(Y\\)): \\[\nY = \\beta_0 + \\beta_1 X\n\\] Donde:\n\n\\(\\beta_0\\): Intercepto, representa la estatura promedio de los hijos cuando la estatura parental es promedio.\n\\(\\beta_1\\): Pendiente, indica cómo cambia la estatura de los hijos por cada unidad de cambio en la estatura media parental.\n\n\nImportancia en la Estadística\n\nRegresión lineal:\nEste estudio introdujo el concepto de recta de regresión, que describe cómo varía la media de una variable dependiente en función de una variable independiente.\nCorrelación:\nGalton también estudió el grado de relación entre variables, precursor del concepto de coeficiente de correlación desarrollado posteriormente por Karl Pearson, un discípulo suyo.\nRegresión a la media:\nEl término y la idea detrás de “regresión a la media” surgieron de estos estudios y son hoy fundamentales en estadística y genética.\n\nEjemplo Gráfico\nGalton representó sus datos en gráficos de dispersión, mostrando cómo los puntos (pares de estatura media parental y estatura de los hijos) se agrupan alrededor de la recta de regresión, ilustrando la tendencia general de la relación.\n\n# Instalar y cargar los paquetes necesarios\nlibrary(ggplot2)\nlibrary(HistData)\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Crear el modelo de regresión lineal\nmodelo &lt;- lm(childHeight ~ midparentHeight, data = galton_data)\n\n# Crear el gráfico con ggplot2\ngrafico &lt;- ggplot(galton_data, aes(x = midparentHeight, y = childHeight)) +\n  geom_point() +  # Añadir puntos de datos\n  geom_smooth(method = \"lm\", col = \"red\") +  # Añadir la recta de regresión\n  labs(title = \"Altura de Padres e Hijos (Datos de Galton)\",\n       x = \"Altura de los Padres\",\n       y = \"Altura de los Hijos\") +\n  annotate(\"text\", x = 67, y = 75, label = paste(\"y =\", round(coef(modelo)[1], 2), \"+\", round(coef(modelo)[2], 2), \"x\"), color = \"red\")\n\n# Mostrar el gráfico\nprint(grafico)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#correlación",
    "href": "tema1.html#correlación",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.2 Correlación",
    "text": "1.2 Correlación\nLa correlación es una medida estadística que describe la relación entre dos variables. Permite evaluar si existe una asociación entre ellas y en qué grado los cambios en una variable están relacionados con los cambios en la otra. En el contexto de modelos de regresión, la correlación es un paso fundamental para explorar la fuerza y la dirección de la relación entre las variables explicativas y la variable respuesta.\n\n\n\n\n\n\nConcepto de correlación\n\n\n\n\n\nLa correlación responde a preguntas como:\n\n¿A valores altos de una variable le corresponden valores altos de la otra? (correlación positiva).\n¿A valores altos de una variable le corresponden valores bajos de la otra? (correlación negativa).\n¿No existe un patrón evidente de asociación? (ausencia de correlación).\n\n\n\n\n\n1.2.1 Covarianza\nLa covarianza es una medida que cuantifica cómo varían conjuntamente dos variables. Su fórmula es:\n\\[\n\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n}\n\\]\n\nSi la covarianza es positiva, indica una tendencia a que ambas variables aumenten juntas (relación positiva).\nSi es negativa, una variable tiende a aumentar mientras la otra disminuye (relación negativa).\n\nUna covarianza cercana a cero sugiere que no hay una relación lineal significativa.\n\nSin embargo, la covarianza tiene una limitación: su valor depende de las unidades de medida de las variables, lo que dificulta su interpretación directa.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Seleccionar las variables de interés\nmidparent_height &lt;- galton_data$midparentHeight\nchild_height &lt;- galton_data$childHeight\n\n# Calcular la covarianza\ncovarianza &lt;- round(cov(midparent_height, child_height),3)\n\n# Mostrar el resultado\nprint(paste(\"La covarianza entre la altura de los padres y la altura de los hijos es:\", covarianza))\n\n[1] \"La covarianza entre la altura de los padres y la altura de los hijos es: 2.07\"\n\n\n\n\n\n\n\n1.2.2 Coeficiente de correlación lineal\nPara superar la limitación anterior, se utiliza el coeficiente de correlación lineal de Pearson (\\(r\\)), que estandariza la covarianza dividiéndola por las desviaciones típicas de las variables:\n\\[\nr(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\]\nEl coeficiente de correlación es adimensional y toma valores entre \\(-1\\) y \\(1\\), facilitando su interpretación sin importar las unidades de las variables.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar los datos de Galton\ndata(\"GaltonFamilies\")\ngalton_data &lt;- GaltonFamilies\n\n# Seleccionar las variables de interés\nmidparent_height &lt;- galton_data$midparentHeight\nchild_height &lt;- galton_data$childHeight\n\n# Calcular la covarianza\ncorrelacion_pearson &lt;- round(cor(midparent_height, child_height),2)\n\n# Mostrar el resultado\nprint(paste(\"El coeficiente de correlación de Pearson entre la altura de los padres y la altura de los hijos es:\", correlacion_pearson))\n\n[1] \"El coeficiente de correlación de Pearson entre la altura de los padres y la altura de los hijos es: 0.32\"\n\n\n\n\n\nInterpretación del Coeficiente de Correlación\n\nMagnitud:\n\nValores cercanos a \\(|1|\\) indican una relación lineal fuerte.\n\nValores cercanos a \\(0\\) sugieren una relación lineal débil o inexistente.\n\nSigno:\n\nPositivo: Ambas variables tienden a moverse en la misma dirección.\n\nNegativo: Las variables tienden a moverse en direcciones opuestas.\n\n\nRelación lineal y otras relaciones\nEs importante destacar que una correlación de \\(r = 0\\) no implica necesariamente que no haya relación entre las variables. Puede haber una relación no lineal que el coeficiente de correlación lineal no detecta.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPor ejemplo, en un gráfico de dispersión en forma de parábola, el coeficiente de correlación lineal podría ser cercano a 0, a pesar de que existe una relación cuadrática clara entre las variables.\n\n# Generar datos de ejemplo\nset.seed(0)\nx &lt;- seq(-10, 10, length.out = 100)\ny &lt;- x^2 + rnorm(100, mean = 0, sd = 10)\n\n# Calcular el coeficiente de correlación de Pearson\ncorrelacion_pearson &lt;- cor(x, y)\n\n# Crear el gráfico de dispersión\nplot(x, y, main = \"Gráfico de Dispersión con Relación Cuadrática\",\n     xlab = \"X\", ylab = \"Y\", pch = 19)\nabline(h = 0, v = 0, col = \"gray\", lty = 2)\n\n# Mostrar el coeficiente de correlación en el gráfico\ntext(0, 80, paste(\"Correlación de Pearson:\", round(correlacion_pearson, 2)), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Aplicación práctica\nLa correlación es una herramienta inicial esencial en la exploración de datos:\n\nIdentifica variables explicativas potenciales para modelos de regresión.\n\nAyuda a entender la estructura de los datos y a verificar si una relación lineal es razonable.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn un análisis del tiempo de estudio semanal y las calificaciones, se puede calcular el coeficiente de correlación \\(r\\) para determinar si los estudiantes que estudian más tienden a obtener mejores resultados.\n\n# Generar datos de ejemplo\n# Calcular el coeficiente de correlación de Pearson\ncoef_correlacion &lt;- round(cor(datos$Tiempo_Estudio, datos$Calificaciones),3)\n\n# Mostrar el coeficiente de correlación\ncat(\"El coeficiente de correlación entre el tiempo de estudio y las calificaciones es:\", coef_correlacion, \"\\n\")\n\nEl coeficiente de correlación entre el tiempo de estudio y las calificaciones es: 0.898",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-simple",
    "href": "tema1.html#regresión-lineal-simple",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.3 Regresión lineal simple",
    "text": "1.3 Regresión lineal simple\nLa regresión lineal simple es una de las herramientas más fundamentales y ampliamente utilizadas en el análisis estadístico. Su objetivo principal es modelar la relación entre dos variables: una variable explicativa (independiente) y una variable respuesta (dependiente). Este modelo permite no solo describir cómo se relacionan estas dos variables, sino también realizar predicciones basadas en dicha relación.\nEl concepto básico de la regresión lineal simple es ajustar una recta que minimice las discrepancias entre los valores observados y los predichos por el modelo. La ecuación general de este modelo es:\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon,\n\\] donde:\n\n\\(Y\\) es la variable dependiente o respuesta.\n\n\\(X\\) es la variable independiente o explicativa.\n\nLa ecuación recibe el nombre de recta de regresión.\n\nLos coeficiente \\(\\beta_0\\) y \\(\\beta_1\\) reciben el nombre de coeficientes del modelo de regresión.\n\n\\(\\beta_0\\) es el intercepto, que representa el valor de \\(Y\\) cuando \\(X = 0\\).\n\n\\(\\beta_1\\) es la pendiente, que indica el cambio esperado en \\(Y\\) por cada unidad de cambio en \\(X\\).\n\n\n\\(\\varepsilon\\) es un término de error que captura la variabilidad no explicada por el modelo.\n\nPara cada valor fijo de \\(X\\), \\(Y\\) es una variable aleatoria, es decir, \\(Y\\) sigue una distribución de probabilidad para cada valor fijo de \\(X\\), con media o valor esperado: \\[\nE[Y|X]=\\beta_0+\\beta_1X,\n\\] y con varianza: \\[\nVar(Y|X)=Var(\\beta_0+\\beta_1X+\\varepsilon)=Var(\\varepsilon)=\\sigma^2.\n\\]\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\nPara que las inferencias del modelo sean válidas, se requieren ciertos supuestos sobre los datos y el término de error \\(\\varepsilon\\):\n\nLinealidad: La relación entre \\(X\\) y \\(Y\\) es lineal. Es decir, la esperanza de \\(Y\\) es una función lineal de \\(X\\).\n\nIndependencia: Los términos de error \\(\\varepsilon\\) son independientes entre sí. Los errores para observaciones distintas son incorrelados, es decir \\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) son no correlados y, por tanto, también lo son \\(Y_i\\) e \\(Y_j\\).\nHomocedasticidad: La varianza de los términos de error \\(\\varepsilon\\) es constante para todos los valores de \\(X\\). Es decir, la varianza de \\(Y\\) es constante, no depende \\(X\\).\n\n\nCuando el objetivo no es sólo estimar la recta, sino inferir con ella, entonces se asume una hipótesis más: la normalidad de la variable respuesta, o lo que es lo mismo, del error aleatorio:\n\nNormalidad: Los términos de error \\(\\varepsilon\\) siguen una distribución normal con media cero y varianza constante:\n\\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2) , \\hspace{0.5cm} i=1,\\ldots,n\n    \\]\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.\n\n\n\nEn resumen, el modelo de regresión lineal simple, implica que las respuestas \\(Y_i\\) proceden de una distribución de probabilidad con esperanza: \\[\nE[Y_i|X_i]=\\beta_0+\\beta_1X_i \\hspace{.5cm} i=1,\\dots,n,\n\\] y con varianza \\(\\sigma^2\\), para todas las \\(Y\\). Además, cualesquiera dos respuestas \\(Y_i\\) e \\(Y_j\\) son incorreladas.\nLa regresión lineal simple es fundamental en estadística y ciencia de datos porque proporciona un marco intuitivo y matemáticamente riguroso para analizar relaciones. Algunas aplicaciones comunes incluyen:\n\nPredecir valores futuros, como ingresos o gastos, en función de un predictor.\n\nEvaluar el impacto de una variable en otra, como el efecto de la inversión publicitaria en las ventas.\n\nExplorar relaciones lineales entre variables en estudios científicos o sociales.\n\nEn esta sección, exploraremos los fundamentos de la regresión lineal simple, desde su formulación teórica hasta su implementación práctica. Veremos cómo ajustar este modelo, interpretar sus parámetros, y evaluar su adecuación utilizando herramientas estadísticas y gráficas. Además, se presentarán ejemplos prácticos para ilustrar su aplicación en situaciones reales.\n\n1.3.1 Estimación de los parámetros del modelo\nLos parámetros del modelo (\\(\\beta_0\\) y \\(\\beta_1\\)) se estiman utilizando el método de mínimos cuadrados. Llamaremos \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) a los estimadores de los parámetros.\nLlamaremos valor predicho (\\(\\hat{Y}\\)):\n\\[\n  \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i,\n  \\] al valor estimado de \\(Y\\) para un valor dado de \\(X\\).\nY nos referimos al residuo (\\(e_i\\)):\n\\[\n  e_i = Y_i - \\hat{Y}_i,\n  \\] para medir la discrepancia entre el valor observado \\(Y_i\\) y el valor predicho \\(\\hat{Y}_i\\).\nEl método de los mínimos cuadrados busca minimizar la suma de los errores cuadráticos (residuos) entre los valores observados de \\(Y\\) y los valores predichos (\\(\\hat{Y}\\)):\n\\[\n\\text{SSE} = \\sum_{i=1}^n e_i^2 =\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n= \\sum_{i=1}^{n} \\left(Y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}\nX_i)\\right)^2.\n\\]\n\n\n\n\n\n\nMinimización de SSE\n\n\n\n\n\nLa obtención de los estimadores de mínimos cuadrados para la regresión lineal simple se basa en minimizar la suma de los cuadrados de los residuos (\\(SSE\\)). Aquí está el proceso paso a paso:\nPara minimizar \\(SSE\\), derivamos parcialmente con respecto a \\(\\beta_0\\) y \\(\\beta_1\\) y resolvemos el sistema de ecuaciones.\n\nPrimera derivada con respecto a \\(\\beta_0\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_0} = -2\\sum_{i=1}^n\n    \\left(Y_i - (\\beta_0 + \\beta_1 X_i)\\right).\n  \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\beta_1\n    X_i\\right) = 0.\n  \\]\nReordenando: \n\\[\n    n\\beta_0 + \\beta_1 \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i. \\tag{1}\n  \\]\n\nPrimera derivada con respecto a \\(\\beta_1\\):\n\n\\[\n    \\frac{\\partial SSE}{\\partial \\beta_1} = -2\\sum_{i=1}^n X_i\n    \\left(Y_i - (\\beta_0 + \\beta_1 X_i)\\right).\n    \\]\nIgualando a cero: \n\\[\n    \\sum_{i=1}^n X_i \\left(Y_i - \\beta_0 -\n    \\beta_1 X_i\\right) = 0.\n   \\]\nReordenando: \n\\[\n    \\beta_0 \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i. \\tag{2}\n   \\]\nResolución del Sistema de Ecuaciones\nEl sistema está dado por las ecuaciones (1) y (2):\n\n\\(n\\beta_0 + \\beta_1 \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i.\\)\n\n\\(\\beta_0 \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i.\\)\n\nResolviendo para \\(\\beta_0\\) y \\(\\beta_1\\):\n\nDe la primera ecuación, despejamos \\(\\beta_0\\):\n\\[\n\\beta_0 = \\frac{\\sum_{i=1}^n Y_i - \\beta_1 \\sum_{i=1}^n X_i}{n}. \\tag{3}\n\\]\nSustituimos \\(\\beta_0\\) en la segunda ecuación:\n\\[\n\\frac{\\sum_{i=1}^n Y_i - \\beta_1 \\sum_{i=1}^n X_i}{n} \\sum_{i=1}^n X_i + \\beta_1 \\sum_{i=1}^n X_i^2 = \\sum_{i=1}^n X_i Y_i.\n\\]\nSimplificando:\n\\[\n\\beta_1 \\left(\\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i)^2}{n}\\right) = \\sum_{i=1}^n X_i Y_i - \\frac{\\sum_{i=1}^n X_i \\sum_{i=1}^n Y_i}{n}.\n\\]\nExpresamos \\(\\beta_1\\):\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^n X_i Y_i - \\frac{\\sum_{i=1}^n X_i \\sum_{i=1}^n Y_i}{n}}{\\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i)^2}{n}}.\n\\] Esta es la fórmula para \\(\\beta_1\\), que puede reescribirse como:\n\\[\n\\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)},\n\\] donde \\(\\text{Cov}(X, Y)\\) y \\(\\text{Var}(X)\\) son la covarianza y la varianza muestral de \\(X\\) y \\(Y\\).\nFinalmente, sustituimos \\(\\beta_1\\) en la ecuación (3) para obtener \\(\\beta_0\\):\n\\[\n\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X},\n\\] donde \\(\\bar{X}\\) y \\(\\bar{Y}\\) son las medias de \\(X\\) y \\(Y\\).\n\n\n\n\nLas fórmulas para los estimadores de mínimos cuadrados son:\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2},\n\\] \\[\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X},\n\\] donde \\(\\bar{X}\\) y \\(\\bar{Y}\\) son las medias de \\(X\\) y \\(Y\\), respectivamente.\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\nAsumiendo la normalidad de la variable respuesta: \\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2) \\Leftrightarrow Y_i \\overset{\\mathrm{iid}}{\\sim} N(\\beta_0+\\beta_1X_i,\\sigma^2), \\hspace{0.5cm} i=1,\\ldots,n.\n    \\]\nSe tiene entonces la verosimilud para \\(\\beta=(\\beta_0,\\beta_1)\\), \\[\nL(\\beta; Y)=exp \\left ( -\\frac{\\sum_{i=1}^{n}(Y_i-\\beta_0-\\beta_1X_i)^2}{2\\sigma^2} \\right).\n\\]\nEn este ejercicio, pedimos buscar los valores de \\(\\beta\\) tales que maximizan la verosimilud. Esto es, los estimadores máximo verosimiles. Para ello derivamos e igualamos a cero. Se pide demostrar que los estimadores máximo verosimiles coinciden con los obtenidos por mínimos cuadrados.\n\n\n\n\n\n1.3.2 Propiedades de los estimadores\nEstudiemos algunas de las propiedades de los estimadores de mı́nimos cuadrados y del modelo de regresión ajustado.\n\nSean: \\[\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar{X})^2\n\\] y \\[\nS_{xy}=\\sum_{i=1}^n (X_i-\\bar{X})(Y_i-\\bar{Y})\n\\] Entonces: \\[\n\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}}\n\\]\nLos estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) son insesgados, es decir: \\[\nE[\\hat{\\beta_0}]=\\beta_0, \\space E[\\hat{\\beta_1}]=\\beta_1\n\\]\nLas varianzas de los estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) son: \\[\nVar(\\hat{\\beta_1})=\\frac{\\sigma^2}{S_{xx}}\n\\] donde \\(\\sigma^2\\) es la varianza del error \\(\\varepsilon\\). En la práctica, como \\(\\sigma^2\\) no se conoce, se estima con la varianza residual (\\(\\hat{\\sigma}^2\\)): \\[\n\\hat{\\sigma}^2=\\frac{SSE}{n-2},\n\\] siendo \\(SSE=\\sum_{i=1}^{n}(Y_i-\\hat{Y_i})^2\\), la suma de los cuadrados de los residos y \\(n-2\\) los grados de libertad (por estimar dos parámetros). De modo que:\n\n\\[\nVar(\\hat{\\beta_1}) \\approx \\frac{\\hat{\\sigma}^2}{S_{xx}}\n\\]\nDe igual modo tenemos: \\[\nVar(\\hat{\\beta_0}) \\approx \\hat{\\sigma}^2 \\left ( \\frac{1}{n}+\\frac{\\bar{X}^2}{S_{xx}}\\right )\n\\]\n\nTeorema de Gaus-Markov: Para el modelo de regresión con \\(E[\\varepsilon] = 0\\), \\(Var(\\varepsilon) = \\sigma^2\\) y los errores incorrelados, se tiene que los estimadores \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) son insesgados y de mı́nima varianza. Demostración en (Kutner et al. 2005).\n\n\n\n\n\n\n\nPropiedades adicionales para las predicciones y para los residuos\n\n\n\n\n\n\nLa suma de los residuos es cero: \\[\n  \\sum_{i=1}^n e_i=\\sum_{i=1}^n(Y_i-\\hat{Y_i})=0\n  \\]\nLa suma de los valores observados es igual a la suma de los valores ajustados: \\[\n  \\sum_{i=1}^n Y_i=\\sum_{i=1}^n \\hat{Y_i}\n  \\]\nLa suma de los residuos ponderados por los regresores es cero: \\[\n  \\sum_{i=1}^n X_ie_i=0\n  \\]\nLa suma de los residuos ponderados por las predicciones es cero: \\[\n  \\sum_{i=1}^n \\hat{Y_i}e_i=0\n  \\]\nLa recta de regresión contiene el punto \\((\\bar{X},\\bar{Y})\\):\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener estimadores de los parámetros y errores estándar\nestimadores &lt;- coef(summary(modelo))  # Incluye estimadores y errores estándar\n\n# Imprimir los resultados\ncat(\"Estimadores para los parámetros del modelo:\\n\")\n\nEstimadores para los parámetros del modelo:\n\nprint(estimadores)\n\n                 Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)    5.00117598 0.11976840 41.75706 3.102826e-64\ntiempo_estudio 0.09874923 0.00488005 20.23529 9.033965e-37\n\n\n\n\n\nUna vez obtenida la recta de regresión, surgen una serie de preguntas interesantes:\n\n¿Cómo de bien describe la ecuación de regresión los datos observados?\n¿Podemos emplear este modelo para predecir nuevas observaciones?\n¿Se cumplen todas las suposiciones del modelo?\n\nTodas estas cuestiones serán analizadas antes de adoptar el modelo como válido. Una herramienta muy importante en la validación del modelo será el estudio de los residuos. Lo veremos más adelante.\n\n\n1.3.3 Inferencia sobre los parámetros del modelo\nEn regresión, con frecuencia interesa realizar contraste de hipótesis o construir intervalos de confianza para los parámetros del modelo.\nPara hacer inferencias consideramos la distribución en el muestreo de los estimadores de dichos parámetros. Este procedimiento requiere que tengamos en cuenta la suposición de normalidad sobre los errores \\(\\varepsilon_i\\) , es decir: \\[\n    \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\] Entonces: \\[\nY_i\\sim N(\\beta_0+\\beta_1X,\\sigma^2),\n\\] y se puede demostrar que: \\[\n\\hat{\\beta_1} \\sim N\\left (\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right),\n\\] \\[\n\\hat{\\beta_0} \\sim N\\left ( \\beta_0, \\sigma^2 \\left ( \\frac{1}{n} + \\frac{\\bar{X}^2}{S_{xx}} \\right) \\right )\n\\]\nPuesto que las varianzas de los parámetros1 dependen de \\(\\sigma^2\\) , cuando el modelo de regresión es adecuado, podemos estimarlas sustituyendo dicho valor por su estimador insesgado \\(s^2\\). En este caso, la distribución de los parámetros ya no será normal, al igual que ocurrı́a en la distribución del valor esperado \\(\\mu\\) en una población i.i.d., aparece la distribución t-Student: \\[\nt=\\frac{\\hat{\\beta_1}-\\beta_1}{SE({\\hat{\\beta_1}})}\\sim t_{n-2},\n\\] siendo \\(SE(\\hat{\\beta_1})=\\frac{s}{\\sqrt{S_{xx}}}\\), es una t-Student con \\(n-2\\) grados de ligertad.\n\n1.3.3.1 Intervalo de confianza\nAsí, su intervalo de confianza al \\(1-\\alpha\\%\\) calculado a partir de la distribución en el muestro es: \\[\nIC(\\beta_1;1-\\alpha)=\\hat{\\beta_1} \\pm t_{1-\\alpha/2,n-2}\\sqrt{\\frac{s^2}{S_{xx}}},\n\\] donde \\(t_{1-\\alpha/2,n-2}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-2\\) grados de libertad (los correspondientes a \\(s^2\\)).\n\n\n1.3.3.2 Contraste de hipótesis\nEn regresión, normalmente estamos interesado en realizar el siguiente contraste de hipótesis: \\[\nH_0:\\beta_1=0 \\text{ frente a } H_1: \\beta_1\\neq 0.\n\\] De este modo, si no podemos rechazar la hipótesis nula, podemos concluir que no existe prueba (en los datos) de una relación lineal significativa entre las variables estudiadas. Este tipo de contraste nos será muy útil cuando trabajemos con modelos con más de una variable para eliminar aquellas que no son importantes dentro del mismo.\nEn el caso del modelo de regresión lineal simple, no rechazar \\(H_0\\) implica que la mejor predicción para todas las observaciones es: \\(\\hat{Y_i}=Y_i\\), o bien que la relación entre las variables \\(X\\) e \\(Y\\) no es lineal y, entonces, demos corregir nuestro modelo.\nPor otro lado, rechazar la hipótesis nula en favor de la alternativa, significa que la variable \\(X\\) influye al explicar la variabilidad de la variable \\(Y\\). Quizás el modelo no sea el más adecuado, pero no podemos eliminar la variable del mismo.\nPara realizar el contraste, como \\(\\varepsilon_i\\) sin variables i.i.d. tales que \\(\\varepsilon_i\\sim N(0,\\sigma^2)\\), entonces:\n\\[\nY_i\\sim N(\\beta_0+\\beta_1X,\\sigma^2), \\text{ } \\hat{\\beta_1} \\sim N\\left (0,\\frac{\\sigma^2}{S_{xx}}\\right)\n\\]\nDe manera que si, como es lo habitual, \\(\\sigma^2\\) es desconocida, como \\(E[\\hat{\\sigma^2}]=\\sigma^2\\), se tiene que, bajo la hipótesis nula: \\[\nt=\\frac{\\hat{\\beta_1}}{SE({\\hat{\\beta_1}})}\\sim t_{n-2},\n\\] donde \\(SE({\\hat{\\beta_1}})=\\sqrt{\\frac{\\hat{\\sigma^2}}{S_{xx}}}\\), es el error estándar estimado del parámetro \\(\\hat{\\beta_1}\\). De este modo, la hipótesis nula será rechazada si \\(|t|&gt;t_{\\alpha/2,n-2}\\), siendo \\(\\alpha\\), el nivel de significación del contraste.\n\n\n\n\n\n\nPara recordar\n\n\n\nEn los programas estadı́sticos se suele proporcionar el p-valor del contraste. Puedes repasar el significado de p-valor proporcionado en la asignatura de Inferencia.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nA continuación se presenta un resumen del modelo. Podemos ver los p-valores asociados a cada uno de los parámetros del modelo.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener resumen del modelo\nsummary(modelo)\n\n\nCall:\nlm(formula = calificaciones ~ tiempo_estudio)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11465 -0.30262 -0.00942  0.29509  1.10533 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.00118    0.11977   41.76   &lt;2e-16 ***\ntiempo_estudio  0.09875    0.00488   20.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4842 on 98 degrees of freedom\nMultiple R-squared:  0.8069,    Adjusted R-squared:  0.8049 \nF-statistic: 409.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n1.3.4 Descomposición de la varianza: ANOVA\nLa descomposición de la varianza es un paso crucial para evaluar la calidad del ajuste del modelo de regresión lineal simple. A través de un análisis de varianza (ANOVA), se divide la variabilidad total de la variable respuesta (\\(Y\\)) en componentes atribuibles al modelo y al error, proporcionando un marco para evaluar si la relación entre \\(X\\) e \\(Y\\) es estadísticamente significativa. Un modelo es bueno si la variabilidad explicada es mucha, o lo que es lo mismo, si las diferencias entre los datos y las predicciones según el modelo son pequeñas.\n\n\n\n\n\n\nRepaso\n\n\n\nEs conveniente repasar el tema de Análisis de la Varianza estudiado en la asignatura de Inferencia.\n\n\n\n1.3.4.1 Variabilidad total\nLa variabilidad total de\\(Y\\) se mide mediante la Suma Total de los Cuadrados (SST):\n\\[ SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2,\n\\]\ndonde \\(\\bar{Y}\\) es la media de \\(Y\\). \\(SST\\) refleja la dispersión general de \\(Y\\) respecto a su media.\nDescomposición de la variabilidad\nEl modelo de regresión lineal permite descomponer\\(SST\\) en dos componentes principales:\n\\[ SST = SSR + SSE,\n\\]\ndonde:\n\n\\(SSR\\) (Suma de los Cuadrados del Modelo): Representa la variabilidad explicada por la regresión, es decir, la parte de \\(Y\\) que se puede predecir a partir de \\(X\\):\n\\[ SSR = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2,\n\\] donde \\(\\hat{Y}_i\\) es el valor predicho por el modelo para el \\(i\\)-ésimo dato.\n\\(SSE\\) (Suma de los Cuadrados de los Errores): Como hemos visto al inicio de esta sección, representa la variabilidad no explicada por el modelo, es decir, la dispersión de los valores observados respecto a los valores predichos:\n\\[ SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2.\n\\]\n\n\n\n1.3.4.2 Tabla ANOVA\nEl análisis de varianza organiza la descomposición de la varianza en una tabla, donde cada componente se asocia con sus grados de libertad (\\(df\\)), suma de cuadrados (\\(SS\\)), media cuadrática (\\(MS\\)) y el estadístico \\(F\\):\n\n\n\nFuente\n\\(df\\)\n\\(SS\\)\n\\(MS = SS/df\\)\nEstadístico\\(F\\)\n\n\n\n\nRegresión\n1\n\\(SSR\\)\n\\(MSR = SSR/1\\)\n\\(F = MSR/MSE\\)\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = SSE/(n-2)\\)\n\n\n\nTotal\n\\(n-1\\)\n\\(SST\\)\n\n\n\n\n\n\n\n1.3.4.3 Prueba de significancia global\nPara evaluar si \\(X\\) tiene un efecto significativo sobre \\(Y\\), se utiliza el estadístico \\(F\\):\n\\[\nF = \\frac{MSR}{MSE}.\n\\]\n\nBajo la hipótesis nula (\\(H_0: \\beta_1 = 0\\)), \\(F\\) sigue una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad.\nSi el valor \\(p\\) asociado al estadístico \\(F\\) es pequeño (\\(p &lt; \\alpha\\)), se rechaza \\(H_0\\), indicando que \\(X\\) tiene un efecto significativo sobre \\(Y\\).\n\nEl análisis ANOVA permite responder preguntas clave:\n\n¿Qué proporción de la variabilidad de \\(Y\\) es explicada por \\(X\\)?\n\n¿Es significativa esta relación desde el punto de vista estadístico?\n\nEn el contexto de la regresión lineal simple, esta herramienta no solo cuantifica el ajuste del modelo, sino que también valida su relevancia estadística.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nA continuación se presenta la tabla ANOVA correspondiente al modelo de horas de estudio y calificaciones, previamente entrenado.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener y mostrar la tabla ANOVA\ntabla_anova &lt;- anova(modelo)\ncat(\"Tabla ANOVA:\\n\")\n\nTabla ANOVA:\n\nprint(tabla_anova)\n\nAnalysis of Variance Table\n\nResponse: calificaciones\n               Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntiempo_estudio  1 96.016  96.016  409.47 &lt; 2.2e-16 ***\nResiduals      98 22.980   0.234                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n1.3.5 Bondad del ajuste: coeficiente de determinación\nEl coeficiente de determinación (\\(R^2\\)) mide qué proporción de la variabilidad total en \\(Y\\) es explicada por \\(X\\) a través del modelo:\n\\[\nR^2 = 1 - \\frac{\\text{Suma de los Cuadrados de los Residuos (SSE)}}{\\text{Suma Total de los Cuadrados (SST)}}\n\\] \\[\n= \\frac{\\text{Suma de los Cuadrados del Modelo (SSR)}}{\\text{Suma Total de los Cuadrados (SST)}}.\n\\]\nDonde:\n\n\\(\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\): Variabilidad total en \\(Y\\).\n\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\): Variabilidad explicada por el modelo.\n\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\): Variabilidad no explicada.\n\nUn \\(R^2\\) cercano a 1 indica que el modelo ajusta bien los datos, mientras que un \\(R^2\\) cercano a 0 indica un ajuste pobre.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEn el resumen del modelo anterior hemos obtenido el siguiente valor para el coeficiente de determinación \\(R^2=0.8069\\). ¿Cómo evaluas el ajuste obtenido en este modelo?\n\n\n\n\n\n\n\n\n\nObservaciones\n\n\n\n\n\n\n\\(R^2\\) debe ser interpretado con cautela, ya que resultará con frecuencia grande a pesar de que la relación entre \\(X\\) e \\(Y\\) no sea lineal.\nAsı́ por ejemplo, la magnitud de \\(R^2\\) depende del rango de variabilidad de la variable explicativa, \\(X\\). Siendo el modelo de regresión adecuado, la magnitud de \\(R^2\\) aumenta (o disminuye) cuando lo hace la dispersión de \\(X\\).\nAdemás \\(R^2\\) podrı́a ser un valor muy pequeño debido a que el rango de variación de \\(X\\) es demasiado pequeño e impide que se detecte la relación con \\(Y\\).\n\n\n\n\n\n\n1.3.6 Diagnóstico del modelo\nEl diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación de los datos a los supuestos de la regresión lineal.\nEl análisis de residuos ayuda a verificar los siguientes supuestos del modelo:\n\nLinealidad: La relación entre \\(X\\) e \\(Y\\)es lineal.\nIndependencia: Los residuos son independientes entre sí.\nHomocedasticidad: Los residuos tienen varianza constante.\nNormalidad: Los residuos se distribuyen de manera aproximadamente normal.\n\nUna vez ajustado el modelo, hemos de detectar desviaciones de las hipótesis: proceder al diagnóstico del modelo. El análisis de los residuos nos permitirá identificar deficiencias en la verificación de estas hipótesis, ası́ como observaciones anómalas o influyentes.\nRecordemos que los residuos del modelo son las diferencias entre los valores observados y los valores predichos por el modelo: \\[\ne_i = Y_i - \\hat{Y}_i.\n\\] Representan la parte de la variabilidad en \\(Y\\) no explicada por el modelo.\nEn ocasiones, es preferible trabakar con los residuos estandarizados, que tienen media cero y varianza aproximadamente unidad: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE}},  \\hspace{0.5cm} i=1,\\ldots,n.\n\\] Otro tipo de residuos habitual es el de los llamados residuos estudentizados: \\[\nr_i = \\frac{e_i}{\\sqrt{MSE \\left ( 1-\\frac{1}{n}-\\frac{(X_i-\\bar{X})^2}{S_{xx}} \\right )}}, \\hspace{0.5cm} i=1,\\ldots,n.\n\\]\nEstos residuos son preferibles a los estandarizados cuando \\(n\\) es pequeño.\n\n1.3.6.1 Pasos en el diagnóstico\nEn primer lugar emplearemos gráficos de los residuos para visualizar cómo se comportan las discrepancias entre los valores observados y los predichos en relación con diversas variables o aspectos del modelo.\nA continuación se ejecutarán pruebas estadísticas para confirmar los hallazgos visuales.\nCorregir los problemas detectados será el último paso. Para ello, y dependiendo del problema, será necesario transformar la variable respuesta, considerar modificaciones del modelo, o el uso de modelos alternativos.\n\n\n1.3.6.2 Gráficos de residuos\nLa utilidad de los gráficos radica en que ofrecen una manera intuitiva de evaluar si los supuestos clave del modelo de regresión se cumplen, ayudando a identificar problemas que podrían comprometer la validez de las conclusiones. Los gráficos de residuos no solo permiten diagnosticar problemas en el ajuste del modelo, sino también identificar áreas donde el modelo puede ser mejorado. Su uso sistemático en el análisis de regresión asegura un modelo más robusto, fiable y ajustado a los datos, mejorando la calidad de las conclusiones y predicciones.\nLas principales utilidades de estos gráficos son:\n\nEvaluación de la linealidad:\n\nUn gráfico de residuos versus valores ajustados ayuda a identificar si la relación entre la variable explicativa y la variable respuesta es lineal. La ausencia de patrones sistemáticos indica que el supuesto de linealidad se cumple.\n\nDetección de heterocedasticidad:\n\nSe detectan problemas de heterocedasticidad, si en el gráfico de residuos versus valores ajustados se observa un patrón de “embudo” o una variación creciente/disminuyente, es decir, cuando se observa que la varianza de los residuos no es constante para todos los valores de \\(X\\).\n\nVerificación de normalidad:\n\nRecordemos que la hipótesis de normalidad de los residuos es esencial para realizar inferencias estadísticas fiables (intervalos de confianza, pruebas de hipótesis). Los histogramas de residuos o los gráficos QQ-plot permiten detectar desviaciones respecto a la distribución normal. Los residuos estandarizados y estudentizados también son útiles para detectar desviaciones de la normalidad, Si los errores se distribuyen según una normal, entonces aproximadamente el \\(68\\%\\) de los residuos estandarizados (estudentizados) quedarán entre \\(−1\\) y \\(+1\\), y el \\(95\\%\\) entre \\(−2\\) y \\(+2\\).\n\nDetección de observaciones atípicas e influyentes:\n\nLos gráficos de residuos ayudan a identificar puntos atípicos (outliers) o valores con alta influencia que podrían afectar de manera desproporcionada el modelo. Esto es especialmente útil para decidir si ajustar el modelo o realizar un análisis más detallado de estos puntos.\n\nFacilidad para diagnóstico rápido:\n\nLos gráficos de residuos son intuitivos y ofrecen una evaluación visual inmediata de posibles problemas, complementando las pruebas estadísticas formales.\nLos gráficos más comunes para llevar a cabo todos los análisis anteriores son:\n\nResiduos vs. Valores Ajustados: Este gráfico permite verificar la linealidad y la homocedasticidad. Los residuos deben distribuirse aleatoriamente alrededor del cero sin patrones discernibles. Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., podemos tener algún problema con la violación de la hipótesis de varianza constante para los residuos (heterocedasticidad). También pueden aparecer gráficos con cierta tendencia cuando la relación entre \\(X\\) e \\(Y\\) es no lineal. Aunque esto se podrı́a ver en un gráfico de dispersión de (\\(X\\) , \\(Y\\) ), suele ser más evidente en un gráfico con los residuos.\nHistograma de Residuos: Muestra si los residuos tienen una distribución aproximadamente normal.\nQQ-Plot (Gráfico Cuantil-Cuantil): Compara la distribución de los residuos con una distribución normal. Si los puntos se alinean con la diagonal, se cumple la normalidad. Se dibujan los residuos ordenados \\(e_{[i]}\\) frente a los cuantiles correspondientes de una normal estándar, \\(\\phi^{−1}_{[(i − 1)/n]}\\). Si es cierta la normalidad de los residuos, el gráfico resultante deberı́a corresponderse con una recta.\nResiduos vs. Variables Predictoras: Verifica si hay patrones entre los residuos y \\(X\\), lo que indicaría problemas de linealidad o varianza no constante.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEs análogo hacer el gráfico de los \\(e_i\\) frente a \\(\\hat{Y_i}\\) o \\(X_i\\), ya que los valores ajustados \\(\\hat{Y_i}\\) son función lineal de \\(X_i\\).\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los residuos del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener los residuos\nresiduos &lt;- resid(modelo)\n\n# Estudiar los residuos\n\n# 1. Gráfico de residuos vs. valores ajustados\nvalores_ajustados &lt;- fitted(modelo)\nplot(valores_ajustados, residuos,\n     main = \"Residuos vs Valores Ajustados\",\n     xlab = \"Valores Ajustados\",\n     ylab = \"Residuos\",\n     pch = 19, col = \"blue\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# 2. Histograma de los residuos\nhist(residuos,\n     main = \"Histograma de Residuos\",\n     xlab = \"Residuos\",\n     col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n# 3. QQ-Plot de residuos\nqqnorm(residuos, main = \"QQ-Plot de los Residuos\")\nqqline(residuos, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# 4. Pruebas de normalidad de los residuos\nshapiro_test &lt;- shapiro.test(residuos)\ncat(\"Prueba de Shapiro-Wilk para normalidad de los residuos:\\n\")\n\nPrueba de Shapiro-Wilk para normalidad de los residuos:\n\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuos\nW = 0.99008, p-value = 0.671\n\n# 5. Gráfico de residuos vs. tiempo de estudio\nplot(tiempo_estudio, residuos,\n     main = \"Residuos vs Tiempo de Estudio\",\n     xlab = \"Tiempo de Estudio (horas/semana)\",\n     ylab = \"Residuos\",\n     pch = 19, col = \"purple\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.6.3 Estadísticas para el diagnóstico\nLas pruebas estadísticas son útiles para confirmar los hallazgos visuales obtenidos al analizar los gráficos de los residuos. Las principales pruebas formales a considerar son:\n\nPrueba de normalidad (Shapiro-Wilk): Confirma si los residuos siguen una distribución normal. Un p-valor por debajo del grado de significatividad elegido (típicamente \\(0.05\\)), indica que no podemos rechazar la hipótesis nula de normalidad de los residuos.\nPrueba de autocorrelación (Durbin-Watson): Evalúa la independencia de los residuos.\nPrueba de homocedasticidad (Breusch-Pagan): Detecta si los residuos presentan varianza constante. Un p-valor pequeño (típicamente menor que \\(0.05\\)) sugiere heterocedasticidad.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los residuos del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Obtener los residuos\nresiduos &lt;- resid(modelo)\n\n# Estadísticas para diagnóstico\n\n# 1. Prueba de normalidad de los residuos (Shapiro-Wilk)\nshapiro_test &lt;- shapiro.test(residuos)\ncat(\"Prueba de Shapiro-Wilk para normalidad de los residuos:\\n\")\n\nPrueba de Shapiro-Wilk para normalidad de los residuos:\n\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuos\nW = 0.99008, p-value = 0.671\n\n# 2. Prueba de homocedasticidad (Breusch-Pagan)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nbreusch_pagan &lt;- bptest(modelo)\ncat(\"\\nPrueba de Breusch-Pagan para homocedasticidad:\\n\")\n\n\nPrueba de Breusch-Pagan para homocedasticidad:\n\nprint(breusch_pagan)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo\nBP = 0.019638, df = 1, p-value = 0.8886\n\n# 3. Estadística Durbin-Watson para autocorrelación de los residuos\ndurbin_watson &lt;- dwtest(modelo)\ncat(\"\\nPrueba de Durbin-Watson para autocorrelación de los residuos:\\n\")\n\n\nPrueba de Durbin-Watson para autocorrelación de los residuos:\n\nprint(durbin_watson)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 2.0565, p-value = 0.6104\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n\n\n\n1.3.7 Observaciones atípicas (outliers)\nEn el contexto de la regresión lineal simple, una observación atípica es un punto de datos que se aleja notablemente del patrón general observado entre la variable independiente (\\(X\\)) y la variable dependiente (\\(Y\\)). Estas observaciones pueden surgir por errores en la recopilación de datos, valores extremos naturales o situaciones especiales que no están representadas en el modelo.\nLas observaciones atípicas pueden tener un impacto significativo en el ajuste del modelo, afectando las estimaciones de los coeficientes de regresión, el análisis de residuos y la validez de las conclusiones. Las observaciones atípicas pueden distorsionar los coeficientes \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), añadiendo sesgos a las estimaciones. Además, pueden conducir a diagnóticos incorrectos, al alterae la normalidad y homocedasticidad de los residuos, afectando las pruebas estadísticas. Por último, conducen a predicciones inexactas, puesto que los modelos ajustados en presencia de valores atípicos pueden no ser representativos del conjunto de datos general.\n\n1.3.7.1 Tipos de observaciones atípicas\n\nOutliers en \\(Y\\):\n\n\nSon puntos que tienen valores de \\(Y\\) mucho mayores o menores en comparación con lo predicho por el modelo.\n\nEstos puntos suelen detectarse como residuos grandes.\n\n\nOutliers en \\(X\\):\n\n\nSon puntos con valores extremos de \\(X\\) que no están bien representados en el rango principal de los datos.\n\nSi bien pueden no afectar directamente la regresión, pueden influir en los coeficientes del modelo si tienen alta “leverage” (potencial para influir en el ajuste).\n\n\nObservaciones Influyentes:\n\n\nSon puntos que, debido a su posición, tienen un impacto desproporcionado en el ajuste del modelo. Pueden ser outliers en \\(X\\), \\(Y\\), o ambos.\n\n\n\n1.3.7.2 Detección de observaciones atípicas\nUn gráfico de dispersión entre la variables respuesta y la variable explicativa permite identificar qué puntos se desvían claramente del patrón general. Además, en un gráfico de los residuos frente a los valores ajutados, los outliers en \\(Y\\) aparecen como residuos grandes.\nGeneralmente, residuos estudentizados con valores mayores a \\(|3|\\) indican outliers.\nEl leverage, o apalancamiento mide el grado en el que una observación individual influye en el ajuste del modelo. En términos simples, el leverage cuantifica cómo de “lejos” está un punto de datos de la media de la variable explicativa \\(X\\) y, por lo tanto, cuánto contribuye al ajuste del modelo.\nEl leverage se calcula a partir de la matriz de proyección \\(H\\) (también llamada matriz “hat”), que transforma los valores observados en valores ajustados. La diagonal de esta matriz (\\(h_{ii}\\)) mide el leverage para cada observación: \\[\nh_{ii}=\\frac{1}{n}+\\frac{(X_i-\\bar{X})^2}{\\sum_{j=1}^n (X_j-\\bar{X})^2}\n\\]\nEl leverage (\\(h_{ii}\\)) toma valores entre \\(0\\) y \\(1\\). Un leverage cercano a \\(0\\) indica que el punto está muy cerca del centro de los datos, mientras que un leverage alto sugiere que el punto está lejos. Valores de leverage altos (\\(h_{ii} &gt; \\frac{2p}{n}\\), donde \\(p\\) es el número de parámetros del modelo) son sospechosos.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los valores de leverage del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# Calcular leverage\nleverage &lt;- hatvalues(modelo)\n\n# Umbral para leverage alto\np &lt;- length(coef(modelo))  # Número de parámetros (incluyendo el intercepto)\nleverage_threshold &lt;- 2 * p / n\n\n# Identificar observaciones con leverage alto\nleverage_high &lt;- which(leverage &gt; leverage_threshold)\n\n# Resultados\ncat(\"Valores de leverage:\\n\")\n\nValores de leverage:\n\nprint(leverage)\n\n         1          2          3          4          5          6          7 \n0.01548204 0.02046914 0.01100581 0.02838063 0.03425201 0.03550447 0.01011261 \n         8          9         10         11         12         13         14 \n0.02920954 0.01034872 0.01021265 0.03617186 0.01024305 0.01397099 0.01066195 \n        15         16         17         18         19         20         21 \n0.02947303 0.03005673 0.01794903 0.03582737 0.01359185 0.03584680 0.02893121 \n        22         23         24         25         26         27         28 \n0.01463144 0.01249149 0.04058238 0.01301990 0.01549100 0.01024494 0.01114180 \n        29         30         31         32         33         34         35 \n0.01548204 0.02548264 0.03682806 0.03034319 0.01463144 0.02088565 0.03780742 \n        36         37         38         39         40         41         42 \n0.01005667 0.01832352 0.01984759 0.01409128 0.01887292 0.02573445 0.01088203 \n        43         44         45         46         47         48         49 \n0.01088203 0.01209977 0.02498512 0.02598829 0.01868408 0.01013361 0.01674089 \n        50         51         52         53         54         55         56 \n0.02600358 0.03550447 0.01038499 0.02131030 0.02755397 0.01047077 0.02066389 \n        57         58         59         60         61         62         63 \n0.02702395 0.01814065 0.02948991 0.01191912 0.01347920 0.03032595 0.01166337 \n        64         65         66         67         68         69         70 \n0.01625359 0.02240745 0.01030996 0.02218396 0.02218396 0.02088565 0.01042556 \n        71         72         73         74         75         76         77 \n0.01814065 0.01210532 0.01564137 0.04091460 0.01007286 0.01964859 0.01174659 \n        78         79         80         81         82         83         84 \n0.01158700 0.01269048 0.02863839 0.01812975 0.01359910 0.01082318 0.02046914 \n        85         86         87         88         89         90         91 \n0.02947303 0.01051278 0.03953410 0.02948991 0.02865490 0.02307628 0.02676199 \n        92         93         94         95         96         97         98 \n0.01301990 0.01301326 0.01313168 0.01396337 0.02194927 0.02006074 0.03032595 \n        99        100 \n0.01013361 0.01002084 \n\ncat(\"\\nUmbral para leverage alto:\", leverage_threshold, \"\\n\")\n\n\nUmbral para leverage alto: 0.04 \n\ncat(\"\\nObservaciones con leverage alto (si las hay):\\n\")\n\n\nObservaciones con leverage alto (si las hay):\n\nprint(leverage_high)\n\n24 74 \n24 74 \n\n# Gráfico de leverage\nplot(leverage, \n     main = \"Leverage de las Observaciones\",\n     xlab = \"Índice de Observación\",\n     ylab = \"Leverage\",\n     pch = 19, col = \"blue\", ylim=c(min(leverage)*.9,max(leverage)*1.05))\nabline(h = leverage_threshold, col = \"red\", lwd = 2, lty = 2)  # Línea del umbral\ntext(leverage_high, leverage[leverage_high], labels = leverage_high, pos = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nLas medidas de influencia son herramientas que evalúan el impacto que tiene cada observación individual en el ajuste del modelo. Mientras que conceptos como los residuos y el leverage analizan diferentes aspectos de las observaciones, las medidas de influencia integran esta información para determinar qué tan decisiva es una observación en la estimación de los coeficientes y en las predicciones del modelo:\n\nDistancia de Cook: Combina leverage y residuos para medir el impacto de una observación en el ajuste del modelo. Valores mayores a \\(1\\) son indicativos de observaciones influyentes.\nDFBETAS: Evalúa cuánto cambia un coeficiente del modelo si se elimina una observación. Valores absolutos mayores a \\(2/\\sqrt{n}\\) sugieren influencia significativa.\nDFFITS: Mide el impacto de una observación en el valor ajustado. Valores absolutos mayores a \\(2\\sqrt{p/n}\\) sugieren influencia significativa.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nEstudiamos los valores de leverage del ejemplo de calificaciones de estudiantes.\n\n# Ajustar el modelo de regresión lineal\nmodelo &lt;- lm(calificaciones ~ tiempo_estudio)\n\n# 1. Calcular Distancia de Cook\ncooks_distance &lt;- cooks.distance(modelo)\n\n# 2. Calcular DFBETAS\ndfbetas_values &lt;- dfbetas(modelo)\n\n# 3. Calcular DFFITS\ndffits_values &lt;- dffits(modelo)\n\n# Umbrales sugeridos\ncooks_threshold &lt;- 4 / n  # Umbral para Distancia de Cook\ndffits_threshold &lt;- 2 * sqrt(length(coef(modelo)) / n)  # Umbral para DFFITS\n\n# Resultados\ncat(\"Distancia de Cook (primeras 10 observaciones):\\n\")\n\nDistancia de Cook (primeras 10 observaciones):\n\nprint(head(cooks_distance, 10))\n\n           1            2            3            4            5            6 \n7.431009e-04 3.985310e-05 2.107588e-07 3.357548e-02 3.184641e-04 4.788575e-02 \n           7            8            9           10 \n1.210846e-02 7.376917e-03 1.793360e-04 4.056442e-04 \n\ncat(\"\\nObservaciones con Distancia de Cook alta (&gt; \", cooks_threshold, \"):\\n\")\n\n\nObservaciones con Distancia de Cook alta (&gt;  0.04 ):\n\nprint(which(cooks_distance &gt; cooks_threshold))\n\n 6 20 47 85 87 89 \n 6 20 47 85 87 89 \n\ncat(\"\\nDFBETAS (primeras 10 observaciones):\\n\")\n\n\nDFBETAS (primeras 10 observaciones):\n\nprint(head(dfbetas_values, 10))\n\n     (Intercept) tiempo_estudio\n1   0.0333531373  -0.0228338026\n2  -0.0032998199   0.0063523298\n3   0.0004275323  -0.0001952665\n4  -0.1294145160   0.2099533405\n5   0.0138396781  -0.0211294746\n6   0.3088623210  -0.2644845389\n7  -0.0478874296  -0.0165389211\n8  -0.0611970157   0.0982450431\n9   0.0043259263   0.0034593203\n10  0.0150833337  -0.0040906772\n\ncat(\"\\nDFFITS (primeras 10 observaciones):\\n\")\n\n\nDFFITS (primeras 10 observaciones):\n\nprint(head(dffits_values, 10))\n\n            1             2             3             4             5 \n 0.0383726006  0.0088823393  0.0006459231  0.2608877009 -0.0251106446 \n            6             7             8             9            10 \n 0.3120568093 -0.1567290069  0.1211475166  0.0188450608  0.0283488066 \n\ncat(\"\\nObservaciones con DFFITS alto (&gt; \", dffits_threshold, \"):\\n\")\n\n\nObservaciones con DFFITS alto (&gt;  0.2828427 ):\n\nprint(which(abs(dffits_values) &gt; dffits_threshold))\n\n 6 20 22 47 85 87 89 \n 6 20 22 47 85 87 89 \n\n# Graficar Distancia de Cook\nplot(cooks_distance,\n     main = \"Distancia de Cook\",\n     xlab = \"Índice de Observación\",\n     ylab = \"Distancia de Cook\",\n     pch = 19, col = \"blue\", ylim=c(min(cooks_distance)*.9,max(cooks_distance)*1.05))\nabline(h = cooks_threshold, col = \"red\", lwd = 2, lty = 2)\ntext(which(cooks_distance &gt; cooks_threshold), cooks_distance[cooks_distance &gt; cooks_threshold],\n     labels = which(cooks_distance &gt; cooks_threshold), pos = 3, col = \"red\")\n\n\n\n\n\n\n\n# Graficar DFFITS\nplot(dffits_values,\n     main = \"DFFITS\",\n     xlab = \"Índice de Observación\",\n     ylab = \"DFFITS\",\n     pch = 19, col = \"green\", ylim=c(min(dffits_values)*.85,max(dffits_values)*1.07))\nabline(h = c(dffits_threshold, -dffits_threshold), col = \"red\", lwd = 2, lty = 2)\ntext(which(abs(dffits_values) &gt; dffits_threshold), dffits_values[abs(dffits_values) &gt; dffits_threshold],\n     labels = which(abs(dffits_values) &gt; dffits_threshold), pos = 3, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.7.3 Tratamiento de observaciones atípicas\nHemos visto que las observaciones atípicas son inevitables en el análisis de datos y pueden surgir por diversas razones, como errores en la recopilación de datos, fenómenos extremos o situaciones únicas en el conjunto de datos. Aunque estas observaciones pueden proporcionar información valiosa, también tienen el potencial de distorsionar los resultados del análisis, afectando la validez de los modelos ajustados y las conclusiones derivadas.\nEl manejo adecuado de las observaciones atípicas es esencial para garantizar que el modelo represente de manera precisa y robusta el comportamiento general de los datos. Este proceso no implica simplemente eliminar valores problemáticos, sino evaluar cuidadosamente su naturaleza y decidir una estrategia adecuada para tratarlos.\nConsideramos cuatro enfoques principales para el manejo de observaciones atípicas:\n\nVerificación de datos:\n\nConfirmar si las observaciones atípicas son errores de registro o si representan casos válidos pero raros.\n\nTransformación de datos:\n\nAplicar transformaciones como logaritmos o raíces cuadradas para reducir la influencia de los valores atípicos.\n\nModelos robustos:\n\nConsiderar modelos de regresión robusta, que son menos sensibles a los valores atípicos.\n\nEliminación justificada:\n\nEn casos donde los valores atípicos son errores claros o no representativos, pueden eliminarse del análisis, siempre documentando esta decisión.\n\n\n\n\n\n1.3.8 Tratamiento de problemas\nSi el modelo de regresión lineal simple no es apropiado porque se incumplen algunas de las suposiciones, algunas opciones básicas son:\n\nAbandonar el modelo de regresión y desarrollar otro más apropiado, según las suposiciones incumplidas.\nEmplear alguna transformación de los datos de forma que el modelo de regresión sea válido para los datos transformados.\n\nTrataremos estas técnicas en próximos capítulos de este libro.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-simple-1",
    "href": "tema1.html#regresión-lineal-simple-1",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Regresión Lineal Simple",
    "text": "1.4 Regresión Lineal Simple\nLa regresión lineal simple es una de las herramientas más fundamentales y ampliamente utilizadas en el análisis estadístico. Su objetivo principal es modelar la relación entre dos variables: una variable explicativa (independiente) y una variable respuesta (dependiente). Este modelo permite no solo describir cómo se relacionan estas dos variables, sino también realizar predicciones basadas en dicha relación.\nEl concepto básico de la regresión lineal simple es ajustar una recta que minimice las discrepancias entre los valores observados y los predichos por el modelo. La ecuación general de este modelo es:\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon,\n\\] donde:\n- \\(Y\\) es la variable dependiente o respuesta.\n- \\(X\\) es la variable independiente o explicativa.\n- \\(\\beta_0\\) es el intercepto, que representa el valor de \\(Y\\) cuando \\(X = 0\\).\n- \\(\\beta_1\\) es la pendiente, que indica el cambio esperado en \\(Y\\) por cada unidad de cambio en \\(X\\).\n- \\(\\varepsilon\\) es un término de error que captura la variabilidad no explicada por el modelo.\nLa regresión lineal simple es fundamental en estadística y ciencia de datos porque proporciona un marco intuitivo y matemáticamente riguroso para analizar relaciones. Algunas aplicaciones comunes incluyen:\n\nPredecir valores futuros, como ingresos o gastos, en función de un predictor.\n\nEvaluar el impacto de una variable en otra, como el efecto de la inversión publicitaria en las ventas.\n\nExplorar relaciones lineales entre variables en estudios científicos o sociales.\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n1.5 Propiedades clave\nEl modelo de regresión lineal simple se basa en ciertos supuestos, como:\n\nUna relación lineal entre \\(X\\) y \\(Y\\).\n\nIndependencia de los términos de error.\n\nVarianza constante de los errores (homocedasticidad).\n\nNormalidad de los errores para inferencias estadísticas.\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.\n\n\n\nEn esta sección, exploraremos los fundamentos de la regresión lineal simple, desde su formulación teórica hasta su implementación práctica. Veremos cómo ajustar este modelo, interpretar sus parámetros, y evaluar su adecuación utilizando herramientas estadísticas y gráficas. Además, se presentarán ejemplos prácticos para ilustrar su aplicación en situaciones reales.\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#propiedades-clave",
    "href": "tema1.html#propiedades-clave",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Propiedades clave",
    "text": "1.4 Propiedades clave\nEl modelo de regresión lineal simple se basa en ciertos supuestos, como:\n\nUna relación lineal entre \\(X\\) y \\(Y\\).\n\nIndependencia de los términos de error.\n\nVarianza constante de los errores (homocedasticidad).\n\nNormalidad de los errores para inferencias estadísticas.\n\nEstos supuestos son esenciales para garantizar la validez de las estimaciones y conclusiones derivadas del modelo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#diagnóstico-del-modelo.-análisis-de-residuos",
    "href": "tema1.html#diagnóstico-del-modelo.-análisis-de-residuos",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Diagnóstico del Modelo. Análisis de Residuos",
    "text": "1.4 Diagnóstico del Modelo. Análisis de Residuos\nEl diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación de los datos a los supuestos de la regresión lineal.\nRecordemos que los residuos del modelo (\\(e_i\\)) son las diferencias entre los valores observados (\\(Y_i\\)) y los valores predichos (\\(\\hat{Y}_i\\)) por el modelo: \\[\ne_i = Y_i - \\hat{Y}_i.\n\\] Representan la parte de la variabilidad en \\(Y\\) no explicada por el modelo.\nEn ocasiones, es preferible trabakar con los residuos estandarizados, que tienen media cero y varianza aproximadamente unidad: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE}},  \\hspace{0.5cm} i=1,\\ldots,n.\n\\] Otro tipo de residuos habitual es el de los llamados residuos estudentizados: \\[\nd_i = \\frac{e_i}{\\sqrt{MSE \\left ( 1-\\frac{1}{n}-\\frac{(X_i-\\bar{X})^2}{S_xx} \\right )}}, \\hspace{0.5cm} i=1,\\ldots,n.\n\\]\nEstos residuos son preferibles a los estandarizados cuando \\(n\\) es pequeño.\n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema1.html#regresión-lineal-múltiple",
    "href": "tema1.html#regresión-lineal-múltiple",
    "title": "1  Modelo de regresión lineal: simple y múltiple",
    "section": "1.4 Regresión lineal múltiple",
    "text": "1.4 Regresión lineal múltiple\nEl modelo de regresión lineal múltiple es una extensión natural del modelo de regresión lineal simple, diseñada para analizar la relación entre una variable respuesta (\\(Y\\)) y múltiples variables explicativas (\\(X_1, X_2, \\dots, X_p\\)). Este modelo permite capturar interacciones más complejas entre las variables y mejora la capacidad de explicar y predecir fenómenos en contextos donde una sola variable explicativa no es suficiente para describir la variabilidad en \\(Y\\).\nLa ecuación general del modelo de regresión lineal múltiple se expresa como: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon,\n\\] donde:\n\n\\(\\beta_0\\): Intercepto, representa el valor esperado de \\(Y\\) cuando todas las variables explicativas son cero.\n\\(\\beta_1, \\beta_2, \\dots, \\beta_p\\): Coeficientes de regresión, que miden el efecto promedio de cada variable explicativa sobre \\(Y\\), manteniendo las demás constantes.\n\\(\\varepsilon\\): Término de error aleatorio, que captura la variabilidad en \\(Y\\) no explicada por las variables \\(X_1, X_2, \\dots, X_p\\).\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nGeneramos nuevos datos para regresión lineal múltiple.\n\n# Generar dos nuevas variables explicativas\n# Establecer la semilla para reproducibilidad\nset.seed(123)\n\n# Generar datos base\nn &lt;- 100\ntiempo_estudio &lt;- round(runif(n, min = 5, max = 40), 1)  # Tiempo de estudio (horas/semana)\nbeta_0_true &lt;- 5  # Intercepto verdadero\nbeta_1_true &lt;- 0.1  # Pendiente verdadera\nsigma &lt;- 0.5  # Desviación estándar del ruido\n\n# Generar calificaciones dependientes del tiempo de estudio con ruido\ncalificaciones &lt;- round(beta_0_true + beta_1_true * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)\n\n\ndistraccion &lt;- sample(0:1, size = n, replace = TRUE)  # Nivel de distracción (0 bajo, 1 alto)\ninteracciones_clase &lt;- sample(0:20, size = n, replace = TRUE)  # Interacciones en clase\n\n# Crear un data frame\ndatos &lt;- data.frame(\n  Tiempo_Estudio = tiempo_estudio,\n  Distraccion = distraccion,\n  Interacciones_Clase = interacciones_clase,\n  Calificaciones = calificaciones\n)\n\n# Mostrar los primeros registros\nhead(datos)\n\n  Tiempo_Estudio Distraccion Interacciones_Clase Calificaciones\n1           15.1           1                  14           6.64\n2           32.6           1                   2           8.25\n3           19.3           0                   8           6.91\n4           35.9           1                   6           9.27\n5           37.9           0                   8           8.68\n6            6.6           0                   3           6.42\n\n\n\n\n\nEl modelo de regresión lineal múltiple es fundamental para analizar fenómenos que dependen de múltiples factores, ya que permite explicar variabilidad compleja al identificar la contribución individual de cada variable explicativa. Al incluir diversas variables relevantes, este modelo mejora significativamente la precisión de las predicciones en comparación con modelos más simples. Además, proporciona una herramienta poderosa para controlar variables confusoras, evaluando el efecto de cada variable explicativa mientras ajusta por la influencia de las demás, lo que garantiza un análisis más robusto y detallado.\nEl modelo de regresión lineal múltiple se basa en los mismos supuestos fundamentales que el modelo simple: linealidad, independencia, homocedasticidad y normalidad de los errores. Sin embargo, la presencia de múltiples variables explicativas introduce nuevos desafíos, como la multicolinealidad, que deben ser diagnosticados y manejados para garantizar la fiabilidad del modelo.\nEn esta sección, exploraremos los fundamentos teóricos del modelo de regresión lineal múltiple, sus aplicaciones prácticas y las técnicas de diagnóstico necesarias para evaluar su calidad y validez. También abordaremos cómo interpretar los coeficientes y realizar predicciones útiles en contextos reales.\n\n1.4.1 Variables regresoras cualitativas\nEl modelo de regresión lineal múltiple no se limita al uso de variables cuantitativas, sino que también permite incorporar variables predictoras cualitativas. Por ejemplo, se pueden incluir características como el género (masculino o femenino) o el grado de satisfacción (nada satisfecho, poco satisfecho, satisfecho, muy satisfecho). Para representar estas variables en el modelo de regresión lineal, se utilizan variables binarias que indican a qué categoría pertenece cada individuo.\nPor ejemplo, consideremos un modelo de regresión lineal para predecir las calificaciones de los estudiantes, \\(Y\\) , en función del tiempo de estudio \\(X_1\\), las interacciones en clase, \\(X_2\\) y el grado de distracción \\(X_3\\).\nEn este caso definimos \\(X_3\\) como sigue:\n\\[\nX_3 =\n\\begin{cases}\n0 & \\text{si el grado de distracción es bajo} \\\\\n1 & \\text{si el grado de distracción es alto}\n\\end{cases}\n\\] En general, una variable cualitativa con \\(c\\) niveles, se puede codificar mediante \\(c − 1\\) variables binarias o variables indicadoras (también llamadas dummy).\nTrataremos ampliamente el tema de creación de variables en temas posteriores.\n\n\n1.4.2 Estimación de los coeficientes de regresión\nEs más conveniente trabajar con los modelos de regresión lineal múltiple si los expresamos en notación matricial. En ese caso, el modelo de regresión lineal múltiple se escribe como:\n\\[\n\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\]\ndonde:\n\n\\(\\mathbf{Y}\\) es el vector de observaciones de la variable dependiente (respuesta), de dimensión \\(n \\times 1\\): \\[\n  \\mathbf{Y} =\n  \\begin{bmatrix}\n  Y_1 \\\\\n  Y_2 \\\\\n  \\vdots \\\\\n  Y_n\n  \\end{bmatrix}.\n  \\]\n\\(\\mathbf{X}\\) es la matriz de diseño de las variables explicativas (predictoras), de dimensión \\(n \\times (p+1)\\), donde \\(p\\) es el número de variables explicativas. Incluye una columna de unos para el término independiente (\\(\\beta_0\\)): \\[\n  \\mathbf{X} =\n  \\begin{bmatrix}\n  1 & X_{11} & X_{12} & \\cdots & X_{1p} \\\\\n  1 & X_{21} & X_{22} & \\cdots & X_{2p} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & X_{n1} & X_{n2} & \\cdots & X_{np}\n  \\end{bmatrix}.\n  \\]\n\\(\\boldsymbol{\\beta}\\) es el vector de coeficientes de regresión, de dimensión \\((p+1) \\times 1\\): \\[\n  \\boldsymbol{\\beta} =\n  \\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_p\n  \\end{bmatrix}.\n  \\]\n\\(\\boldsymbol{\\varepsilon}\\) es el vector de errores aleatorios, de dimensión \\(n \\times 1\\): \\[\n  \\boldsymbol{\\varepsilon} =\n  \\begin{bmatrix}\n  \\varepsilon_1 \\\\\n  \\varepsilon_2 \\\\\n  \\vdots \\\\\n  \\varepsilon_n\n  \\end{bmatrix}.\n  \\]\n\nEn esta formulación, el modelo expresa que cada observación de la variable dependiente es una combinación lineal de las variables predictoras, más un término de error aleatorio. La notación matricial es compacta y facilita la manipulación algebraica para obtener estimaciones y realizar inferencias.\nComo en el caso de la regresión lineal simple, asumimos que los errores \\(\\varepsilon\\) son i.i.d. según una normal:\n\\[\n\\boldsymbol{\\varepsilon} \\overset{i.i.d.}{\\sim} \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}),\n\\]\ndonde \\(\\mathbf{I}\\) denota la matriz identidad,\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{pmatrix}.\n\\]\nEl criterio de mínimos cuadrados es el método más común para estimar los parámetros del modelo de regresión lineal múltiple. Este método busca minimizar la suma de los cuadrados de los residuos. Es decir, la suma de los cuadrados de las diferencias entre los valores observados de la variable dependiente y los valores predichos por el modelo.\nEl objetivo es estimar \\(\\boldsymbol{\\beta}\\), el conjunto de coeficientes de regresión.\nLa función de pérdida que se minimiza es la suma de los cuadrados de los residuos (\\(S(\\boldsymbol{\\beta})\\)): \\[\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^2,\n\\] o en notación matricial: \\[\nS(\\boldsymbol{\\beta}) = (\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^\\top (\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}),\n\\] donde \\((\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\) es el vector de residuos.\nPara encontrar el valor de \\(\\boldsymbol{\\beta}\\) que minimiza \\(S(\\boldsymbol{\\beta})\\), derivamos \\(S(\\boldsymbol{\\beta})\\) con respecto a \\(\\boldsymbol{\\beta}\\) y la igualamos a cero:\n\nExpandimos \\(S(\\boldsymbol{\\beta})\\): \\[\nS(\\boldsymbol{\\beta}) = \\mathbf{Y}^\\top \\mathbf{Y} - 2 \\mathbf{Y}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\nDerivamos con respecto a \\(\\boldsymbol{\\beta}\\): \\[\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2 \\mathbf{X}^\\top \\mathbf{Y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\nIgualamos a cero: \\[\n-2 \\mathbf{X}^\\top \\mathbf{Y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = 0.\n\\]\nResolviendo para \\(\\boldsymbol{\\beta}\\): \\[\n\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\nFinalmente, el estimador de mínimos cuadrados es: \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}.\n\\]\n\nEl criterio de mínimos cuadrados ajusta un plano en el espacio multidimensional de las variables explicativas que mejor representa los datos en términos de minimizar los residuos al cuadrado. Este estimador es ampliamente usado por su simplicidad y eficacia bajo los supuestos de normalidad y homocedasticidad.\n\n\n\n\n\n\nPara recordar\n\n\n\nCuando se trata de un modelo de regresión con múltiples regresores, el análisis visual mediante diagramas de dispersión puede resultar engañoso. Construir gráficos que muestren la relación entre la variable respuesta y cada regresor de forma aislada no siempre proporciona una representación precisa de la realidad. Esto ocurre porque los efectos conjuntos de los regresores no se reflejan en estos gráficos individuales. Incluso en un caso ideal, donde existe una relación perfecta entre la variable respuesta y los regresores, sin ningún tipo de ruido o error, los diagramas de dispersión individuales pueden sugerir una relación débil o inexistente si no se tiene en cuenta el contexto multivariado.\nEn situaciones más comunes, donde los datos incluyen errores en los valores observados de la variable respuesta y los regresores interactúan de manera compleja, los gráficos univariantes tienden a complicar aún más la interpretación. Pueden surgir aparentes inconsistencias o patrones confusos, ya que no se está considerando la influencia simultánea de todos los regresores en la respuesta. Por esta razón, es fundamental adoptar enfoques que analicen las relaciones de manera conjunta, como los coeficientes estimados en un modelo de regresión múltiple, que capturan el impacto parcial de cada regresor ajustando por los efectos de los demás.\n\n\n\n1.4.2.1 Propiedades del estimador\nEstas son las propiedades del estimador de mínimos cuadrados:\n\nLinealidad: El estimador es una combinación lineal de los valores observados (\\(\\mathbf{Y}\\)).\nInsesgado: En ausencia de errores correlacionados y bajo supuestos estándar, \\(E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\\).\nVarianza mínima: Entre los estimadores lineales insesgados, \\(\\hat{\\boldsymbol{\\beta}}\\) tiene la menor varianza posible (propiedad BLUE: Best Linear Unbiased Estimator).\n\n\n\n\n1.4.3 Varianza del error\nLa varianza de los errores (\\(\\sigma^2\\)) mide cuánto varían las observaciones reales (\\(Y_i\\)) con respecto a los valores predichos por el modelo (\\(\\hat{Y}_i\\)). En el modelo de regresión múltiple, esta varianza puede ser estimada utilizando la suma de los cuadrados de los residuos (SSE).\nLa estimación de \\(\\sigma^2\\), denotada como \\(\\hat{\\sigma}^2\\), se calcula como: \\[\n\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n - p - 1}, \\] donde:\n\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\mathbf{e}^\\top \\mathbf{e}\\) es la suma de los cuadrados de los residuos.\n\\(\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}} = \\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\) es el vector de residuos.\n\nLa cantidad \\(n - p - 1\\) es conocida como los grados de libertad residuales, que reflejan el número de datos disponibles para estimar la varianza de los errores después de ajustar los parámetros del modelo.\nEn términos matriciales, \\(\\hat{\\sigma}^2\\) se expresa como: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - p - 1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}})^\\top (\\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}),\n\\] o de forma más compacta: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - p - 1} \\mathbf{e}^\\top \\mathbf{e}.\n\\]\nLa varianza estimada \\(\\hat{\\sigma}^2\\) se encuentra en las mismas unidades al cuadrado que la variable dependiente (\\(\\mathbf{Y}\\)). Este valor es crucial para construir intervalos de confianza y realizar pruebas de hipótesis sobre los coeficientes del modelo (\\(\\boldsymbol{\\beta}\\)). La raíz cuadrada de \\(\\hat{\\sigma}^2\\) proporciona el error estándar de los residuos.\nEste valor permite cuantificar la variabilidad no explicada por el modelo ajustado y es un componente esencial en la evaluación de la calidad del ajuste.\n\n\n1.4.4 Inferencia sobre los parámetros del modelo\nTal como indicamos en el modelo de regresión simple, en el análisis de regresión lineal múltiple, la inferencia estadística permite evaluar la significancia de los parámetros estimados y del modelo completo. Estas evaluaciones son cruciales para determinar si el modelo ajustado es útil para explicar y predecir la variable respuesta.\n\n1.4.4.1 Intervalos de confianza para los coeficientes de regresión\nLos intervalos de confianza proporcionan un rango plausible para los valores verdaderos de los coeficientes de regresión (\\(\\beta_j\\)), permitiendo evaluar su significancia.\nPara construir intervalos de confianza para los coeficientes de la regresión, \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\), procedemos de forma análoga a como hacíamos en el caso de la regresión lineal simple. Usamos la distribución en el muestreo de los estimadores, \\(\\hat{\\beta}\\): \\[\n\\hat{\\beta} \\sim N_p(\\beta, \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}).\n\\]\nEsto implica que cada coeficiente \\(\\hat{\\beta}_j\\) es normal con media \\(\\beta_j\\) y varianza \\(\\sigma^2 C_{jj}\\), con \\(C_{jj}\\) el elemento \\(j\\)-ésimo de la diagonal de la matriz \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\).\nDe este modo, los estadísticos: \\[\nt_j=\\frac{\\hat{\\beta_j}-\\beta_j}{\\sqrt{\\hat{\\sigma^2}C_{jj}}} \\hspace{0.5cm} j=0,1,\\ldots,p\n\\] se distribuye como una t-Student, con \\(n-p\\) grados de libertad.\nPo tanto, el intervalo de confianza para cada coeficiente \\(\\beta_j\\) se calcula como:\n\\[\n\\hat{\\beta}_j \\pm t_{n-p-1,\\alpha/2} \\cdot \\text{SE}(\\hat{\\beta}_j) \\hspace{0.5cm} j=0,1,\\ldots,p\n\\]\ndonde:\n\\[\n  \\text{SE}(\\hat{\\beta}_j) = \\sqrt{\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}_{jj}}.\n  \\]\nes el error estándar del estimador \\(\\hat{\\beta}_j\\).\n\n\n1.4.4.2 Contraste global sobre lo significativo que es el modelo de regresión lineal múltiple\nEl contraste global evalúa si el modelo de regresión, como un todo, explica una proporción significativa de la variabilidad de la variable respuesta. Esto se realiza mediante un test F y una tabla ANOVA.\nLa tabla ANOVA para la regresión múltiple descompone la variabilidad total de la variable respuesta (\\(\\text{SCT}\\)) en dos componentes principales:\n\nSuma de Cuadrados del Modelo de Regresión (SSR): Variabilidad explicada por los regresores.\nSuma de Cuadrados de los Residuos (SSE): Variabilidad no explicada por el modelo.\n\nLa descomposición se expresa como: \\[\n\\text{SST} = \\text{SSE} + \\text{SSR}.\n\\]\nLos términos involucrados son:\n\n\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2\\), donde \\(\\hat{Y}_i\\) son los valores predichos por el modelo y \\(\\bar{Y}\\) es la media de \\(Y\\).\n\\(\\text{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\), la suma de los cuadrados de los residuos.\n\\(\\text{SST} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\), la suma total de los cuadrados.\n\nLa tabla ANOVA incluye los siguientes elementos:\n\n\n\n\n\n\n\n\n\n\nFuente de Variación\nSuma de Cuadrados\nGrados de Libertad\nCuadrados Medios\nEstadístico F\n\n\n\n\nModelo\n\\(\\text{SSR}\\)\n\\(p\\)\n\\(MSR=\\text{SSR}/p\\)\n\\(F = \\frac{\\text{SCM}/p}{\\text{SCR}/(n-p-1)}\\)\n\n\nResiduos\n\\(\\text{SSE}\\)\n\\(n-p-1\\)\n\\(MSE=\\text{SSE}/(n-p-1)\\)\n\n\n\nTotal\n\\(\\text{SST}\\)\n\\(n-1\\)\n\n\n\n\n\nLa hipótesis del test F global es:\n\n\\(H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\) (el modelo no es significativo),\n\\(H_1:\\) al menos uno de los \\(\\beta_j \\neq 0\\).\n\nEl estadístico F se calcula como: \\[\nF = \\frac{\\text{SSR}/p}{\\text{SSE}/(n-p-1)}=\\frac{MSR}{MSE}.\n\\] Se compara con un valor crítico de la distribución \\(F_{p, n-p-1}\\). Si \\(F\\) es grande o el valor \\(p\\) asociado es pequeño, se rechaza \\(H_0\\).\n\n\n1.4.4.3 Test global sobre el ajuste del modelo\nEl test global evalúa si la proporción de la variabilidad total explicada por el modelo (\\(R^2\\)) es significativa. Esto se relaciona con el test F global presentado en la sección anterior.\nEl coeficiente de determinación \\(R^2\\) se define como: \\[\nR^2 = \\frac{\\text{SSR}}{\\text{SST}}.\n\\] El test F analiza si este valor es significativamente distinto de cero, como se muestra en la tabla ANOVA.\n\n\n1.4.4.4 Test sobre coeficientes individuales\nEl objetivo aquí es determinar si cada variable explicativa tiene un efecto significativo en la variable respuesta, después de ajustar por las demás.\nPara cada coeficiente \\(\\beta_j\\):\n\n\\(H_0: \\beta_j = 0\\) (la variable \\(X_j\\) no contribuye al modelo),\n\\(H_1: \\beta_j \\neq 0\\) (la variable \\(X_j\\) tiene un efecto significativo).\n\nEl estadístico \\(t\\) para cada \\(\\beta_j\\) se calcula como: \\[\nt_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)},\n\\]\ndonde \\(\\text{SE}(\\hat{\\beta}_j)\\) es el error estándar del coeficiente. Este estadístico sigue una distribución \\(t_{n-p-1}\\).\nSe compara \\(|t_j|\\) con el valor crítico de \\(t\\) para un nivel de significancia \\(\\alpha\\), o se evalúa el valor \\(p\\). Si \\(|t_j|\\) es suficientemente grande (o \\(p\\) es pequeño), se rechaza \\(H_0\\), indicando que \\(X_j\\) contribuye significativamente al modelo.\nEl test sobre los coeficientes individuales permite identificar qué variables son más importantes en la explicación de la variabilidad de la variable respuesta, ajustando por las demás. Esto ayuda a interpretar y refinar el modelo.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:\n\n# Ajustar el modelo de regresión lineal\nmodelo_regresion &lt;- lm(Calificaciones ~ Tiempo_Estudio + Distraccion + Interacciones_Clase, data = datos)\n\n# Mostrar los resultados del modelo\nsummary(modelo_regresion)\n\n\nCall:\nlm(formula = Calificaciones ~ Tiempo_Estudio + Distraccion + \n    Interacciones_Clase, data = datos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05269 -0.30234 -0.00226  0.27892  1.15358 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         4.829436   0.148491  32.524   &lt;2e-16 ***\nTiempo_Estudio      0.097585   0.004902  19.906   &lt;2e-16 ***\nDistraccion         0.075166   0.098742   0.761   0.4484    \nInteracciones_Clase 0.015429   0.008310   1.857   0.0664 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4801 on 96 degrees of freedom\nMultiple R-squared:  0.8141,    Adjusted R-squared:  0.8083 \nF-statistic: 140.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\nDraper, NR. 1998. Applied regression analysis. McGraw-Hill. Inc.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelo de regresión lineal: simple y múltiple</span>"
    ]
  },
  {
    "objectID": "tema2.html",
    "href": "tema2.html",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "2.1 Proceso de construcción del modelo de regresión\nLa construcción de un modelo de regresión múltiple es un proceso sistemático que busca explicar la relación entre una variable respuesta (\\(Y\\)) y múltiples variables predictoras (\\(X_1, X_2, \\dots, X_k\\)). Este proceso consta de varias etapas clave (Kutner et al. 2005):\nEl objetivo principal de este tema es presentar las técnicas más relevantes para la selección de variables y regularización, entender sus fundamentos teóricos, y aplicarlas a casos prácticos. Esto no solo permitirá construir modelos más robustos y eficientes, sino que también ayudará a obtener insights más claros y útiles a partir de los datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-de-selección-directa",
    "href": "tema2.html#métodos-de-selección-directa",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "2.4 Métodos de selección directa",
    "text": "2.4 Métodos de selección directa\nLos métodos de selección directa son un enfoque fundamental en la búsqueda de un subconjunto óptimo de variables predictoras en modelos de regresión. Este enfoque evalúa de manera sistemática diferentes combinaciones de variables para identificar cuál de ellas proporciona el mejor ajuste al modelo en función de un criterio predefinido, como el coeficiente de determinación ajustado (\\(R^2\\) ajustado), el error cuadrático medio (ECM) o criterios de información como AIC o BIC.\nA diferencia de los métodos automáticos, los métodos de selección directa no dependen de un proceso iterativo de adición o eliminación de variables. En cambio, buscan exhaustivamente (o mediante aproximaciones computacionalmente más eficientes) entre todas las posibles combinaciones de variables, lo que garantiza un análisis completo de las interacciones y relevancias potenciales.\nEstos métodos son especialmente útiles cuando el número de predictores no es demasiado grande, ya que el esfuerzo computacional crece exponencialmente con el número de variables. Aunque el costo computacional puede ser elevado en datasets amplios, los métodos de selección directa proporcionan una referencia sólida y transparente para evaluar qué variables son fundamentales en el modelo.\nEn esta sección, analizaremos los métodos de selección directa más comunes, su implementación práctica y las métricas utilizadas para comparar modelos, destacando sus ventajas y limitaciones.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-automáticos",
    "href": "tema2.html#métodos-automáticos",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "2.5 Métodos automáticos",
    "text": "2.5 Métodos automáticos\nLos métodos automáticos de selección de variables son herramientas prácticas y eficientes diseñadas para identificar subconjuntos relevantes de predictores en un modelo de regresión. A diferencia de los métodos de selección directa, que exploran exhaustivamente todas las combinaciones posibles de variables, los métodos automáticos siguen un enfoque iterativo que simplifica el proceso de selección. Estos métodos son especialmente útiles en situaciones donde el número de predictores es elevado, ya que reducen significativamente el esfuerzo computacional.\nEl principio clave detrás de los métodos automáticos es el ajuste dinámico del conjunto de variables en función de criterios estadísticos, como \\(p\\)-valores, coeficientes de determinación ajustados (\\(R^2\\)) ajustado), o criterios de información como AIC y BIC. Entre las estrategias más comunes se encuentran:\n\nMétodo Forward (selección progresiva): Parte de un modelo vacío e incorpora variables de manera secuencial, añadiendo en cada paso la variable que mejora más el modelo.\nMétodo Backward (eliminación regresiva): Comienza con todas las variables en el modelo y elimina iterativamente aquellas que tienen menor impacto.\nMétodo Stepwise: Combina las estrategias forward y backward, permitiendo tanto la inclusión como la exclusión de variables en cada iteración.\n\nEstos métodos ofrecen una manera estructurada y ágil de seleccionar variables, aunque no garantizan encontrar el mejor modelo global debido a su naturaleza secuencial. A lo largo de esta sección, examinaremos cada uno de estos métodos, sus ventajas, limitaciones y aplicaciones en diferentes contextos de análisis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#métodos-basados-en-regularización",
    "href": "tema2.html#métodos-basados-en-regularización",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "2.6 Métodos basados en regularización",
    "text": "2.6 Métodos basados en regularización\nEn los modelos de regresión, especialmente cuando se trabaja con un gran número de variables predictoras o con datos multicolineales, los métodos tradicionales de selección de variables pueden resultar ineficaces o inestables. En estos casos, los métodos basados en regularización surgen como una alternativa poderosa que no solo selecciona variables, sino que también mejora la estabilidad y la precisión del modelo.\nLa regularización consiste en introducir una penalización en la función de ajuste del modelo, lo que tiene dos efectos principales: controlar el sobreajuste al reducir la complejidad del modelo y forzar la selección de un subconjunto más parsimonioso de predictores. Estas penalizaciones ajustan los coeficientes de las variables predictoras, favoreciendo soluciones más simples y robustas.\nEntre los métodos de regularización más destacados se encuentran:\n\nRidge Regression: Aplica una penalización proporcional al cuadrado de los coeficientes, lo que permite manejar problemas de multicolinealidad pero no conduce a la eliminación completa de variables.\nLasso (Least Absolute Shrinkage and Selection Operator): Introduce una penalización basada en el valor absoluto de los coeficientes, lo que no solo reduce su magnitud, sino que también puede anularlos completamente, realizando una selección automática de variables.\nElastic Net: Combina las penalizaciones de Ridge y Lasso, ofreciendo mayor flexibilidad en situaciones donde hay una gran correlación entre los predictores.\n\nEstos métodos son especialmente útiles en problemas donde el número de variables predictoras excede el número de observaciones, o cuando se desea un modelo más interpretable. En esta sección, exploraremos en detalle los fundamentos teóricos, la implementación práctica y las aplicaciones de cada uno de estos métodos, destacando sus ventajas en escenarios complejos y desafiantes.\n\n2.6.1 Ridge regression\nLa regresión Ridge introduce una penalización en la estimación de los coeficientes de regresión, lo que ayuda a reducir la varianza del modelo y mejora su capacidad predictiva en presencia de datos altamente correlacionados o con muchas variables (Marquardt y Snee 1975). El modelo de regresión Ridge es una extensión de la regresión lineal estándar. Dado un conjunto de datos con \\(n\\) observaciones y \\(p\\) predictores, expresamos el modelo de regresión lineal múltiple como:\n\\[\n\\mathbf{Y}= \\mathbf{X} \\beta + \\boldsymbol{\\varepsilon}\n\\]\ndonde:\n\n\\(\\mathbf{Y}\\) es el vector de respuesta de dimensión \\(n \\times 1\\).\n\\(\\mathbf{X}\\) es la matriz de diseño de dimensión \\(n \\times p\\).\n\\(\\beta\\) es el vector de coeficientes de regresión de dimensión \\(p \\times 1\\).\n\\(\\boldsymbol{\\varepsilon}\\) es el vector de errores aleatorios.\n\nEn mínimos cuadrados ordinarios (OLS), los coeficientes se estiman minimizando la suma de los errores al cuadrado:\n\\[\nSSE = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\| \\mathbf{Y} - \\mathbf{X} \\beta \\|^2.\n\\]\nSin embargo, cuando hay multicolinealidad, la matriz \\(X^T X\\) puede ser casi singular, generando coeficientes inestables. Para evitar esto, la regresión Ridge añade un término de penalización \\(\\lambda\\), de la siguiente manera:\n\\[\nSSE_{ridge} = \\| \\mathbf{Y} - \\mathbf{X} \\beta \\|^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2.\n\\]\nEste término adicional, es un término de penalización (\\(L_2=\\sum \\beta_j^2\\)) impone una restricción sobre los coeficientes, evitando que tomen valores excesivamente grandes. La estimación de \\(\\beta\\) en Ridge se obtiene resolviendo:\n\\[\n\\hat{\\beta}_{ridge} = (\\mathbf{X}^T \\mathbf{X} + \\lambda I)^{-1} \\mathbf{X}^T \\mathbf{Y}.\n\\]\ndonde \\(I\\) es la matriz identidad y \\(\\lambda \\geq 0\\) es un hiperparámetro que controla la cantidad de penalización aplicada.\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo Ridge es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, los coeficientes \\(\\beta_j\\) se reducen en magnitud, lo que ayuda a controlar la varianza del modelo y a prevenir el sobreajuste.\nSi \\(\\lambda\\) es demasiado grande, los coeficientes se acercan a cero y el modelo puede perder interpretabilidad.\n\nLa elección óptima de \\(\\lambda\\) se determina generalmente mediante validación cruzada.\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles de la validación cruzada son tratados en la asignatura de Aprendizaje Automático.\n\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la multicolinealidad: La regularización reduce la sensibilidad del modelo cuando los predictores están altamente correlacionados.\nMenor varianza en las predicciones: El modelo Ridge tiende a ser más estable en comparación con OLS, lo que mejora la capacidad de generalización en conjuntos de datos nuevos.\nNo realiza selección de variables: A diferencia de Lasso, Ridge no anula coeficientes, sino que reduce su magnitud. Esto es útil cuando se sospecha que todas las variables tienen algún grado de importancia en el modelo.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librerías\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n# Datos simulados\nset.seed(123)\nX &lt;- matrix(rnorm(100 * 10), 100, 10)  # 100 observaciones, 10 predictores\nY &lt;- X %*% rnorm(10) + rnorm(100)  # Variable de respuesta con ruido\n\n# Ajustar modelo Ridge\nmodelo_ridge &lt;- glmnet(X, Y, alpha = 0)  # alpha = 0 indica regresión Ridge\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_ridge &lt;- cv.glmnet(X, Y, alpha = 0)\nlambda_optimo &lt;- cv_ridge$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.2583753\n\n# Ajustar modelo final con lambda óptimo\nmodelo_ridge_final &lt;- glmnet(X, Y, alpha = 0, lambda = lambda_optimo)\n\nmodelo_ridge_final\n\n\nCall:  glmnet(x = X, y = Y, alpha = 0, lambda = lambda_optimo) \n\n  Df  %Dev Lambda\n1 10 93.55 0.2584\n\n# Comparación modelo clásico\n\nmodelo_lm &lt;- lm(Y~X)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_ridge_final),3),\n            round(coef(modelo_lm),3))\n\ncolnames(output)=c(\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 2 sparse Matrix of class \"dgCMatrix\"\n             RIDGE    OLS\n(Intercept)  0.118  0.132\nV1          -0.874 -0.995\nV2          -1.019 -1.131\nV3           0.040  0.039\nV4           0.002  0.001\nV5          -2.500 -2.703\nV6           1.001  1.104\nV7           0.247  0.274\nV8           2.125  2.244\nV9           0.635  0.658\nV10         -0.390 -0.427\n\n\n\n\n\n\nLa regresión Ridge es una técnica poderosa para mejorar la estabilidad de los modelos de regresión en presencia de multicolinealidad. A diferencia de OLS, que puede generar coeficientes inestables, Ridge introduce una penalización que reduce la magnitud de los coeficientes, evitando valores extremos. Aunque Ridge no realiza selección de variables, su capacidad para reducir la varianza y mejorar la capacidad predictiva lo convierte en una herramienta esencial en el análisis de datos modernos.\nEn la siguiente sección, exploraremos la regresión Lasso, que extiende este concepto permitiendo la eliminación de variables irrelevantes del modelo.\n\n\n2.6.2 Regresión Lasso\nCuando se tiene un conjunto de predictores con posibles redundancias o ruido, Lasso permite identificar cuáles son las variables más relevantes para el modelo, lo que facilita la interpretación y reduce la complejidad del análisis.\nAl igual que ocurría en Ridge Regression, el modelo de regresión Lasso se basa en la minimización de la siguiente función de error (Ranstam y Cook 2018): \\[\nSSE_{lasso} = \\| \\mathbf{Y}- \\mathbf{X} \\beta \\|^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\ndonde el término de penalización, (\\(L_1=\\sum |\\beta_j|\\)) hace que algunos coeficientes se reduzcan exactamente a cero, lo que significa que esas variables son eliminadas del modelo.\nLa diferencia clave con Ridge Regressión, visto anteriormente, es que Ridge reduce la magnitud de los coeficientes pero no los anula, mientras que Lasso puede eliminar variables por completo.\nInterpretación del parámetro \\(\\lambda\\)\n\nSi \\(\\lambda = 0\\), el modelo es equivalente a la regresión lineal tradicional (OLS).\nA medida que \\(\\lambda\\) aumenta, más coeficientes se reducen a cero, lo que equivale a realizar selección de variables.\nSi \\(\\lambda\\) es demasiado grande, se eliminan demasiadas variables, lo que puede resultar en un modelo subóptimo.\n\nAl igual que en el método Risge, la selección óptima de \\(\\lambda\\) se realiza generalmente mediante validación cruzada.\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nSelección de variables automática: Lasso no solo regulariza, sino que también selecciona las variables más importantes eliminando aquellas menos relevantes.\nManejo de la multicolinealidad: Puede mejorar la interpretación del modelo cuando hay muchas variables correlacionadas.\nSimplicidad y interpretabilidad: Un modelo con menos variables es más fácil de interpretar y aplicar en la práctica.\nReduce el sobreajuste: La penalización \\(L_1\\) evita que el modelo se ajuste demasiado a los datos de entrenamiento, mejorando su capacidad predictiva en datos nuevos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Lasso\nmodelo_lasso &lt;- glmnet(X, Y, alpha = 1)  # alpha = 1 indica regresión Lasso\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_lasso &lt;- cv.glmnet(X, Y, alpha = 1)\nlambda_optimo &lt;- cv_lasso$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.03260326\n\n# Ajustar modelo final con lambda óptimo\nmodelo_lasso_final &lt;- glmnet(X, Y, alpha = 1, lambda = lambda_optimo)\n\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_lasso_final),3),output)\n\ncolnames(output)=c(\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 3 sparse Matrix of class \"dgCMatrix\"\n             LASSO  RIDGE    OLS\n(Intercept)  0.131  0.118  0.132\nV1          -0.950 -0.874 -0.995\nV2          -1.078 -1.019 -1.131\nV3           0.006  0.040  0.039\nV4           .      0.002  0.001\nV5          -2.652 -2.500 -2.703\nV6           1.058  1.001  1.104\nV7           0.235  0.247  0.274\nV8           2.213  2.125  2.244\nV9           0.629  0.635  0.658\nV10         -0.392 -0.390 -0.427\n\n\n\n\n\nConsideraciones Importantes\nLa regresión Lasso es una poderosa técnica de regularización que no solo mejora la estabilidad del modelo en presencia de muchas variables predictoras, sino que también realiza una selección automática de las más relevantes. Su capacidad para reducir coeficientes a cero la convierte en una herramienta esencial en el análisis de datos de alta dimensión.\n\nLasso puede eliminar demasiadas variables si \\(\\lambda\\) es demasiado grande, lo que puede llevar a la pérdida de información importante.\nNo maneja bien grupos de predictores altamente correlacionados, ya que selecciona solo uno de ellos y elimina los demás.\nElastic Net, que combina Ridge y Lasso, puede ser una mejor opción cuando hay multicolinealidad fuerte en los datos.\n\nEn la siguiente sección, exploraremos Elastic Net, una técnica híbrida que combina las ventajas de Ridge y Lasso para mejorar la selección de variables en presencia de predictores altamente correlacionados.\n\n\n2.6.3 Elastic Net\nLa regresión Elastic Net es una técnica de regularización que combina las propiedades de Ridge y Lasso, abordando algunas de sus limitaciones individuales (Zou y Hastie 2005). Mientras que Ridge es útil para manejar la multicolinealidad sin eliminar variables y Lasso selecciona un subconjunto de predictores, Elastic Net equilibra ambos enfoques permitiendo la selección de variables en presencia de alta correlación entre los predictores.\nEste método es particularmente efectivo cuando el número de predictores es grande y existe multicolinealidad, ya que permite controlar simultáneamente la reducción de la magnitud de los coeficientes y la eliminación de variables irrelevantes.\nElastic Net introduce una penalización que combina los términos de Ridge (\\(L_2\\)) y Lasso (\\(L_1\\)):\n\\[\nSSE_{\\text{Elastic Net}} = \\| Y - X \\beta \\|^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n\\]\ndonde:\n\n\\(\\lambda_1\\) (asociado a Lasso) controla la cantidad de coeficientes que se reducen a cero.\n\\(\\lambda_2\\) (asociado a Ridge) controla la reducción de magnitud de los coeficientes sin anularlos.\n\\(\\alpha\\) es un parámetro adicional que pondera la combinación entre Lasso y Ridge, con:\n\n\\(\\alpha = 1\\) → Elastic Net se comporta como Lasso.\n\\(\\alpha = 0\\) → Elastic Net se comporta como Ridge.\n\\(0 &lt; \\alpha &lt; 1\\) → Elastic Net combina ambos métodos.\n\n\nLa estimación de los coeficientes en Elastic Net se obtiene resolviendo:\n\\[\n\\hat{\\beta}_{\\text{Elastic Net}} = \\arg \\min_{\\beta} \\left( \\| Y - X \\beta \\|^2 + \\lambda \\left( \\alpha \\sum |\\beta_j| + (1 - \\alpha) \\sum \\beta_j^2 \\right) \\right)\n\\]\n\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\n\nManejo de la Multicolinealidad: A diferencia de Lasso, que selecciona solo una de las variables correlacionadas y elimina las demás, Elastic Net distribuye la penalización entre todas las variables correlacionadas, evitando una selección arbitraria.\nSelección de variables más estable: La combinación de Lasso y Ridge permite una selección más robusta, manteniendo información relevante del modelo sin eliminar predictores clave.\nMejora del rendimiento predictivo: Al utilizar validación cruzada para seleccionar los hiperparámetros \\(\\lambda_1\\), \\(\\lambda_2\\) y \\(\\alpha\\), se optimiza la capacidad del modelo para generalizar a nuevos datos.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ajustar modelo Elastic Net\nmodelo_elastic_net &lt;- glmnet(X, Y, alpha = 0.5)  # Alpha = 0.5 (50% Ridge, 50% Lasso)\n\n# Seleccionar lambda óptimo con validación cruzada\ncv_elastic_net &lt;- cv.glmnet(X, Y, alpha = 0.5)\nlambda_optimo &lt;- cv_elastic_net$lambda.min  # Mejor valor de lambda\n\nprint(lambda_optimo)\n\n[1] 0.0213522\n\n# Ajustar modelo final con lambda óptimo\nmodelo_elastic_final &lt;- glmnet(X, Y, alpha = 0.5, lambda = lambda_optimo)\n\n# Mostrar coeficientes\noutput=cbind(round(coef(modelo_elastic_final),3),output)\n\ncolnames(output)=c(\"ELASTIC\",\"LASSO\",\"RIDGE\",\"OLS\")\n\noutput\n\n11 x 4 sparse Matrix of class \"dgCMatrix\"\n            ELASTIC  LASSO  RIDGE    OLS\n(Intercept)   0.131  0.131  0.118  0.132\nV1           -0.975 -0.950 -0.874 -0.995\nV2           -1.108 -1.078 -1.019 -1.131\nV3            0.028  0.006  0.040  0.039\nV4            .      .      0.002  0.001\nV5           -2.677 -2.652 -2.500 -2.703\nV6            1.084  1.058  1.001  1.104\nV7            0.260  0.235  0.247  0.274\nV8            2.229  2.213  2.125  2.244\nV9            0.647  0.629  0.635  0.658\nV10          -0.414 -0.392 -0.390 -0.427\n\n\n\n\n\nPara determinar el mejor valor de \\(\\alpha\\), se usa validación cruzada probando distintos valores entre \\(0\\) y 1. Algunas estrategias comunes incluyen:\n\nSi hay muchas variables irrelevantes, se recomienda \\(\\alpha\\) cercano a 1 (Lasso).\nSi hay fuerte multicolinealidad, se recomienda \\(\\alpha\\) cercano a 0 (Ridge).\nSi se desea un balance entre selección y estabilidad, se suele usar \\(\\alpha = 0.5\\).\n\nLa regresión Elastic Net combina lo mejor de Ridge y Lasso, ofreciendo un método de regularización robusto para modelos con muchas variables predictoras y posible multicolinealidad. Su capacidad para seleccionar variables sin eliminar información clave lo convierte en una opción ideal para modelos complejos y de alta dimensionalidad.\n\n\n2.6.4 Comparación de los métodos de Regularización\n\n\n\n\n\n\n\n\nMétodo\nPenalización\nEfecto sobre los coeficientes\n\n\n\n\nOLS\nNinguna\nSin restricción, puede haber multicolinealidad\n\n\nRidge\n\\(L_2\\)\nReduce la magnitud de los coeficientes, pero no los anula\n\n\nLasso\n\\(L_1\\)\nPuede anular coeficientes, permitiendo selección de variables\n\n\nElastic Net\n\\(L_1 + L_2\\)\nCombinación de Ridge y Lasso\n\n\n\n\nLasso es especialmente útil cuando se sospecha que muchas variables son irrelevantes, mientras que Ridge es preferido cuando se espera que todas las variables aporten información al modelo.\nElastic Net es ideal cuando hay muchas variables correlacionadas y se desea un modelo estable y parsimonioso.\n\nElastic Net mejora la estabilidad del modelo en comparación con Lasso, especialmente cuando hay variables predictoras altamente correlacionadas.\nEs más flexible que Ridge y Lasso individualmente, permitiendo un ajuste más fino a distintos tipos de problemas.\nRequiere la selección de hiperparámetros (\\(\\lambda\\) y \\(\\alpha\\)), por lo que debe usarse validación cruzada para encontrar la combinación óptima.\n\n\n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, y William Li. 2005. Applied linear statistical models. McGraw-hill.\n\n\nMarquardt, Donald W, y Ronald D Snee. 1975. «Ridge regression in practice». The American Statistician 29 (1): 3-20.\n\n\nRanstam, Jonas, y Jonathan A Cook. 2018. «LASSO regression». Journal of British Surgery 105 (10): 1348-48.\n\n\nZou, Hui, y Trevor Hastie. 2005. «Regularization and variable selection via the elastic net». Journal of the Royal Statistical Society Series B: Statistical Methodology 67 (2): 301-20.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#resumen-del-proceso-de-construcción-del-modelo-de-regresión-múltiple",
    "href": "tema2.html#resumen-del-proceso-de-construcción-del-modelo-de-regresión-múltiple",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "Definición del problema y variables de interés:\n\nIdentificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.\nSeleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.\n\nAnálisis Exploratorio de Datos (EDA):\n\nInspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.\nEscalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.\n\nAjuste del modelo:\n\nEspecificar el modelo de regresión múltiple en su forma general:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon,\n\\] donde \\(\\varepsilon\\) representa los errores aleatorios.\nEstimar los coeficientes del modelo (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.\n\nEvaluación del modelo:\n\nAnalizar el ajuste general del modelo utilizando métricas como \\(R^2\\) y \\(R^2\\) ajustado, que miden la proporción de la variabilidad explicada.\nExaminar la tabla ANOVA para evaluar la significancia global del modelo.\nRealizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.\n\nDiagnóstico del modelo:\n\nExaminar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.\nIdentificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.\n\nSelección de variables:\n\nSimplificar el modelo eliminando variables irrelevantes mediante métodos de selección de variables (directos, automáticos o por regularización) para mejorar la parsimonia y evitar el sobreajuste.\nLos métodos de selección de variables permiten identificar subconjuntos óptimos de predictores, mejorando tanto la simplicidad como la precisión del modelo. Además, discutiremos los problemas de regularización, donde se introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste. Estos métodos, como Ridge, Lasso y Elastic Net, se han convertido en herramientas esenciales en el análisis de datos modernos.\n\nValidación del modelo:\n\nEvaluar el desempeño del modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#proceso-de-construcción-del-modelo-de-regresión",
    "href": "tema2.html#proceso-de-construcción-del-modelo-de-regresión",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "",
    "text": "Definición del problema y variables de interés:\n\nIdentificar claramente el objetivo del análisis, ya sea realizar predicciones, evaluar relaciones o controlar por efectos de variables confusoras.\nSeleccionar las variables predictoras potenciales en función de su relevancia teórica, conocimiento previo o exploración inicial de los datos.\n\nRecogida de datos:\n\n\nLa calidad de los datos recogidos influye directamente en la validez de los resultados y conclusiones obtenidas. El proceso de recogida de datos consiste en recopilar información de manera organizada y sistemática para responder a las preguntas de investigación planteadas. Dependiendo del diseño del estudio y los objetivos del análisis, se pueden emplear diferentes tipos de experimentos o métodos de recogida de datos.\nDebemos asegurar las siguientes características sobre los datos.\n\nFiabilidad: Asegurar que los datos sean consistentes y puedan reproducirse bajo condiciones similares.\nValidez: Garantizar que los datos recojan realmente la información necesaria para responder a las preguntas de investigación.\nÉtica: Asegurar la privacidad y el consentimiento informado de los participantes.\nControl de Sesgos: Diseñar el estudio de manera que se minimicen los sesgos que puedan distorsionar los resultados.\n\n\n\n\n\n\n\n\nTipos de experimentos\n\n\n\n\n\nLa elección del tipo de experimento o método de recogida de datos dependerá de la naturaleza del problema a investigar, los recursos disponibles y las limitaciones del estudio. Una correcta planificación y ejecución de esta etapa sienta las bases para un análisis robusto y confiable.\n\nExperimentos controlados:\n\nLos experimentos controlados son diseñados de manera que los investigadores manipulan deliberadamente una o más variables independientes (llamadas factores o variables controladas) para observar su efecto en la variable dependiente.\nIncluyen la aleatorización de sujetos entre grupos (por ejemplo, grupos de control y tratamiento) para minimizar sesgos y asegurar comparabilidad.\nEn muchas ocasiones la información suplementaria no se puede incorporar en el diseño del experimento. A esas variables, no controladas, se les suel llamar covariables.\nEjemplo: Un estudio clínico donde se prueba un nuevo medicamento y se compara su efecto con un placebo.\n\nEstudios observacionales exploratorios:\n\nEn este enfoque, los datos se recogen sin intervenir ni manipular las condiciones. Los investigadores observan y registran los fenómenos tal como ocurren en la naturaleza.\nPueden clasificarse en:\n\nEstudios transversales: Los datos se recogen en un único punto temporal.\nEstudios longitudinales: Los datos se recogen durante un periodo para analizar cambios a lo largo del tiempo.\n\nEjemplo: Investigar los hábitos alimenticios y su asociación con enfermedades cardiovasculares en una población.\n\nEstudios observacionales confirmatorios:\n\nEn este enfoque, los datos se recogen para testear (confirmar o no) hipótesis derivadas de estudios previos o de ideas que pueden tener los investigadores.\nEn este contexto, las variables que aparecen involucradas en la hipótesis que se quiere confirmar se denominan variables primarias, y las variables explicativas que se sabe inluyen en la respuesta se llaman variables de control (en Epidemiología nos referimos a ellas como factores de riesgo)\nEjemplo: Un equipo de investigadores, basándose en estudios previos, plantea la hipótesis de que existe una relación positiva entre el hábito de fumar (variable explicativa principal) y la incidencia de cáncer de pulmón (variable respuesta). Para confirmar esta hipótesis, realizan un estudio observacional en el que recopilan datos de una población durante un periodo determinado. Dado que no es ético inducir a las personas a fumar para realizar un experimento controlado, este estudio se realiza de forma observacional. Los datos se analizan para evaluar la asociación entre las variables, permitiendo confirmar (o refutar) la hipótesis planteada con un diseño adecuado y controlando los posibles factores de confusión.\n\nEncuestas y cuestionarios:\n\nLas encuestas son una técnica común para recoger datos de manera estructurada sobre actitudes, opiniones, comportamientos o características demográficas.\nPueden aplicarse en formato presencial, en línea, por teléfono o mediante correo.\nEjemplo: Una encuesta para medir el grado de satisfacción de los clientes con un servicio.\n\nExperimentos naturales:\n\nSe producen cuando un fenómeno natural o social actúa como una intervención en un entorno sin que los investigadores tengan control sobre el experimento.\nEste tipo de estudio aprovecha eventos únicos para analizar sus impactos.\nEjemplo: Estudiar los efectos económicos de una nueva política fiscal aplicada en una región específica.\n\nEstudios de simulación:\n\nLos datos se generan a través de modelos matemáticos o computacionales que representan un sistema real o hipotético.\nEste método se usa cuando es difícil o costoso realizar experimentos reales.\nEjemplo: Simular el comportamiento de un mercado financiero bajo diferentes escenarios económicos.\n\nRecogida de datos secundarios:\n\nEn lugar de recoger datos nuevos, se utilizan datos ya existentes recopilados por terceros, como censos, registros administrativos o bases de datos públicas.\nAunque es eficiente en tiempo y costos, el investigador tiene menor control sobre la calidad y las características de los datos.\nEjemplo: Analizar datos de encuestas nacionales para estudiar tendencias sociales.\n\n\n\n\n\n\nAnálisis Exploratorio de Datos (EDA):\n\nInspeccionar los datos mediante análisis descriptivo y visual para identificar posibles problemas como valores atípicos, datos faltantes y multicolinealidad.\nEscalar o transformar las variables si es necesario, especialmente si están en diferentes escalas o presentan distribuciones no lineales.\n\nAjuste del modelo:\n\nEspecificar el modelo de regresión múltiple en su forma general:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon,\n\\] donde \\(\\varepsilon\\) representa los errores aleatorios.\nEstimar los coeficientes del modelo (\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)) utilizando el método de mínimos cuadrados, que minimiza la suma de los errores al cuadrado.\n\nEvaluación del modelo:\n\nAnalizar el ajuste general del modelo utilizando métricas como \\(R^2\\) y \\(R^2\\) ajustado, que miden la proporción de la variabilidad explicada.\nExaminar la tabla ANOVA para evaluar la significancia global del modelo.\nRealizar pruebas de hipótesis para los coeficientes individuales, verificando si las variables predictoras tienen un efecto significativo en la variable respuesta.\n\nDiagnóstico del modelo:\n\nExaminar los residuos para evaluar supuestos como la linealidad, homocedasticidad, normalidad de los errores y ausencia de autocorrelación.\nIdentificar observaciones atípicas, leverage y puntos de influencia utilizando herramientas como la distancia de Cook, DFBETAS y DFFITS.\n\nReducción de variables:\n\nEn análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.\n\nValidación del modelo:\n\nEvaluar el desempeño del modelo con datos de validación o mediante técnicas como validación cruzada para garantizar su capacidad predictiva en nuevos conjuntos de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#selección-de-variables",
    "href": "tema2.html#selección-de-variables",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "2.3 Selección de variables",
    "text": "2.3 Selección de variables\nSimplificar un modelo eliminando variables irrelevantes es fundamental para mejorar su parsimonia y evitar el sobreajuste. Este objetivo puede lograrse mediante métodos de selección de variables, ya sean directos, automáticos o basados en regularización. Estas técnicas permiten identificar subconjuntos óptimos de predictores, optimizando tanto la simplicidad como la precisión del modelo. En particular, los métodos de regularización, como Ridge, Lasso y Elastic Net, introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste, convirtiéndose en herramientas clave en el análisis de datos modernos.\nCuando se dispone de \\(p\\) variables explicativas, es posible construir hasta \\(2^p\\) modelos diferentes considerando todas las combinaciones posibles de estas variables. Sin embargo, explorar de manera exhaustiva todos estos modelos puede ser inviable, especialmente si \\(p\\) es grande. Por ejemplo, con solo 10 variables regresoras, se generarían \\(2^{10} = 1024\\) modelos posibles. Aunque la tecnología actual permite ajustar todos estos modelos, evaluar cada uno en términos de bondad de ajuste, gráficos de residuos, detección de observaciones influyentes y otros diagnósticos sería extremadamente complejo y costoso.\nPara superar este desafío, se han desarrollado criterios específicos de selección de variables que ayudan a los analistas a identificar un pequeño subconjunto de modelos que cumplan con los estándares de calidad deseados. Este enfoque permite centrar el análisis en un grupo reducido de modelos “buenos”, generalmente entre 4 y 6, y realizar un estudio más profundo y detallado de ellos. Esta estrategia facilita tanto la interpretación como la eficiencia del proceso analítico, optimizando el uso de recursos computacionales y asegurando que los resultados sean robustos y fiables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema2.html#reducción-de-variables",
    "href": "tema2.html#reducción-de-variables",
    "title": "2  Métodos de selección de variables y problemas de regularización",
    "section": "2.2 Reducción de variables",
    "text": "2.2 Reducción de variables\nEn análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.\nEs especialmente importante reducir el número de variables explicativas en los estudios observacionales exploratorios, ya que en otros tipos de estudios, como los diseñados previamente, las variables incluidas suelen estar seleccionadas de antemano porque se conoce su relación con la variable respuesta o porque han sido identificadas como relevantes en investigaciones previas.\nLa reducción de variables explicativas busca simplificar el modelo al seleccionar un subconjunto de predictores que capturen la mayor parte de la información relevante de los datos. Este proceso puede realizarse a través de diferentes enfoques, dependiendo del contexto y de las características del conjunto de datos.\n\n2.2.1 Motivaciones para reducir variables\nAl limitar el número de predictores, no solo se simplifica el modelo, sino que también se optimizan diversos aspectos fundamentales en el análisis.\n\nEvitar el zobreajuste:\n\nCuando hay demasiadas variables en relación al número de observaciones, el modelo puede ajustarse demasiado a los datos de entrenamiento y perder capacidad predictiva en nuevos conjuntos de datos.\n\nMejorar la interpretabilidad:\n\nUn modelo con menos variables es más fácil de interpretar, lo que resulta fundamental en aplicaciones como ciencias sociales, biomedicina o economía.\n\nReducción de complejidad computacional:\n\nAl disminuir el número de variables, se reducen los costos de tiempo y memoria en el ajuste y evaluación del modelo.\n\nManejo de multicolinealidad:\n\nLa reducción puede eliminar variables redundantes que presentan una alta correlación entre sí, estabilizando las estimaciones del modelo.\n\n\n\n\n2.2.2 Métodos de reducción de variables\nAlgunas de las ideas más comunes para tratar de reducir el número de variables de un modelo son:\n\nSelección de variables:\n\nUtiliza estrategias como selección directa, métodos automáticos (forward, backward o stepwise), o técnicas basadas en regularización (Lasso, Elastic Net) para seleccionar las variables más relevantes.\n\nTécnicas de transformación de datos:\n\nSe proyectan las variables explicativas en un nuevo espacio de menor dimensionalidad, manteniendo la mayor cantidad posible de información. Algunas de estas técnicas se estudian en la asignatura de Aprendizaje Automático:\n\nAnálisis de Componentes Principales (PCA): Reduce las variables explicativas a un conjunto de componentes ortogonales que explican la mayor parte de la varianza.\nAnálisis de Factores: Agrupa variables relacionadas en factores latentes que capturan la esencia de la información.\n\n\nFiltrado basado en información:\n\nIdentifica y descarta variables con baja variabilidad o poca relación con la variable respuesta, utilizando métricas como la correlación o importancia estadística.\n\nMétodos de selección basados en modelos:\n\nAjusta modelos iterativamente para evaluar la contribución de cada variable explicativa y descartar aquellas con menor relevancia según criterios como el \\(p\\)-valor, AIC o BIC.\n\n\n\n\n\n\n\n\nConsideraciones importantes\n\n\n\n\nLa reducción de variables debe realizarse cuidadosamente para evitar la pérdida de información clave que pueda comprometer la calidad del modelo.\nEs fundamental validar el modelo resultante, asegurándose de que mantenga su capacidad predictiva mediante técnicas como validación cruzada.\nEn algunos casos, la selección o transformación de variables puede implicar compromisos entre simplicidad e interpretabilidad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de selección de variables y problemas de regularización</span>"
    ]
  },
  {
    "objectID": "tema3.html",
    "href": "tema3.html",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "",
    "text": "3.1 Modelos no lineales\nLos modelos de regresión no lineal son herramientas esenciales cuando las relaciones entre las variables no pueden capturarse adecuadamente con modelos lineales. La regresión polinómica, exponencial, logarítmica y los modelos por tramos ofrecen diferentes enfoques para representar patrones complejos en los datos. Comprender cuándo y cómo aplicar estos modelos es fundamental para mejorar la precisión y la interpretabilidad en el análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#modelos-no-lineales",
    "href": "tema3.html#modelos-no-lineales",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "",
    "text": "3.1.1 Regresión Polinómica\nLa regresión polinómica es una extensión de la regresión lineal que permite capturar relaciones no lineales mediante la inclusión de términos polinómicos (cuadráticos, cúbicos, etc.). Aunque sigue siendo un modelo lineal en los parámetros, la inclusión de potencias de las variables independientes permite ajustar curvas en lugar de líneas rectas.\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_k X^k + \\varepsilon\n\\]\nDonde \\(k\\) es el grado del polinomio. A medida que aumenta el grado, el modelo se vuelve más flexible y puede ajustarse a relaciones más complejas.\nEste tipo de modelos son capaces de capturar curvaturas suaves en los datos. Se trata de modelos fáciles de implementar y comprender, aunque la interpretación de los coeficientes asociados a altos grados del polinomio puede ser compleja. De hecho, exite un claro riesgo de sobreajuste cuando se utilizan polinomios de alto grado.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\nx &lt;- 1:20\ny &lt;- 3 + 2 * x + 0.5 * x^2 + rnorm(20, mean = 0, sd = 10)\n\n# Ajuste del modelo polinómico de grado 2\nmodelo_polinomico &lt;- lm(y ~ poly(x, 2))\n\nsummary(modelo_polinomico)\n\n\nCall:\nlm(formula = y ~ poly(x, 2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.9643  -6.4011  -0.8541   5.8504  17.2160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    97.17       2.27  42.796  &lt; 2e-16 ***\npoly(x, 2)1   318.21      10.15  31.339  &lt; 2e-16 ***\npoly(x, 2)2    60.98      10.15   6.006 1.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.15 on 17 degrees of freedom\nMultiple R-squared:  0.9836,    Adjusted R-squared:  0.9816 \nF-statistic: 509.1 on 2 and 17 DF,  p-value: 6.778e-16\n\n# Visualización\nplot(x, y, main = \"Regresión Polinómica de Segundo Grado\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_polinomico), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Modelos de Regresión Exponencial y Logarítmica\nCuando la relación entre la variable dependiente y la independiente sigue un crecimiento o decaimiento exponencial, o una relación logarítmica, los modelos lineales tradicionales no son suficientes. En estos casos, se pueden utilizar transformaciones exponenciales o logarítmicas.\nRegresión Exponencial\nEste modelo es útil cuando la variable dependiente crece (o decrece) a una tasa proporcional a su valor actual.\n\\[\nY = \\beta_0 e^{\\beta_1 X} + \\varepsilon\n\\]\nEste modelo puede linearizarse tomando el logaritmo de la variable dependiente:\n\\[\n\\log(Y) = \\log(\\beta_0) + \\beta_1 X + \\varepsilon\n\\]\nRegresión Logarítmica\nÚtil cuando la tasa de cambio de la variable dependiente disminuye a medida que aumenta la variable independiente.\n\\[\nY = \\beta_0 + \\beta_1 \\log(X) + \\varepsilon\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados para un modelo exponencial\nset.seed(123)\nx &lt;- 1:20\ny &lt;- exp(0.3 * x) + rnorm(20, mean = 0, sd = 20)\n\n# Asegurarse de que todos los valores de 'y' sean positivos para aplicar logaritmo\ny[y &lt;= 0] &lt;- min(y[y &gt; 0]) * 0.5  # Reemplaza valores no positivos por un valor pequeño positivo\n\n# Ajuste del modelo exponencial (transformación logarítmica)\nmodelo_exponencial &lt;- lm(log(y) ~ x)\n\n# Resumen del modelo\nsummary(modelo_exponencial)\n\n\nCall:\nlm(formula = log(y) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9269 -0.1997  0.1612  0.3837  2.6104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02791    0.59598   0.047    0.963    \nx            0.29240    0.04975   5.877 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.283 on 18 degrees of freedom\nMultiple R-squared:  0.6574,    Adjusted R-squared:  0.6384 \nF-statistic: 34.54 on 1 and 18 DF,  p-value: 1.45e-05\n\n# Visualización\nplot(x, y, main = \"Regresión Exponencial\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_exponencial, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original (exponencial inverso del log)\nlines(x, exp(predicciones), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Regresión Spline y modelos basados en Segmentos\nLos splines y los modelos segmentados son técnicas que permiten ajustar relaciones no lineales mediante la división de los datos en segmentos y el ajuste de funciones lineales o polinómicas en cada segmento. Estos métodos son especialmente útiles cuando la relación entre las variables cambia en diferentes rangos de los datos.\nModelos por tramos (Piecewise Regression)\nEn este enfoque, se ajustan diferentes regresiones lineales a distintos rangos de la variable independiente. A diferencia de los splines, las transiciones entre segmentos no necesariamente son suaves.\nEstos modelos permiten capturar relaciones complejas con menor riesgo de sobreajuste en comparación con polinomios de alto grado.\nSplines\nLos splines son una poderosa herramienta en el análisis de regresión para modelar relaciones no lineales entre variables. A diferencia de los modelos polinómicos tradicionales, que ajustan un solo polinomio a todos los datos, los splines permiten dividir el rango de la variable independiente en diferentes tramos y ajustar polinomios separados en cada uno de ellos. Esto proporciona mayor flexibilidad para capturar patrones complejos en los datos, sin los problemas de inestabilidad y sobreajuste que pueden surgir al utilizar polinomios de alto grado.\nUn spline es una función que está compuesta por múltiples polinomios por tramos que se ajustan en diferentes intervalos del dominio de la variable independiente. Estos polinomios están conectados en puntos específicos llamados nudos (knots), que marcan el final de un tramo y el inicio de otro. La principal característica de los splines es que estos polinomios están diseñados para unirse de manera suave en los nudos, asegurando que la función resultante sea continua y, en muchos casos, que sus derivadas también sean continuas.\n\n\n\n\n\n\nElementos clave\n\n\n\n\n\n\nTramos: Intervalos del dominio de la variable independiente en los que se ajusta un polinomio distinto.\nNudos: Puntos donde los tramos se conectan. Los nudos definen la estructura del spline y determinan dónde la función puede cambiar de forma.\nContinuidad: Los splines están construidos para que no haya saltos abruptos en la función o en sus derivadas en los nudos. Por ejemplo, un spline cúbico asegura continuidad en la función, la primera derivada (pendiente) y la segunda derivada (curvatura).\n\n\n\n\n\n\n\n\n\n\nTipos de Splines\n\n\n\n\n\nSplines Lineales:\n\nSe ajustan líneas rectas entre los nudos.\nGarantizan la continuidad en los puntos de unión, pero no necesariamente en la pendiente.\nSon simples, pero pueden generar ángulos abruptos en los nudos.\n\nSplines Cuadráticos:\n\nSe utilizan polinomios de segundo grado en cada tramo.\nAseguran continuidad en la función y en la pendiente, pero no en la curvatura.\n\nSplines Cúbicos:\n\nLos splines cúbicos son los más utilizados en análisis de regresión.\nUtilizan polinomios de tercer grado en cada tramo.\nGarantizan suavidad en la función y en sus primeras dos derivadas, lo que significa que la función es continua, su pendiente es continua y la curvatura es suave.\nEvitan el sobreajuste que puede ocurrir con polinomios de alto grado, proporcionando un ajuste flexible sin perder estabilidad.\nA diferencia de los polinomios globales de alto grado, los splines cúbicos pueden capturar patrones complejos sin oscilar de manera excesiva entre los puntos de datos.\nLos splines permiten que el modelo se adapte localmente a diferentes patrones en distintos tramos del dominio de la variable independiente.\n\nSplines Naturales:\n\nSon una variante de los splines cúbicos que imponen condiciones adicionales en los extremos del rango de los datos, forzando la segunda derivada a ser cero en los extremos. Esto ayuda a evitar oscilaciones no deseadas fuera del rango de los datos.\n\n\n\n\nEl uso de splines en regresión permite modelar relaciones no lineales de manera flexible. La elección del número y la ubicación de los nudos es un aspecto fundamental del ajuste con splines:\n\nNúmero de nudos: Demasiados nudos pueden llevar a un sobreajuste, mientras que muy pocos pueden no capturar adecuadamente la relación entre las variables. El uso de técnicas de validación cruzada puede ayudar a encontrar el equilibrio adecuado.\nUbicación de los nudos: Los nudos pueden colocarse en puntos equidistantes, en cuantiles de la variable independiente, o en puntos donde se sospecha que la relación entre las variables cambia. La colocación de nudos es clave para obtener un buen ajuste. Los nudos pueden seleccionarse de manera automática (por ejemplo, en los cuantiles de la variable independiente) o manualmente según el conocimiento del problema.\n\n\n\n\n\n\n\nAviso\n\n\n\nA diferencia de la regresión lineal simple, los coeficientes de los splines no tienen una interpretación directa. El enfoque se centra en la forma general del ajuste en lugar de en el valor de los coeficientes individuales.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería para splines\nlibrary(splines)\n\n# Datos simulados\nset.seed(123)\nx &lt;- seq(1, 100, by = 1)\ny &lt;- ifelse(x &lt;= 50, 2 * x + rnorm(100, 0, 10), 0.5 * x + rnorm(100, 0, 10))\n\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, knots = c(30, 60, 80)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Cambio en la posición de los nodos\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, knots = c(40, 50)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Ajustamos un spline cuadrático \n\n# Ajuste del modelo spline\nmodelo_spline &lt;- lm(y ~ bs(x, degree=2, knots = c(30, 60, 80)))\n\n# Visualización\nplot(x, y, main = \"Regresión Spline\", pch = 19, col = \"blue\")\nlines(x, predict(modelo_spline), col = \"red\", lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#transformación-de-variables",
    "href": "tema3.html#transformación-de-variables",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "3.2 Transformación de variables",
    "text": "3.2 Transformación de variables\nEn el análisis de datos y la construcción de modelos estadísticos, no siempre es posible capturar adecuadamente la relación entre las variables independientes y la variable dependiente utilizando modelos lineales en su forma original. Aquí es donde entran en juego las transformaciones de variables, que permiten modificar la estructura de los datos para mejorar el ajuste del modelo, cumplir con los supuestos de la regresión y, en muchos casos, facilitar la interpretación.\nLas transformaciones de variables son una herramienta fundamental para mejorar el rendimiento y la precisión de los modelos estadísticos. Estas transformaciones pueden aplicarse tanto a la variable dependiente como a las variables independientes.\n\n\n\n\n\n\nObjetivos\n\n\n\n\n\nLinearizar relaciones no lineales:\nMuchas relaciones entre variables no son lineales en su forma original. Aplicar una transformación adecuada puede convertir una relación no lineal en lineal, permitiendo el uso de técnicas de regresión lineal. Por ejemplo, una relación exponencial \\[Y = \\beta_0 e^{\\beta_1 X}\\] puede linearizarse tomando el logaritmo de \\(Y\\):\n\\[\n   \\log(Y) = \\log(\\beta_0) + \\beta_1 X\n   \\]\nCorregir problemas de heterocedasticidad:\nLa regresión lineal asume que los errores tienen varianza constante (homocedasticidad). Sin embargo, en la práctica, es común encontrar datos con heterocedasticidad (la varianza de los errores cambia con el nivel de la variable independiente). Las transformaciones pueden ayudar a estabilizar la varianza. Por ejemplo, transformar la variable dependiente \\(Y\\) usando un logaritmo o una raíz cuadrada puede reducir la heterocedasticidad.\nNormalizar la distribución de los errores:\nLa regresión lineal también asume que los errores están normalmente distribuidos. Las transformaciones pueden ayudar a que los residuos del modelo se ajusten mejor a una distribución normal, lo que mejora la validez de los intervalos de confianza y las pruebas de hipótesis.\nReducir la influencia de valores atípicos:\nAlgunas transformaciones pueden disminuir la influencia de los valores atípicos en el modelo, haciendo que el ajuste sea más robusto.\nMejorar la interpretabilidad del modelo:\nAunque algunas transformaciones pueden complicar la interpretación directa de los coeficientes, otras pueden facilitar el entendimiento de la relación entre variables (por ejemplo, tasas de crecimiento constantes).\n\n\n\n\n3.2.1 Tipos de transformaciones comunes\nExisten diversas transformaciones que pueden aplicarse a los datos según el problema que se desea abordar. A continuación, se describen las transformaciones más utilizadas en el análisis de regresión.\n\nTransformación Logarítmica (\\(\\log\\))\nSe emplea para linearizar relaciones exponenciales, reducir la heterocedasticidad, y estabilizar la varianza: - \\[ Y = \\log(Y) \\] o \\[ X = \\log(X) \\]\nEs una transformación adecuada cuando la variable tiene una distribución sesgada a la derecha o cuando el efecto marginal disminuye con el valor de la variable (Ingresos, crecimiento poblacional, y tasas de interés, etc).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con crecimiento exponencial\nset.seed(123)\nx &lt;- 1:20\ny &lt;- exp(0.3 * x) + rnorm(20, mean = 0, sd = 20)\n\n\n# Transformación logarítmica para linearizar la relación\nmodelo_log &lt;- lm(log(y) ~ x)\n\nWarning in log(y): Se han producido NaNs\n\nsummary(modelo_log)\n\n\nCall:\nlm(formula = log(y) ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.00071 -0.11760  0.04277  0.35876  1.73316 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.10652    0.59757   1.852 0.083853 .  \nx            0.22528    0.04655   4.839 0.000217 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 15 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.6096,    Adjusted R-squared:  0.5835 \nF-statistic: 23.42 on 1 and 15 DF,  p-value: 0.0002166\n\n# Visualización\nplot(x, y, main = \"Transformación Log\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_log, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original (exponencial inverso del log)\nlines(x, exp(predicciones), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nTransformación de Raíz Cuadrada (\\(\\sqrt{}\\))\nSe emplea para reducir la heterocedasticidad, especialmente cuando la varianza aumenta linealmente con la media: \\[Y = \\sqrt{Y}\\] o \\[X = \\sqrt{X}\\]\nSe emplea comúnmente en conteos de eventos o variables positivas (número de llamadas, defectos, etc.).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con variabilidad creciente\nset.seed(123)\nx &lt;- 1:50\ny &lt;- x + rnorm(50, mean = 0, sd = x)\n\n# Aplicando raíz cuadrada a la variable dependiente\nmodelo_sqrt &lt;- lm(sqrt(y) ~ x)\n\nWarning in sqrt(y): Se han producido NaNs\n\nsummary(modelo_sqrt)\n\n\nCall:\nlm(formula = sqrt(y) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4838 -1.3312  0.1822  1.3441  4.4278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.12028    0.53305   3.978 0.000284 ***\nx            0.11955    0.01819   6.574 7.39e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 40 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.5193,    Adjusted R-squared:  0.5073 \nF-statistic: 43.22 on 1 and 40 DF,  p-value: 7.387e-08\n\n# Visualización\nplot(x, y, main = \"Transformación SQRT\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_sqrt, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original\nlines(x, predicciones^2, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nTransformación Inversa (\\(\\frac{1}{X}\\))\nSe emplea la tranformación inversa para modelar relaciones donde el efecto de la variable independiente disminuye rápidamente. \\[ Y = \\frac{1}{X} \\]\nEs especialmente útil cuando se espera que un aumento en \\(X\\) tenga un efecto decreciente en \\(Y\\) (Relaciones físicas como la ley de la gravitación, velocidad vs. tiempo en fricción, etc).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados con relación inversa\nset.seed(123)\nx &lt;- 1:50\ny &lt;- 1 / x + rnorm(50, mean = 0, sd = 0.05)\n\n# Ajuste del modelo con transformación inversa\nmodelo_inverso &lt;- lm(y ~ I(1/x))\nsummary(modelo_inverso)\n\n\nCall:\nlm(formula = y ~ I(1/x))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.100193 -0.028703 -0.005628  0.033009  0.106450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.002092   0.007633   0.274    0.785    \nI(1/x)      0.995869   0.042338  23.522   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04677 on 48 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.9185 \nF-statistic: 553.3 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n# Visualización\nplot(x, y, main = \"Transformación Inversa\", pch = 19, col = \"blue\", ylab = \"y\", xlab = \"x\")\n\n# Predicciones para los mismos valores de x\npredicciones &lt;- predict(modelo_inverso, newdata = data.frame(x = x))\n\n# Convertir predicciones a la escala original\nlines(x, predicciones, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Transformación de Box-Cox\nLa transformación de Box-Cox es un método que busca automáticamente la mejor transformación para estabilizar la varianza y aproximar la normalidad de los errores. La transformación se define como:\n\\[\nY(\\lambda) =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(Y), & \\lambda = 0\n\\end{cases}\n\\]\nSe emplea para encontrar la transformación óptima para los datos. Se utiliza cuando no está claro qué transformación aplicar. Por ejemplo, en caso de variables continuas con varianza no constante o distribución no normal.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería para Box-Cox\nlibrary(MASS)\n\n# Datos simulados\nset.seed(123)\nx &lt;- 1:20\ny &lt;- x^2 + rnorm(20, mean = 0, sd = 5)\n\n# Verificar si hay valores negativos\nmin(y)\n\n[1] -1.802378\n\n# Si hay valores negativos, sumar una constante para que todos los valores sean positivos\nif (min(y) &lt;= 0) {\n  y &lt;- y + abs(min(y)) + 1  # Desplaza todos los valores para que sean positivos\n}\n\n# Ajuste de un modelo lineal simple\nmodelo_bc &lt;- lm(y ~ x)\n\n# Aplicación de la transformación de Box-Cox\nboxcox(modelo_bc)\n\n\n\n\n\n\n\n\n\n\n\nLa gráfica de Box-Cox mostrará el valor óptimo de \\(\\lambda\\), que indica la mejor transformación para los datos.\n\n\n\n3.2.3 Consideracione sobre las transformaciones\nAntes de aplicar transformaciones, es importante diagnosticar si realmente son necesarias. Existen varias herramientas para identificar problemas en los datos que pueden solucionarse con transformaciones:\n\nGráficos de Dispersión: Visualizar la relación entre la variable dependiente y las independientes puede revelar patrones no lineales o heterocedasticidad.\nAnálisis de Residuos: Un gráfico de los residuos frente a los valores ajustados debe mostrar una distribución aleatoria. Patrones sistemáticos o “abanicos” indican la necesidad de transformación. El gráfico de QQ-Plot de los residuos ayuda a evaluar la normalidad.\nPruebas Estadísticas: Pruebas de normalidad como Shapiro-Wilk para los residuos. Pruebas de heterocedasticidad como Breusch-Pagan.\n\nSi bien las transformaciones pueden mejorar el ajuste del modelo, también pueden afectar la interpretación de los coeficientes. Es importante tener en cuenta cómo cambia el significado de los resultados:\n\nTransformaciones en la variable dependiente:\n\nSi aplicas \\(\\log(Y)\\), los coeficientes representan cambios proporcionales en \\(Y\\).\nSi aplicas \\(\\sqrt{Y}\\), los coeficientes representan la tasa de cambio en la raíz cuadrada de \\(Y\\).\n\nTransformaciones en la variable independiente:\n\nSi transformas \\(X\\) con \\(\\log(X)\\), los coeficientes indican cómo cambia \\(Y\\) por cada incremento porcentual en \\(X\\).\nSi transformas \\(X\\) con \\(\\frac{1}{X}\\), los coeficientes representan el cambio en \\(Y\\) por cada unidad de disminución en \\(X\\).\n\nRevertir Transformaciones para Interpretación: Después de ajustar un modelo, es posible transformar las predicciones de nuevo a la escala original para facilitar la interpretación.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema3.html#ingeniería-de-características",
    "href": "tema3.html#ingeniería-de-características",
    "title": "3  Modelos no lineales. Transformación de variables. Ingeniería de características.",
    "section": "3.3 Ingeniería de características",
    "text": "3.3 Ingeniería de características\nLa ingeniería de características es el arte y la ciencia de transformar los datos brutos en representaciones que faciliten el aprendizaje y mejoren la capacidad predictiva de los modelos. Consiste en el proceso de crear, transformar y seleccionar las variables que se utilizan en un modelo para mejorar su rendimiento. Una característica bien diseñada puede hacer que un modelo simple supere a modelos más complejos, mientras que características irrelevantes o mal definidas pueden degradar significativamente la calidad del análisis. Este proceso incluye:\n\nCreación de nuevas variables a partir de las existentes.\nTransformación de variables para mejorar su distribución o relación con la variable objetivo.\nSelección de las características más relevantes, eliminando aquellas que no aportan valor o introducen ruido.\nPreparación de datos para modelos específicos, asegurando que las variables cumplan con los requisitos del algoritmo (por ejemplo, escalado, normalización o codificación).\n\n\n3.3.1 Creación de nuevas variables\nUna de las tareas más importantes en la ingeniería de características es la creación de nuevas variables que puedan capturar relaciones complejas entre las variables independientes y la variable objetivo.\nLas interacciones entre variables permiten capturar relaciones no lineales entre las variables al considerar cómo el efecto de una variable puede depender del valor de otra. Si se dispone de dos variables \\(X_i\\) y \\(X_j\\), es posible crear una nueva variable de interacción $X_{} = X_i X_j $.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Ejemplo en R de interacción de variables\nset.seed(123)\nX1 &lt;- rnorm(100)\nX2 &lt;- rnorm(100)\nY &lt;- 3 + 2 * X1 + 4 * X2 + 1.5 * X1 * X2 + rnorm(100)\n\n# Crear variable de interacción\nX_interaccion &lt;- X1 * X2\n\n# Ajustar modelo con interacción\nmodelo_interaccion &lt;- lm(Y ~ X1 + X2 + X_interaccion)\nsummary(modelo_interaccion)\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X_interaccion)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8719 -0.6777 -0.1086  0.5897  2.3166 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.14098    0.09578   32.80   &lt;2e-16 ***\nX1             1.90719    0.10834   17.60   &lt;2e-16 ***\nX2             4.03434    0.09881   40.83   &lt;2e-16 ***\nX_interaccion  1.65911    0.11449   14.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9468 on 96 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.9516 \nF-statistic: 649.2 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nTal y como estudiamos cuando tratamos el tema de regresión polinómica, agregar términos polinómicos permite capturar relaciones no lineales al incluir potencias de las variables independientes. Por ejemplo, para una variable \\(X\\), se puede crear \\(X^2\\), \\(X^3\\), etc.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nx &lt;- 1:20\ny &lt;- 5 + 2 * x + 0.5 * x^2 + rnorm(20, mean = 0, sd = 5)\n\n# Incluir término cuadrático en el modelo\nmodelo_polinomico &lt;- lm(y ~ x + I(x^2))\nsummary(modelo_polinomico)\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9235 -2.9142  0.6081  3.0085  9.6944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.49742    4.18141  -0.119  0.90670    \nx            2.85574    0.91705   3.114  0.00631 ** \nI(x^2)       0.47433    0.04242  11.182 2.94e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.62 on 17 degrees of freedom\nMultiple R-squared:  0.9953,    Adjusted R-squared:  0.9947 \nF-statistic:  1792 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCrear combinaciones simples de variables puede capturar relaciones ocultas en los datos. Por ejemplo:\n\nSumas o diferencias: \\(X_{\\text{nuevo}} = X_1 + X_2\\)\nRatios: \\(X_{\\text{ratio}} = \\frac{X_1}{X_2}\\)\nVariables categóricas combinadas: Fusionar categorías relacionadas en una nueva variable. Deben ser categorías que, desde un punto de vista del dominio de aplicación, tenga sentido combinar.\n\n\n\n\n3.3.2 Selección y Reducción de variables\nUna vez que se han creado nuevas características, es importante seleccionar las que son más relevantes para el modelo y eliminar aquellas que no aportan valor o introducen ruido.\nSe trató con detalle estos conceptos en el tema anterior. Planteamos un ejemplo de selección Stepwise.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\nX1 &lt;- rnorm(100)\nX2 &lt;- rnorm(100)\nX3 &lt;- rnorm(100)\nY &lt;- 3 + 2 * X1 + 4 * X2 + rnorm(100)\n\n# Modelo completo con todas las variables\nmodelo_completo &lt;- lm(Y ~ X1 + X2 + X3)\n\n# Selección de variables usando stepwise\nmodelo_seleccionado &lt;- step(modelo_completo, direction = \"both\")\n\nStart:  AIC=13.96\nY ~ X1 + X2 + X3\n\n       Df Sum of Sq     RSS     AIC\n- X3    1      0.29  106.43  12.236\n&lt;none&gt;               106.15  13.964\n- X1    1    306.06  412.21 147.636\n- X2    1   1510.95 1617.09 284.321\n\nStep:  AIC=12.24\nY ~ X1 + X2\n\n       Df Sum of Sq     RSS     AIC\n&lt;none&gt;               106.43  12.236\n+ X3    1      0.29  106.15  13.964\n- X1    1    313.60  420.04 147.517\n- X2    1   1510.83 1617.26 282.332\n\nsummary(modelo_seleccionado)\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47672 -0.67285  0.09839  0.70676  2.62566 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9729     0.1059   28.08   &lt;2e-16 ***\nX1            1.9522     0.1155   16.91   &lt;2e-16 ***\nX2            4.0449     0.1090   37.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.048 on 97 degrees of freedom\nMultiple R-squared:  0.943, Adjusted R-squared:  0.9418 \nF-statistic: 802.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nRecordemos que los métodos de regularización, estudiados en el tema 2, no solo ajustan el modelo, sino que también penalizan la complejidad, lo que ayuda a eliminar variables irrelevantes.\n\n\n3.3.3 Escalado y Normalización de Variables\nMuchos algoritmos de aprendizaje automático, como la regresión, las redes neuronales y los métodos basados en distancia (k-NN, SVM), son sensibles a la escala de las variables. Por lo tanto, es fundamental escalar o normalizar los datos para garantizar que todas las variables contribuyan de manera equitativa al modelo.\nEstandarización (Z-Score Normalization)\nLa estandarización consiste en restar la media y dividir por la desviación estándar, lo que produce variables con media cero y desviación estándar uno.\n\\[\nX_{\\text{estandarizado}} = \\frac{X - \\bar{X}}{\\sigma_X}\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Estandarización de una variable\nX1_estandarizado &lt;- scale(X1)\n\n\n\n\n\nNormalización Min-Max\nLa normalización Min-Max escala las variables a un rango específico, típicamente entre 0 y 1.\n\\[\nX_{\\text{normalizado}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Normalización Min-Max\nX1_min_max &lt;- (X1 - min(X1)) / (max(X1) - min(X1))\n\n\n\n\n\n\n3.3.4 Técnicas avanzadas de Ingeniería de Características\nCuando se trabaja con grandes conjuntos de datos o con variables altamente correlacionadas, puede ser necesario aplicar técnicas más avanzadas para reducir la dimensionalidad y extraer características relevantes.\n\n3.3.4.1 Análisis de Componentes Principales (PCA)\nEl Análisis de Componentes Principales (PCA) es una técnica de reducción de dimensionalidad que transforma un conjunto de variables correlacionadas en un conjunto más pequeño de componentes principales no correlacionados que explican la mayor parte de la varianza en los datos.\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles del Análisis de Componentes Principales son tratados en la asignatura de Aprendizaje Automático.\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Datos simulados\nset.seed(123)\ndatos &lt;- data.frame(X1, X2, X3)\n\n# Aplicar PCA\npca_resultado &lt;- prcomp(datos, scale. = TRUE)\n\n# Visualización de los resultados\nsummary(pca_resultado)\n\nImportance of components:\n                          PC1    PC2    PC3\nStandard deviation     1.0726 0.9900 0.9324\nProportion of Variance 0.3835 0.3267 0.2898\nCumulative Proportion  0.3835 0.7102 1.0000\n\nbiplot(pca_resultado)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4.2 Codificación de variables categóricas\nLas variables categóricas deben convertirse en variables numéricas antes de ser utilizadas en muchos modelos. Esto puede hacerse mediante:\nCodificación One-Hot\nEl One-Hot Encoding es una técnica utilizada en el preprocesamiento de datos para convertir variables categóricas en variables numéricas. Muchos algoritmos de aprendizaje automático y estadística (como la regresión lineal, redes neuronales y máquinas de soporte vectorial) requieren que las variables de entrada sean numéricas, ya que no pueden manejar directamente datos categóricos.\nEl One-Hot Encoding transforma cada categoría en una nueva columna binaria (0 o 1), donde el 1 indica la presencia de una categoría específica y el 0 su ausencia.\nSupongamos que tienes una variable categórica llamada Color con tres categorías: Rojo, Verde, y Azul.\n\n\n\nID\nColor\n\n\n\n\n1\nRojo\n\n\n2\nVerde\n\n\n3\nAzul\n\n\n4\nRojo\n\n\n5\nVerde\n\n\n\nCon One-Hot Encoding, creamos una nueva columna para cada categoría única:\n\n\n\nID\nColor_Rojo\nColor_Verde\nColor_Azul\n\n\n\n\n1\n1\n0\n0\n\n\n2\n0\n1\n0\n\n\n3\n0\n0\n1\n\n\n4\n1\n0\n0\n\n\n5\n0\n1\n0\n\n\n\nCada fila tiene un único 1 que indica la categoría correspondiente y ceros en las otras columnas. Esto convierte la información categórica en un formato que los algoritmos numéricos pueden procesar.\n\n\n\n\n\n\nPropiedades Clave\n\n\n\n\n\nVentajas del One-Hot Encoding\n\nCompatibilidad con algoritmos numéricos: La mayoría de los modelos de aprendizaje automático requieren variables numéricas. El One-Hot Encoding convierte las categorías en un formato adecuado.\nEvita suposiciones erróneas A diferencia de la codificación ordinal, que asigna valores numéricos secuenciales a categorías (por ejemplo, Rojo = 1, Verde = 2, Azul = 3), el One-Hot Encoding no introduce un orden artificial entre las categorías. Esto es importante cuando no hay una jerarquía natural.\nMejora la Interpretabilidad en Modelos Lineales: En modelos como la regresión lineal, cada columna creada mediante One-Hot Encoding representa el efecto específico de esa categoría.\n\nDesventajas del One-Hot Encoding\n\nIncremento de la Dimensionalidad: Si la variable categórica tiene muchas categorías únicas (por ejemplo, países o códigos postales), el número de columnas creadas puede ser muy grande. Esto puede conducir a problemas de “curse of dimensionality” (la maldición de la dimensionalidad), afectando el rendimiento del modelo y aumentando el tiempo de computación.\nColinealidad Perfecta (Dummy Variable Trap): Al crear una columna para cada categoría, una de las columnas se puede representar como una combinación lineal de las demás. Esto puede causar problemas en modelos lineales. Para evitarlo, se elimina una categoría de referencia (típicamente la primera), lo que se conoce como evitar la trampa de las variables ficticias (dummy variable trap).\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Variable categórica simulada\ncategoria &lt;- factor(c(\"bajo\", \"medio\", \"alto\", \"medio\", \"alto\"))\n\n# Codificación one-hot\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ndummies &lt;- dummyVars(~ categoria, data = data.frame(categoria))\nprint(dummies)\n\nDummy Variable Object\n\nFormula: ~categoria\n1 variables, 1 factors\nVariables and levels will be separated by '.'\nA less than full rank encoding is used\n\ndatos_codificados &lt;- predict(dummies, newdata = data.frame(categoria))\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería caret\nlibrary(caret)\n\n# Crear un data frame\ndatos &lt;- data.frame(ID = 1:5, Color = c(\"Rojo\", \"Verde\", \"Azul\", \"Rojo\", \"Verde\"))\n\n# Aplicar One-Hot Encoding usando dummyVars\ndummy &lt;- dummyVars(~ Color, data = datos)\none_hot &lt;- predict(dummy, newdata = datos)\n\n# Ver los resultados\nprint(one_hot)\n\n  ColorAzul ColorRojo ColorVerde\n1         0         1          0\n2         0         0          1\n3         1         0          0\n4         0         1          0\n5         0         0          1\n\n\n\n\n\n\n\n\n\n\n\nAviso\n\n\n\nCuando aplicamos One-Hot Encoding, el conjunto de variables resultantes puede generar colinealidad perfecta, lo que puede ser problemático en modelos lineales. Para evitarlo, es común eliminar una de las columnas creadas (que actuará como categoría de referencia).\n\n# One-Hot Encoding con eliminación de una categoría (categoría de referencia)\none_hot_ref &lt;- model.matrix(~ Color, data = datos)[, -1]  # Eliminar la primera columna\n\nprint(one_hot_ref)\n\n  ColorRojo ColorVerde\n1         1          0\n2         0          1\n3         0          0\n4         1          0\n5         0          1\n\n\nEsto elimina una columna (por ejemplo, ColorAzul) y permite que las otras columnas se interpreten en relación a la categoría de referencia.\n\n\n\n\n3.3.4.3 Codificación Ordinal\nLa codificación ordinal es una técnica de preprocesamiento de datos utilizada para convertir variables categóricas en valores numéricos manteniendo el orden natural entre las categorías. A diferencia del One-Hot Encoding, que trata a todas las categorías como independientes y sin relación entre sí, la codificación ordinal es útil cuando las categorías tienen una jerarquía o un orden lógico.\nEn la codificación ordinal, a cada categoría se le asigna un número entero que refleja su posición o nivel en un orden determinado. Esto permite que los modelos estadísticos y de aprendizaje automático interpreten que algunas categorías son mayores o menores que otras, lo que es especialmente útil en variables que representan rangos, niveles o clasificaciones.\nSupongamos que tenemos una variable llamada “Nivel de Satisfacción” con las siguientes categorías:\n\n\n\nNivel de Satisfacción\n\n\n\n\nMuy Insatisfecho\n\n\nInsatisfecho\n\n\nNeutral\n\n\nSatisfecho\n\n\nMuy Satisfecho\n\n\n\nAquí, hay un orden lógico desde “Muy Insatisfecho” hasta “Muy Satisfecho”. Aplicando codificación ordinal, asignamos números que reflejen esta jerarquía:\n\n\n\nNivel de Satisfacción\nCodificación Ordinal\n\n\n\n\nMuy Insatisfecho\n1\n\n\nInsatisfecho\n2\n\n\nNeutral\n3\n\n\nSatisfecho\n4\n\n\nMuy Satisfecho\n5\n\n\n\n\n\n\n\n\n\n\nPropiedades clave\n\n\n\n\n\nLa codificación ordinal es adecuada cuando:\n\nLas categorías tienen un orden natural: Ejemplos incluyen niveles educativos (Primaria, Secundaria, Universidad), calificaciones (Bajo, Medio, Alto), o satisfacción del cliente (Insatisfecho, Neutral, Satisfecho).\nEl modelo puede interpretar la relación de orden: Algunos algoritmos, como la regresión lineal o las máquinas de vectores soporte (SVM), pueden beneficiarse de la codificación ordinal si el orden es relevante para la variable objetivo.\nReducción de dimensionalidad: A diferencia del One-Hot Encoding, que puede crear muchas columnas para variables con múltiples categorías, la codificación ordinal mantiene la variable en una sola columna, lo que es más eficiente para conjuntos de datos grandes.\n\n\n\n\n\n\n\n\n\n\nVentajas y desventajas\n\n\n\n\n\nVentajas\n\nPreserva la jerarquía de las categorías: Permite que el modelo entienda que ciertas categorías son mayores o menores que otras.\nReducción de la dimensionalidad: A diferencia del One-Hot Encoding, no aumenta el número de columnas, lo que reduce el riesgo de la maldición de la dimensionalidad.\nSimplicidad Computacional: Es más eficiente en términos de almacenamiento y tiempo de computación, especialmente para variables con muchas categorías.\n\nDesventajas\n\nRiesgo de interpretación incorrecta del orden: Si la variable categórica no tiene un orden lógico, la codificación ordinal puede inducir al modelo a asumir relaciones que no existen. Por ejemplo, supongamos que tienes una variable Color con categorías Rojo, Verde, y Azul. Asignarles valores como Rojo = 1, Verde = 2, Azul = 3 podría inducir al modelo a pensar que Verde es “mayor” que Rojo y Azul es “mayor” que Verde, lo cual no tiene sentido en este contexto.\nNo Captura la Magnitud de la Diferencia: Aunque las categorías están ordenadas, la codificación ordinal no refleja la magnitud real de las diferencias entre categorías. Por ejemplo, la diferencia entre “Insatisfecho” y “Neutral” puede no ser la misma que entre “Satisfecho” y “Muy Satisfecho”.\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Crear un data frame con una variable categórica ordinal\ndatos &lt;- data.frame(\n  ID = 1:5,\n  Satisfaccion = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\")\n)\n\n# Convertir la variable en un factor ordenado\ndatos$Satisfaccion_ordinal &lt;- factor(datos$Satisfaccion, \n                                     levels = c(\"Muy Insatisfecho\", \"Insatisfecho\", \"Neutral\", \"Satisfecho\", \"Muy Satisfecho\"), \n                                     ordered = TRUE)\n\n# Ver la estructura del factor\nstr(datos)\n\n'data.frame':   5 obs. of  3 variables:\n $ ID                  : int  1 2 3 4 5\n $ Satisfaccion        : chr  \"Muy Insatisfecho\" \"Insatisfecho\" \"Neutral\" \"Satisfecho\" ...\n $ Satisfaccion_ordinal: Ord.factor w/ 5 levels \"Muy Insatisfecho\"&lt;..: 1 2 3 4 5\n\n# Asignar valores numéricos a las categorías ordenadas\ndatos$Satisfaccion_codificada &lt;- as.numeric(datos$Satisfaccion_ordinal)\n\n# Ver el resultado\nprint(datos)\n\n  ID     Satisfaccion Satisfaccion_ordinal Satisfaccion_codificada\n1  1 Muy Insatisfecho     Muy Insatisfecho                       1\n2  2     Insatisfecho         Insatisfecho                       2\n3  3          Neutral              Neutral                       3\n4  4       Satisfecho           Satisfecho                       4\n5  5   Muy Satisfecho       Muy Satisfecho                       5\n\n\n\n\n\nPodemos usar la variable codificada ordinalmente en un modelo de regresión para evaluar su impacto en una variable dependiente, como una puntuación de satisfacción general.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simular una variable de respuesta (puntuación general)\nset.seed(123)\ndatos$Puntuacion &lt;- c(2, 4, 6, 8, 10) + rnorm(5, mean = 0, sd = 0.5)\n\n# Ajustar un modelo de regresión lineal usando la variable codificada\nmodelo_ordinal &lt;- lm(Puntuacion ~ Satisfaccion_codificada, data = datos)\n\n# Resumen del modelo\nsummary(modelo_ordinal)\n\n\nCall:\nlm(formula = Puntuacion ~ Satisfaccion_codificada, data = datos)\n\nResiduals:\n      1       2       3       4       5 \n-0.2090 -0.1279  0.6826 -0.1455 -0.2002 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -0.1552     0.4640  -0.335 0.759968    \nSatisfaccion_codificada   2.0840     0.1399  14.896 0.000657 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4424 on 3 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9822 \nF-statistic: 221.9 on 1 and 3 DF,  p-value: 0.0006565\n\n\n\n\n\nEl modelo ajustará la relación entre la puntuación de satisfacción y el nivel de satisfacción codificado ordinalmente. El coeficiente de la variable Satisfaccion_codificada indicará cómo cambia la puntuación a medida que aumenta el nivel de satisfacción.\n\n\n3.3.4.4 Diferencias entre Codificación Ordinal y One-Hot Encoding\n\n\n\n\n\n\n\n\nCaracterística\nCodificación Ordinal\nOne-Hot Encoding\n\n\n\n\nPreserva el orden\nSí, refleja la jerarquía entre categorías.\nNo, trata cada categoría como independiente.\n\n\nAumenta la dimensionalidad\nNo, mantiene la variable en una sola columna.\nSí, crea una columna para cada categoría única.\n\n\nAdecuado para\nVariables con orden natural (ej. educación).\nVariables sin orden (ej. color, género, ciudad).\n\n\nRiesgo\nPuede inducir al modelo a asumir relaciones falsas si no hay orden real.\nIncremento de la complejidad del modelo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos no lineales. Transformación de variables. Ingeniería de características.</span>"
    ]
  },
  {
    "objectID": "tema4.html",
    "href": "tema4.html",
    "title": "4  Modelos de regresión generalizada",
    "section": "",
    "text": "4.1 Introducción a los GLM",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#introducción-a-los-glm",
    "href": "tema4.html#introducción-a-los-glm",
    "title": "4  Modelos de regresión generalizada",
    "section": "",
    "text": "4.1.1 ¿Qué son los Modelos Lineales Generalizados?\nLos Modelos Lineales Generalizados (GLM) son una extensión de los modelos de regresión lineal que permiten manejar una mayor variedad de tipos de datos y relaciones entre variables (Nelder y Wedderburn 1972). Mientras que la regresión lineal tradicional asume que la variable dependiente es continua y sigue una distribución normal, los GLM permiten trabajar con variables dependientes que:\n\nSon binarias (como éxito/fracaso o sí/no).\nRepresentan conteos de eventos (número de llamadas, accidentes, etc.).\nSon continuas positivas y no siguen una distribución normal (como tiempos o costos).\n\nLos GLM proporcionan una estructura flexible para modelar la relación entre una o más variables independientes y una variable dependiente que sigue alguna distribución de la familia exponencial (binomial, Poisson, gamma, entre otras).\n\n\n4.1.2 Componentes de un Modelo Lineal Generalizado\nUn GLM se define por tres componentes clave:\n\nComponente Aleatorio:\nEste componente describe la distribución de la variable dependiente. En la regresión lineal, la variable dependiente sigue una distribución normal. En los GLM, puede seguir otras distribuciones de la familia exponencial, como:\n\nDistribución Binomial: Para variables categóricas binarias (0/1, éxito/fracaso).\nDistribución de Poisson: Para datos de conteo (número de eventos).\nDistribución Gamma: Para variables continuas y positivas (como costos o tiempos).\n\nComponente Sistemático:\nEste componente describe cómo las variables independientes se combinan linealmente en el modelo. Se define como:\n\\[\n\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde \\(\\eta\\) es el predictor lineal y \\(\\beta\\) representa los coeficientes del modelo.\nFunción de Enlace:\nLa función de enlace conecta el componente sistemático con la media de la variable dependiente. Mientras que en la regresión lineal la relación es directa ($Y = $), en los GLM se utiliza una función de enlace \\(g(\\mu)\\) para transformar la media \\(\\mu\\) y ajustar diferentes tipos de datos.\n\\[\ng(\\mu) = \\eta\n\\]\n\nEjemplos de funciones de enlace:\n\nLogística (Logit): Para la regresión logística, que modela la probabilidad de un evento. \\[\ng(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)\n\\]\nLogarítmica: Para la regresión de Poisson, que modela tasas de eventos. \\[\ng(\\mu) = \\log(\\mu)\n\\]\nIdentidad: Para la regresión lineal estándar. \\[\ng(\\mu) = \\mu\n\\]\n\n\n\n\n\n\n\nAplicaciones\n\n\n\n\n\nLos GLM se utilizan en una amplia variedad de disciplinas para resolver problemas del mundo real:\nRegresión Logística (para variables binarias):\n\nMedicina: Predicción de la presencia o ausencia de una enfermedad basada en factores de riesgo.\nMarketing: Determinación de la probabilidad de que un cliente compre un producto.\nFinanzas: Evaluación de la probabilidad de incumplimiento de pago de un préstamo.\n\nRegresión de Poisson (para datos de conteo):\n\nTransporte: Modelado del número de accidentes en una carretera en un período de tiempo.\nEcología: Conteo de especies en un área determinada.\nTelecomunicaciones: Número de llamadas recibidas por un centro de atención.\n\nRegresión Binomial Negativa (para conteos con sobredispersión):\n\nSalud Pública: Modelado del número de visitas al médico o incidentes de una enfermedad en una población.\n\nModelos Gamma (para variables continuas positivas):\n\nSeguros: Estimación de los costos de reclamos de seguros.\nIngeniería: Modelado de tiempos de falla en procesos industriales.\n\n\n\n\n\n\n4.1.3 Diferencias clave entre la Regresión Lineal y los GLM\n\n\n\n\n\n\n\n\nCaracterística\nRegresión Lineal\nModelos Lineales Generalizados (GLM)\n\n\n\n\nDistribución de la variable dependiente\nNormal\nFamilia exponencial (binomial, Poisson, gamma, etc.)\n\n\nTipo de variable dependiente\nContinua\nBinaria, de conteo, continua positiva\n\n\nRelación entre las variables\nLineal directa\nRelación transformada mediante una función de enlace\n\n\nFunción de Enlace\nIdentidad (\\(g(\\mu) = \\mu\\))\nLogit, logarítmica, inversa, etc.\n\n\n\n\nLas ventajas principales de los GLM son:\n\nFlexibilidad: Los GLM permiten modelar diferentes tipos de variables dependientes, lo que amplía significativamente el rango de problemas que se pueden abordar.\nInterpretación Coherente: Aunque se utilizan funciones de enlace, los coeficientes de los GLM pueden interpretarse de manera similar a los modelos lineales, proporcionando información sobre el impacto de cada variable independiente.\nEvaluación Estadística Robusta: Los GLM permiten la realización de pruebas de hipótesis, la construcción de intervalos de confianza y la evaluación de la bondad del ajuste mediante medidas como el AIC y el BIC.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Cargar librería y datos\nlibrary(MASS)\ndata(Pima.tr)  # Datos sobre diabetes en mujeres de origen pima\n\n# Ajustar un modelo de regresión logística\nmodelo_logistico &lt;- glm(type ~ npreg + glu + bmi, data = Pima.tr, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_logistico)\n\n\nCall:\nglm(formula = type ~ npreg + glu + bmi, family = binomial, data = Pima.tr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.718723   1.411080  -6.179 6.46e-10 ***\nnpreg        0.149213   0.051833   2.879  0.00399 ** \nglu          0.033879   0.006327   5.355 8.55e-08 ***\nbmi          0.094817   0.032405   2.926  0.00343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.41  on 199  degrees of freedom\nResidual deviance: 189.89  on 196  degrees of freedom\nAIC: 197.89\n\nNumber of Fisher Scoring iterations: 5\n\n# Predicciones de la probabilidad de tener diabetes\npredicciones &lt;- predict(modelo_logistico, type = \"response\")\n\n# Ver primeras predicciones\nhead(predicciones)\n\n         1          2          3          4          5          6 \n0.10014804 0.78786795 0.12244031 0.80425012 0.06975347 0.21233644 \n\n\n\n\n\n\nLos Modelos Lineales Generalizados amplían el alcance de la regresión lineal clásica, proporcionando herramientas para modelar una amplia variedad de tipos de datos, desde variables binarias hasta datos de conteo y variables continuas no normales. A través del uso de funciones de enlace y distribuciones flexibles, los GLM permiten resolver problemas complejos del mundo real en campos tan diversos como la medicina, el marketing, la ingeniería y las ciencias sociales.\nEn las próximas secciones, exploraremos en detalle cómo aplicar estos modelos específicos, como la regresión logística y la regresión de Poisson, y cómo interpretar sus resultados en diferentes contextos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#regresión-logística",
    "href": "tema4.html#regresión-logística",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.2 Regresión Logística",
    "text": "4.2 Regresión Logística\nLa regresión logística es una herramienta fundamental para modelar la probabilidad de eventos binarios en una variedad de contextos, desde la medicina hasta la economía y el marketing (Hosmer Jr, Lemeshow, y Sturdivant 2013). La correcta interpretación de los coeficientes mediante odds ratios, así como la evaluación del ajuste del modelo mediante curvas ROC y matrices de confusión, son esenciales para extraer conclusiones válidas de los datos.\n\n4.2.1 Fundamentos de la Regresión Logística\nLa regresión logística es una técnica estadística utilizada para modelar la probabilidad de ocurrencia de un evento binario, es decir, cuando la variable dependiente toma solo dos posibles valores (por ejemplo, éxito/fracaso, sí/no, enfermo/sano). A diferencia de la regresión lineal, que modela una relación lineal entre variables, la regresión logística utiliza una función logística para asegurar que las predicciones estén en el rango [0,1], lo cual es necesario para interpretar los resultados como probabilidades.\nLa función Logística (Sigmoide)\nLa función logística transforma cualquier valor real en un valor comprendido entre 0 y 1. La forma matemática de la función logística es:\n\\[\nP(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p)}}\n\\]\nDonde:\n\n\\(P(Y = 1 | X)\\) es la probabilidad de que el evento ocurra.\n\\(\\beta_0\\) es el intercepto y \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\) son los coeficientes asociados a las variables independientes \\(X_1, X_2, \\dots, X_p\\).\n\nLa curva sigmoide que representa esta función tiene forma de “S”, lo que refleja que para valores muy pequeños o muy grandes del predictor, la probabilidad se aplana hacia 0 o 1, respectivamente.\nFunción de Enlace Logit\nEn la regresión logística, la relación entre el predictor lineal y la probabilidad se establece mediante la función de enlace logit. El logit de una probabilidad \\(p\\) se define como:\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nEsta transformación convierte una probabilidad en una escala que va de \\(-\\infty\\) a \\(+\\infty\\), lo que permite ajustar un modelo lineal a los datos. El modelo logístico puede expresarse como:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\n\n4.2.2 Interpretación de coeficientes y Odds Ratios\nUno de los aspectos más importantes de la regresión logística es la interpretación de los coeficientes. Dado que los coeficientes están en la escala del logit, su interpretación directa no es tan intuitiva como en la regresión lineal. Sin embargo, podemos interpretarlos utilizando odds y odds ratios.\nEl odds o razón de probabilidades de que ocurra un evento es el cociente entre la probabilidad de que ocurra el evento y la probabilidad de que no ocurra:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nPor ejemplo, si la probabilidad de éxito es 0.8, el odds sería:\n\\[\n\\text{odds} = \\frac{0.8}{1 - 0.8} = 4\n\\]\nEsto significa que el evento es 4 veces más probable que no ocurra.\nEl odds ratio (OR) mide el cambio en los odds cuando una variable independiente aumenta en una unidad. Se calcula como el exponencial del coeficiente de la regresión logística:\n\\[\n\\text{OR} = e^{\\beta}\n\\]\nInterpretación de OR:\n\nSi OR &gt; 1, el evento es más probable a medida que aumenta la variable independiente.\nSi OR &lt; 1, el evento es menos probable a medida que aumenta la variable independiente.\nSi OR = 1, no hay efecto.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nSupongamos que ajustamos un modelo de regresión logística para predecir la probabilidad de tener diabetes en función del índice de masa corporal (BMI). El coeficiente asociado a BMI es 0.08.\n\\[\n\\text{OR} = e^{0.08} \\approx 1.083\n\\]\nEsto significa que por cada incremento de 1 unidad en el BMI, la odds de tener diabetes aumentan en un 8.3%.\n\n\n\n\n\n4.2.3 Evaluación del modelo Logístico\nA diferencia de la regresión lineal, donde se usa el coeficiente de determinación (\\(R^2\\)) para evaluar el ajuste, en la regresión logística se utilizan otros métodos para medir la calidad del modelo.\nMatriz de Confusión\nLa matriz de confusión compara las predicciones del modelo con los valores reales, clasificando las observaciones en:\n\nVerdaderos Positivos (VP): Predijo positivo y es positivo.\nFalsos Positivos (FP): Predijo positivo pero es negativo.\nVerdaderos Negativos (VN): Predijo negativo y es negativo.\nFalsos Negativos (FN): Predijo negativo pero es positivo.\n\nA partir de esta matriz, se pueden calcular métricas importantes como:\n\nPrecisión (Accuracy): \\(\\frac{VP + VN}{\\text{Total}}\\)\nSensibilidad (Recall o Tasa de Verdaderos Positivos): \\(\\frac{VP}{VP + FN}\\)\nEspecificidad (Tasa de Verdaderos Negativos): \\(\\frac{VN}{VN + FP}\\)\n\n\n\n\n\n\n\nAviso\n\n\n\nLos detalles de la evaluación de un modelo empleando la Matríz de Confusión son ampliamente tratados en la asignatura de Aprendizaje Automático.\n\n\nCurva ROC y AUC\nLa Curva ROC (Receiver Operating Characteristic) muestra la relación entre la tasa de verdaderos positivos y la tasa de falsos positivos a diferentes umbrales de clasificación.\nEl AUC (Área Bajo la Curva ROC) mide la capacidad del modelo para discriminar entre las clases. Un AUC de \\(0.5\\) indica que el modelo no tiene capacidad predictiva, mientras que un AUC de \\(1.0\\) indica un modelo perfecto.\nPseudo R² (Nagelkerke, McFadden)\nAunque el \\(R^2\\) tradicional no se aplica directamente a la regresión logística, existen medidas como el pseudo \\(R^2\\) que proporcionan una idea de la bondad del ajuste del modelo.\n\nMcFadden’s \\(R^2\\):\n\\[\nR^2_{\\text{McFadden}} = 1 - \\frac{\\log L_{\\text{modelo}}}{\\log L_{\\text{modelo nulo}}}\n\\]\nDonde \\(\\log L_{\\text{modelo}}\\) es el log-likelihood del modelo ajustado y \\(\\log L_{\\text{modelo nulo}}\\) es el log-likelihood de un modelo sin predictores.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a aplicar la regresión logística en R utilizando el conjunto de datos Pima.tr del paquete MASS, que contiene información sobre mujeres pima y si tienen o no diabetes.\n\n# Cargar la librería y el conjunto de datos\nlibrary(MASS)\ndata(Pima.tr)\n\n# Ajustar el modelo de regresión logística\nmodelo_logistico &lt;- glm(type ~ npreg + glu + bmi, data = Pima.tr, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_logistico)\n\n\nCall:\nglm(formula = type ~ npreg + glu + bmi, family = binomial, data = Pima.tr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.718723   1.411080  -6.179 6.46e-10 ***\nnpreg        0.149213   0.051833   2.879  0.00399 ** \nglu          0.033879   0.006327   5.355 8.55e-08 ***\nbmi          0.094817   0.032405   2.926  0.00343 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.41  on 199  degrees of freedom\nResidual deviance: 189.89  on 196  degrees of freedom\nAIC: 197.89\n\nNumber of Fisher Scoring iterations: 5\n\n# Predicciones de probabilidad\npredicciones_prob &lt;- predict(modelo_logistico, type = \"response\")\n\n# Clasificación con un umbral de 0.5\npredicciones_clase &lt;- ifelse(predicciones_prob &gt; 0.5, \"Yes\", \"No\")\n\n# Crear matriz de confusión\ntabla_confusion &lt;- table(Predicted = predicciones_clase, Actual = Pima.tr$type)\nprint(tabla_confusion)\n\n         Actual\nPredicted  No Yes\n      No  114  29\n      Yes  18  39\n\n# Calcular precisión\naccuracy &lt;- sum(diag(tabla_confusion)) / sum(tabla_confusion)\nprint(paste(\"Precisión:\", round(accuracy, 3)))\n\n[1] \"Precisión: 0.765\"\n\n# Cargar librería para curvas ROC\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Curva ROC\nroc_obj &lt;- roc(Pima.tr$type, predicciones_prob)\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj, main = \"Curva ROC para Regresión Logística\")\n\n\n\n\n\n\n\n# Calcular AUC\nauc_valor &lt;- auc(roc_obj)\nprint(paste(\"AUC:\", round(auc_valor, 3)))\n\n[1] \"AUC: 0.831\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#regresión-de-poisson",
    "href": "tema4.html#regresión-de-poisson",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.3 Regresión de Poisson",
    "text": "4.3 Regresión de Poisson\nLa regresión de Poisson es una técnica estadística utilizada para modelar datos de conteo, es decir, situaciones en las que la variable dependiente representa el número de veces que ocurre un evento en un período de tiempo o espacio específico (Coxe, West, y Aiken 2009). Este tipo de modelo es adecuado cuando la variable dependiente toma valores enteros no negativos (\\(0, 1, 2, \\dots\\)) y sigue una distribución de Poisson.\nLa distribución de Poisson describe la probabilidad de que ocurra un número determinado de eventos en un intervalo fijo, dado que estos eventos ocurren de forma independiente y a una tasa constante.\nLa función de probabilidad de la distribución de Poisson es:\n\\[\nP(Y = y) = \\frac{e^{-\\lambda} \\lambda^y}{y!}\n\\]\nDonde:\n\n\\(Y\\) es la variable aleatoria que representa el número de eventos.\n\\(\\lambda\\) es la tasa media de ocurrencia de los eventos (esperanza de \\(Y\\)).\n\\(y\\) es el número de eventos observados (\\(y = 0, 1, 2, \\dots\\)).\n\n\n4.3.1 Modelo de regresión de Poisson\nEn la regresión de Poisson, el objetivo es modelar la relación entre la tasa de ocurrencia de los eventos (\\(\\lambda\\)) y un conjunto de variables predictoras \\(X_1, X_2, \\dots, X_p\\).\nLa forma funcional del modelo de Poisson es:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nDonde:\n\n\\(\\log(\\lambda)\\) es la función de enlace logarítmica que asegura que la tasa \\(\\lambda\\) sea siempre positiva.\n\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) son los coeficientes del modelo que describen la influencia de cada predictor sobre la tasa de eventos.\n\nEl modelo puede expresarse en términos de la tasa esperada de eventos como:\n\\[\n\\lambda = e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}\n\\]\n\n\n4.3.2 Supuestos y limitaciones de la regresión de Poisson\nTal y como ocurre en el modelo de regresión lineal, para que la regresión de Poisson sea adecuada, se deben cumplir ciertos supuestos:\n\nIndependencia de los eventos: Los eventos deben ocurrir de manera independiente unos de otros.\nDistribución de Poisson de la variable dependiente: La variable de respuesta debe seguir una distribución de Poisson, donde la media y la varianza son iguales:\n\n\\[\n   E(Y) = Var(Y) = \\lambda\n   \\]\n\nNo sobredispersión: Uno de los problemas comunes en los datos de conteo es la sobredispersión, que ocurre cuando la varianza de los datos es mayor que la media (\\(Var(Y) &gt; E(Y)\\)). La presencia de sobredispersión indica que el modelo de Poisson puede no ser adecuado, y puede ser necesario considerar modelos alternativos como la regresión binomial negativa.\nNo exceso de ceros: Si hay demasiados ceros en los datos (por ejemplo, en el número de accidentes en diferentes localidades donde muchas tienen cero accidentes), puede ser necesario utilizar modelos de Poisson inflados en ceros (ZIP) (Lambert 1992).\n\n\n\n4.3.3 Interpretación de los resultados\nLa interpretación de los coeficientes en la regresión de Poisson difiere de la regresión lineal debido al uso de la función de enlace logarítmica.\nLos coeficientes \\(\\beta\\) representan el logaritmo de la tasa de eventos asociados con un cambio en la variable independiente. Para interpretar en términos de la tasa de ocurrencia, se utiliza el exponencial de los coeficientes:\n\\[\n  e^{\\beta_i}\n\\]\nEsto representa el factor de cambio multiplicativo en la tasa de eventos por cada unidad adicional en la variable \\(X_i\\).\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nSi \\(\\beta_1 = 0.5\\), entonces \\(e^{0.5} \\approx 1.65\\). Esto significa que por cada unidad adicional en \\(X_1\\), la tasa de ocurrencia de eventos aumenta en un 65%.\nSi \\(\\beta_1 = -0.3\\), entonces \\(e^{-0.3} \\approx 0.74\\). Esto indica que por cada unidad adicional en \\(X_1\\), la tasa de eventos disminuye en un 26%.\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nVamos a utilizar R para ajustar un modelo de regresión de Poisson. Supongamos que tenemos datos sobre el número de accidentes de tráfico en diferentes intersecciones de una ciudad, junto con variables como el volumen de tráfico y la visibilidad.\n\n# Simulación de datos para el número de accidentes\nset.seed(123)\nn &lt;- 100  # Número de observaciones\n\n# Variables predictoras\ntrafico &lt;- rnorm(n, mean = 1000, sd = 300)  # Volumen de tráfico en vehículos por día\nvisibilidad &lt;- rnorm(n, mean = 5, sd = 2)   # Visibilidad en kilómetros\n\n# Generar la tasa de accidentes (lambda) usando un modelo logarítmico\nlambda &lt;- exp(0.01 * trafico - 0.2 * visibilidad)\n\n# Generar el número de accidentes como una variable de Poisson\naccidentes &lt;- rpois(n, lambda = lambda)\n\n# Crear el data frame\ndatos_accidentes &lt;- data.frame(accidentes, trafico, visibilidad)\nhead(datos_accidentes)\n\n  accidentes   trafico visibilidad\n1       2102  831.8573    3.579187\n2       3744  930.9468    5.513767\n3     959848 1467.6125    4.506616\n4      11356 1021.1525    4.304915\n5      17411 1038.7863    3.096763\n6    1415794 1514.5195    4.909945\n\n# Ajustar el modelo de regresión de Poisson\nmodelo_poisson &lt;- glm(accidentes ~ trafico + visibilidad, data = datos_accidentes, family = poisson)\n\n# Resumen del modelo\nsummary(modelo_poisson)\n\n\nCall:\nglm(formula = accidentes ~ trafico + visibilidad, family = poisson, \n    data = datos_accidentes)\n\nCoefficients:\n              Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept) -1.610e-03  2.116e-03    -0.761    0.447    \ntrafico      1.000e-02  1.323e-06  7557.681   &lt;2e-16 ***\nvisibilidad -2.001e-01  1.025e-04 -1951.765   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1.2621e+08  on 99  degrees of freedom\nResidual deviance: 9.5511e+01  on 97  degrees of freedom\nAIC: 1216.4\n\nNumber of Fisher Scoring iterations: 3\n\n\nEl coeficiente asociado a trafico indica cómo el volumen de tráfico afecta la tasa de accidentes.\nEl coeficiente asociado a visibilidad muestra cómo la visibilidad afecta la frecuencia de accidentes.\n\n# Exponenciar los coeficientes para interpretar en términos de tasas\nexp(coef(modelo_poisson))\n\n(Intercept)     trafico visibilidad \n  0.9983910   1.0100516   0.8186814 \n\n\nUn coeficiente positivo implica que un aumento en la variable está asociado con un aumento en la tasa de accidentes.\nUn coeficiente negativo implica que un aumento en la variable está asociado con una disminución en la tasa de accidentes.\n\n\n\n\n\n4.3.4 Evaluación del modelo de Poisson\nLa sobredispersión ocurre cuando la varianza de los datos es mayor que la media, lo que puede invalidar los supuestos de la regresión de Poisson.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Calcular la relación entre el deviance y los grados de libertad\ndeviance &lt;- modelo_poisson$deviance\ngrados_libertad &lt;- modelo_poisson$df.residual\nsobredispersion &lt;- deviance / grados_libertad\n\nprint(paste(\"Índice de Sobredispersión:\", round(sobredispersion, 2)))\n\n[1] \"Índice de Sobredispersión: 0.98\"\n\n\n\n\n\nUn valor del índice de sobredispersión cercano a \\(1\\) sugiere que no hay sobredispersión. Por contra, un valor significativamente mayor que \\(1\\) sugiere la presencia de sobredispersión, y puede ser necesario considerar una regresión binomial negativa.\nDiagnóstico de Residuos:\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Gráfico de residuos deviance para evaluar el ajuste\nplot(residuals(modelo_poisson, type = \"deviance\"), main = \"Residuos Deviance\", ylab = \"Residuos\", xlab = \"Índice\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo sobre datos reales\n\n\n\n\n\nUn conjunto de datos clásico en R es warpbreaks, que contiene el número de roturas de hilo en diferentes condiciones de tensión y longitud del hilo.\n\n# Datos de ejemplo: número de roturas de hilo\ndata(warpbreaks)\n\n# Ajustar un modelo de Poisson para el número de roturas en función de la tensión\nmodelo_poisson_real &lt;- glm(breaks ~ wool + tension, data = warpbreaks, family = poisson)\n\n# Resumen del modelo\nsummary(modelo_poisson_real)\n\n\nCall:\nglm(formula = breaks ~ wool + tension, family = poisson, data = warpbreaks)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.69196    0.04541  81.302  &lt; 2e-16 ***\nwoolB       -0.20599    0.05157  -3.994 6.49e-05 ***\ntensionM    -0.32132    0.06027  -5.332 9.73e-08 ***\ntensionH    -0.51849    0.06396  -8.107 5.21e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 297.37  on 53  degrees of freedom\nResidual deviance: 210.39  on 50  degrees of freedom\nAIC: 493.06\n\nNumber of Fisher Scoring iterations: 4\n\n# Interpretación de coeficientes\nexp(coef(modelo_poisson_real))\n\n(Intercept)       woolB    tensionM    tensionH \n 40.1235380   0.8138425   0.7251908   0.5954198 \n\n\n\n\n\n\n\n4.3.5 Limitaciones y alternativas\nSi la varianza de los datos es mayor que la media, el modelo de Poisson no será adecuado. En este caso, se recomienda utilizar la regresión binomial negativa, que introduce un parámetro adicional para manejar la sobredispersión.\nSi hay más ceros de los esperados (por ejemplo, muchas intersecciones con cero accidentes), puede ser necesario utilizar modelos de Poisson inflados en ceros (ZIP) o binomial negativa inflada en ceros (ZINB).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#otros-glms",
    "href": "tema4.html#otros-glms",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.4 Otros GLMs",
    "text": "4.4 Otros GLMs\nLa regresión binomial negativa y los modelos basados en distribuciones como Gamma e Inversa Gaussiana amplían la capacidad de los Modelos Lineales Generalizados (GLM) para adaptarse a una amplia variedad de situaciones del mundo real. Estos modelos son especialmente útiles cuando los datos presentan características como sobredispersión, sesgo o restricciones en el dominio (por ejemplo, solo valores positivos). La elección adecuada del modelo y la función de enlace garantiza predicciones precisas y válidas, contribuyendo a la toma de decisiones informadas en campos como la salud, la ingeniería y la economía.\n\n4.4.1 Regresión Binomial Negativa\nTal y como hemos visto en apartados anteriores, la sobredispersión ocurre cuando la varianza de los datos de conteo es mayor que la media, lo cual viola uno de los supuestos clave de la regresión de Poisson, que asume que la media y la varianza son iguales (\\(E(Y) = Var(Y)\\)). La sobredispersión puede surgir por varias razones:\n\nHeterogeneidad no modelada: Existen factores que afectan la variable dependiente pero no han sido incluidos en el modelo.\nDependencia entre eventos: Los eventos no ocurren de forma independiente.\nExceso de ceros: Hay más ceros en los datos de los que predice la distribución de Poisson.\n\nCuando la sobredispersión está presente, la regresión de Poisson subestima los errores estándar, lo que puede llevar a conclusiones incorrectas sobre la significancia de los predictores.\nLa regresión binomial negativa es una extensión de la regresión de Poisson que introduce un parámetro adicional para manejar la sobredispersión. Este modelo permite que la varianza sea mayor que la media:\n\\[\nVar(Y) = \\lambda + \\alpha \\lambda^2\n\\]\nDonde \\(\\alpha\\) es el parámetro de dispersión. Si \\(\\alpha = 0\\), el modelo se reduce a la regresión de Poisson.\nLa forma funcional del modelo binomial negativa es similar al de Poisson:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\nPero la varianza ahora incluye el término adicional \\(\\alpha\\) para capturar la sobredispersión.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería MASS que contiene la función glm.nb\nlibrary(MASS)\n\n# Ajuste de un modelo binomial negativo con los datos simulados de accidentes\nmodelo_binom_neg &lt;- glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes)\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n\nWarning in glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes):\nalternation limit reached\n\n# Resumen del modelo\nsummary(modelo_binom_neg)\n\n\nCall:\nglm.nb(formula = accidentes ~ trafico + visibilidad, data = datos_accidentes, \n    init.theta = 245451587.1, link = log)\n\nCoefficients:\n              Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept) -1.620e-03  2.122e-03    -0.763    0.445    \ntrafico      1.000e-02  1.329e-06  7523.489   &lt;2e-16 ***\nvisibilidad -2.001e-01  1.036e-04 -1931.078   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(245394240) family taken to be 1)\n\n    Null deviance: 1.2568e+08  on 99  degrees of freedom\nResidual deviance: 9.5469e+01  on 97  degrees of freedom\nAIC: 1218.5\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  245451587 \n          Std. Err.:  488350955 \nWarning while fitting theta: alternation limit reached \n\n 2 x log-likelihood:  -1210.497 \n\n# Comparar la dispersión con el modelo de Poisson\ncat(\"Dispersión en Poisson:\", modelo_poisson$deviance / modelo_poisson$df.residual, \"\\n\")\n\nDispersión en Poisson: 0.9846515 \n\ncat(\"Dispersión en Binomial Negativa:\", modelo_binom_neg$theta, \"\\n\")\n\nDispersión en Binomial Negativa: 245451587 \n\n\n\n\n\n\nEl parámetro \\(\\theta\\) (dispersión) ajustado en el modelo binomial negativa ayuda a corregir la varianza subestimada en el modelo de Poisson.\nSi \\(\\theta\\) es significativamente mayor que 1, se confirma la presencia de sobredispersión.\n\n\n\n\n4.4.2 Modelos para variables continuas No Normales\nExisten situaciones en las que la variable dependiente es continua, pero no sigue una distribución normal. En estos casos, los Modelos Lineales Generalizados (GLM) permiten utilizar distribuciones alternativas como Gamma o Inversa Gaussiana, junto con funciones de enlace específicas.\n\n4.4.2.1 Regresión Gamma para datos positivos y sesgados\nLa regresión Gamma es adecuada para modelar variables continuas que son positivas y tienen una distribución sesgada a la derecha. Ejemplos típicos incluyen tiempos de espera, costos médicos o duración de procesos.\n\nLa distribución Gamma asume que la variable dependiente es continua y positiva.\nLa varianza de la variable dependiente aumenta proporcionalmente al cuadrado de la media.\n\nFunción de Enlace Común: \\[\n\\log(\\mu) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Simulación de costos médicos\nset.seed(123)\nn &lt;- 100\ningresos &lt;- rnorm(n, mean = 50000, sd = 10000)\nedad &lt;- rnorm(n, mean = 45, sd = 10)\ncostos &lt;- rgamma(n, shape = 2, rate = 0.00005 * ingresos + 0.01 * edad)\n\n# Ajuste del modelo Gamma\nmodelo_gamma &lt;- glm(costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\n# Resumen del modelo\nsummary(modelo_gamma)\n\n\nCall:\nglm(formula = costos ~ ingresos + edad, family = Gamma(link = \"log\"))\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  3.440e-01  5.294e-01   0.650   0.5173  \ningresos    -1.807e-05  7.804e-06  -2.316   0.0227 *\nedad         3.584e-03  7.366e-03   0.487   0.6277  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.5010938)\n\n    Null deviance: 60.771  on 99  degrees of freedom\nResidual deviance: 58.345  on 97  degrees of freedom\nAIC: 105.47\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nLos coeficientes muestran cómo los ingresos y la edad afectan los costos médicos esperados.\nEl enlace logarítmico asegura que las predicciones sean siempre positivas.\n\n\n\n\n\n\n4.4.2.2 Regresión Inversa Gaussiana\nLa regresión Inversa Gaussiana es útil para modelar tiempos de respuesta o variables donde la varianza disminuye rápidamente a medida que la media aumenta. Este modelo se aplica en campos como la ingeniería, donde se analizan tiempos hasta fallas de sistemas.\nFunción de Enlace Común: \\[\n\\frac{1}{\\mu^2} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\n\n# Instalar y cargar la librería correcta\nlibrary(statmod)\n\n# Simulación de datos\nset.seed(123)\nn &lt;- 100\ncarga_trabajo &lt;- rnorm(n, mean = 50, sd = 10)\n\n# Generar tiempos hasta el fallo usando la distribución inversa gaussiana\n# Aseguramos que los valores de carga_trabajo sean positivos para evitar problemas numéricos\ncarga_trabajo[carga_trabajo &lt;= 0] &lt;- 1\ntiempo_fallo &lt;- rinvgauss(n, mean = 100 / carga_trabajo, dispersion = 1)\n\n# Ajuste del modelo Inversa Gaussiana con enlace logarítmico\nmodelo_inversa_gauss &lt;- glm(tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"),start = c(0.01, 0.01))\n\nWarning in sqrt(eta): Se han producido NaNs\n\n\nWarning: step size truncated due to divergence\n\n# Resumen del modelo\nsummary(modelo_inversa_gauss)\n\n\nCall:\nglm(formula = tiempo_fallo ~ carga_trabajo, family = inverse.gaussian(link = \"1/mu^2\"), \n    start = c(0.01, 0.01))\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -0.106143   0.462230  -0.230    0.819\ncarga_trabajo  0.008261   0.009623   0.859    0.393\n\n(Dispersion parameter for inverse.gaussian family taken to be 1.348442)\n\n    Null deviance: 94.085  on 99  degrees of freedom\nResidual deviance: 93.171  on 98  degrees of freedom\nAIC: 294.2\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  },
  {
    "objectID": "tema4.html#comparación-de-modelos-y-evaluación-del-ajuste",
    "href": "tema4.html#comparación-de-modelos-y-evaluación-del-ajuste",
    "title": "4  Modelos de regresión generalizada",
    "section": "4.5 Comparación de modelos y evaluación del ajuste",
    "text": "4.5 Comparación de modelos y evaluación del ajuste\nUna vez que se han ajustado varios modelos, es crucial evaluar su rendimiento y seleccionar el más adecuado para el problema en cuestión. La evaluación del ajuste y la comparación de modelos permiten identificar cuál modelo describe mejor los datos sin caer en el sobreajuste, es decir, ajustarse demasiado a los datos de entrenamiento a costa de un mal desempeño en datos nuevos.\nEsta sección abordará los métodos más comunes para evaluar y comparar modelos, incluyendo criterios estadísticos, técnicas de validación y análisis de residuos.\n\n4.5.1 Criterios de selección de modelos (AIC, BIC)\nLos criterios de selección de modelos como el AIC y el BIC permiten comparar modelos que han sido ajustados a los mismos datos, penalizando la complejidad para evitar el sobreajuste.\nAkaike Information Criterion (AIC)\nEl Criterio de Información de Akaike (AIC) es una medida que equilibra la calidad del ajuste y la complejidad del modelo. Se calcula como:\n\\[\nAIC = -2 \\log(L) + 2k\n\\]\nDonde:\n\n\\(L\\) es el log-likelihood del modelo (medida de la probabilidad de los datos dados los parámetros del modelo).\n\\(k\\) es el número de parámetros del modelo.\n\nRespecto a su interpretación, el AIC no tiene una interpretación absoluta, solo es útil para comparar modelos ajustados a los mismos datos. Un AIC menor indica un mejor equilibrio entre ajuste y simplicidad.\nBayesian Information Criterion (BIC)\nEl Criterio de Información Bayesiano (BIC) es similar al AIC, pero penaliza más severamente la complejidad del modelo, especialmente cuando el número de observaciones es grande. Se calcula como:\n\\[\nBIC = -2 \\log(L) + k \\log(n)\n\\]\nDonde: - \\(n\\) es el número de observaciones.\nEl BIC favorece modelos más simples en comparación con el AIC. Un BIC menor indica un mejor modelo.\n\n\n\n\n\n\nEjemplo: comparación de AIC y BIC\n\n\n\n\n\n\n# Comparación de modelos: Poisson vs Binomial Negativa\n\n# Modelo de Poisson\nmodelo_poisson &lt;- glm(accidentes ~ trafico + visibilidad, family = poisson, data = datos_accidentes)\n\n# Modelo Binomial Negativa\nlibrary(MASS)\nmodelo_binom_neg &lt;- glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes)\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n\nWarning in glm.nb(accidentes ~ trafico + visibilidad, data = datos_accidentes):\nalternation limit reached\n\n# Calcular AIC y BIC\nAIC(modelo_poisson, modelo_binom_neg)\n\n                 df      AIC\nmodelo_poisson    3 1216.421\nmodelo_binom_neg  4 1218.497\n\nBIC(modelo_poisson, modelo_binom_neg)\n\n                 df      BIC\nmodelo_poisson    3 1224.236\nmodelo_binom_neg  4 1228.918\n\n\n\n\n\nEl modelo con el menor AIC o BIC es preferido. Si ambos modelos tienen valores similares, se puede preferir el modelo más simple (con menos parámetros).\n\n\n4.5.2 Validación cruzada y técnicas de evaluación predictiva\nLa validación cruzada y otras técnicas de evaluación predictiva permiten estimar cómo se desempeñará un modelo en datos no vistos, lo que es esencial para evitar el sobreajuste.\nLa validación cruzada k-fold divide el conjunto de datos en k subconjuntos (o “folds”). El modelo se ajusta k veces, cada vez utilizando \\(k-1\\) folds para el entrenamiento y el fold restante para la prueba. El rendimiento se promedia sobre todas las iteraciones. La validación cruzada utiliza eficientemente los datos disponibles, proporcionando una estimación robusta del rendimiento del modelo.\n\n\n\n\n\n\nEjemplo: Validación cruzada\n\n\n\n\n\n\n# Instalar y cargar la librería caret\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n# Definir la validación cruzada de 5-fold\ncontrol &lt;- trainControl(method = \"cv\", number = 5)\n\n# Ajustar un modelo de regresión logística con validación cruzada\nmodelo_cv &lt;- train(type ~ npreg + glu + bmi, data = Pima.tr, method = \"glm\", family = \"binomial\", trControl = control)\n\n# Resultados de la validación cruzada\nprint(modelo_cv)\n\nGeneralized Linear Model \n\n200 samples\n  3 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 161, 160, 161, 159, 159 \nResampling results:\n\n  Accuracy   Kappa    \n  0.7603815  0.4378568\n\n\n\n\n\nOtra técnica común es dividir el conjunto de datos en un conjunto de entrenamiento (por ejemplo, el \\(70\\%\\)) y un conjunto de prueba (\\(30\\%\\)). El modelo se ajusta en el conjunto de entrenamiento y se evalúa en el conjunto de prueba.\n\n\n\n\n\n\nEjemplo: Train/Test\n\n\n\n\n\n\n# Dividir el conjunto de datos en entrenamiento y prueba\nset.seed(123)\nlibrary(caret)\nindices &lt;- createDataPartition(Pima.tr$type, p = 0.7, list = FALSE)\nentrenamiento &lt;- Pima.tr[indices, ]\nprueba &lt;- Pima.tr[-indices, ]\n\n# Ajustar el modelo en el conjunto de entrenamiento\nmodelo_entrenamiento &lt;- glm(type ~ npreg + glu + bmi, data = entrenamiento, family = binomial)\n\n# Realizar predicciones en el conjunto de prueba\npredicciones &lt;- predict(modelo_entrenamiento, newdata = prueba, type = \"response\")\n\n# Evaluar precisión\npredicciones_clase &lt;- ifelse(predicciones &gt; 0.5, \"Yes\", \"No\")\nconfusionMatrix(as.factor(predicciones_clase), as.factor(prueba$type))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  34  10\n       Yes  5  10\n                                          \n               Accuracy : 0.7458          \n                 95% CI : (0.6156, 0.8502)\n    No Information Rate : 0.661           \n    P-Value [Acc &gt; NIR] : 0.1062          \n                                          \n                  Kappa : 0.3959          \n                                          \n Mcnemar's Test P-Value : 0.3017          \n                                          \n            Sensitivity : 0.8718          \n            Specificity : 0.5000          \n         Pos Pred Value : 0.7727          \n         Neg Pred Value : 0.6667          \n             Prevalence : 0.6610          \n         Detection Rate : 0.5763          \n   Detection Prevalence : 0.7458          \n      Balanced Accuracy : 0.6859          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\n\n\n\n4.5.3 Diagnóstico de residuos y buenas prácticas\nEl análisis de residuos es fundamental para evaluar el ajuste del modelo y detectar problemas como la falta de ajuste, valores atípicos o violaciones de los supuestos del modelo.\nLos residuos deviance son una medida común en los Modelos Lineales Generalizados (GLM). Representan la diferencia entre el modelo ajustado y el modelo perfecto (donde la predicción es exactamente igual al valor observado).\nTipos de residuos:\n\nResiduos Pearson:\n\\[\nr_i = \\frac{y_i - \\hat{y}_i}{\\sqrt{\\hat{V}(y_i)}}\n\\] Donde \\(\\hat{V}(y_i)\\) es la varianza estimada de \\(y_i\\).\nResiduos Deviance:\nRepresentan la contribución de cada observación al deviance total del modelo.\n\n\n\n\n\n\n\n\nEjemplo: Análisis de los residuos\n\n\n\n\n\n\n# Gráfico de residuos deviance para un modelo de Poisson\nplot(residuals(modelo_poisson, type = \"deviance\"), \n     main = \"Residuos Deviance del Modelo Poisson\", \n     ylab = \"Residuos Deviance\", xlab = \"Índice\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Gráfico de residuos para la regresión logística\nplot(residuals(modelo_entrenamiento, type = \"deviance\"), \n     main = \"Residuos Deviance del Modelo Logístico\", \n     ylab = \"Residuos Deviance\", xlab = \"Índice\")\nabline(h = 0, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nPara detectar valores atípicos y su influencia emplearemos:\n\nDistancia de Cook: Identifica observaciones influyentes que tienen un impacto significativo en los coeficientes del modelo.\nLeverage: Mide el impacto potencial de una observación en el ajuste del modelo.\n\n\n\n\n\n\n\nEjemplo: Detección de observaciones influyentes\n\n\n\n\n\n\n# Distancia de Cook para identificar observaciones influyentes\ncooksd &lt;- cooks.distance(modelo_poisson)\n\n# Gráfico de la distancia de Cook\nplot(cooksd, main = \"Distancia de Cook\", ylab = \"Influencia\", xlab = \"Índice\")\nabline(h = 4 / length(cooksd), col = \"red\")  # Línea de referencia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoxe, Stefany, Stephen G West, y Leona S Aiken. 2009. «The analysis of count data: A gentle introduction to Poisson regression and its alternatives». Journal of personality assessment 91 (2): 121-36.\n\n\nHosmer Jr, David W, Stanley Lemeshow, y Rodney X Sturdivant. 2013. Applied logistic regression. John Wiley & Sons.\n\n\nLambert, Diane. 1992. «Zero-inflated Poisson regression, with an application to defects in manufacturing». Technometrics 34 (1): 1-14.\n\n\nNelder, John Ashworth, y Robert WM Wedderburn. 1972. «Generalized linear models». Journal of the Royal Statistical Society Series A: Statistics in Society 135 (3): 370-84.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelos de regresión generalizada</span>"
    ]
  }
]